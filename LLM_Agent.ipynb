{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tttequila/Kaggle_20Q/blob/main/LLM_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqjWHE8-MQVl"
      },
      "source": [
        "Configuring your kaggle token, see more details in the [**Configure your API key**](https://ai.google.dev/gemma/docs/setup) section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### set up env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndGYpeKSFV5U",
        "outputId": "84bb69d3-e2d6-414c-bca5-f8589978fece"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNrgspAf9T8W"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mkdir ~/.kaggle\n",
        "# change the first path to your path of kaggle.json\n",
        "cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-l45FATYz0T",
        "outputId": "3418fd87-0721-4a26-c3ca-617516d878fd"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -q -U torch immutabledict sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### set up Gemma lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "bba625c15982460e98611f8edbdaa565",
            "ae11770fb0ce446bbcf16c4ff3a23ec6",
            "eae207bb982e48b4b7d16973ab46866c",
            "5f83051bc9ca40d3b5f1f1a66f1e95c1",
            "0f7960a68e12442ebf4e560d1206f268",
            "0a412d2972f340b49ad07708fc95e8f0",
            "baed24fddd77489ebe9d5d1f7ac9f8c7",
            "f3eae9c63d1545b786d49525d43deef9",
            "52f425bb48984e01acee327c0de2c6b3",
            "1c49d383eb5a45a99f87d5814a3a2ae3",
            "78354105a73c48a6942a19416ed861e9",
            "61e3c070558e4bde80ba13ebc6dd64ae",
            "0f0521043eae4862938eb1ccfb160dfa",
            "612928d980594c66aa52f38d6037d4e5",
            "85a7112f258748938453b9f66379a229",
            "6de1853e4d924c3d887c3fedbfa7d184",
            "df33256681e1416bb90d761086e6f02a"
          ]
        },
        "id": "HhC3bL88SqDk",
        "outputId": "a279eccd-bae8-4714-a561-9db2ca16b3d9"
      },
      "outputs": [],
      "source": [
        "# login the kaggle (need to store you kaggle.json to your google dirve)\n",
        "import kagglehub\n",
        "kagglehub.login()\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"gemma_pytorch/gemma\")\n",
        "sys.path.append(\"gemma_pytorch\")\n",
        "from gemma.config import get_config_for_7b, get_config_for_2b\n",
        "from gemma.model import GemmaForCausalLM, GemmaModel\n",
        "import contextlib, torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F70B5D8GKLSJ",
        "outputId": "4f1c02f0-f4d6-4ec0-f49c-ca5c9af08da3"
      },
      "outputs": [],
      "source": [
        "# Choose variant and machine type\n",
        "VARIANT = '7b-it-quant'\n",
        "MACHINE_TYPE = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y10m95XaBeQ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gemma\n",
        "import itertools\n",
        "from typing import Iterable\n",
        "from typing import Any, List, Optional, Sequence, Tuple, Union\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def _set_default_tensor_type(dtype: torch.dtype):\n",
        "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
        "  torch.set_default_dtype(dtype)\n",
        "  yield\n",
        "  torch.set_default_dtype(torch.float)\n",
        "  \n",
        "\n",
        "def interleave_unequal(x, y):\n",
        "    '''\n",
        "        Interleave two lists of unequal length.\n",
        "    '''\n",
        "    return [\n",
        "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building Agents\n",
        "\n",
        "<details>\n",
        "  <summary> model </summary>\n",
        "  \n",
        "  - `self.forward()`: getting next token and corresponding logits\n",
        "  - `self.generate()`: see summary below. May need to be rewriten if we wanna get the cumulative logits for the whole sentence  \n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary> model.generate() </summary>\n",
        "  \n",
        "  - **prompts** | `Union[str, Sequence[str]]`: Your prompts\n",
        "  - **device** | `Any`: Devices\n",
        "  - **output_len** | `int`: max output length\n",
        "  - **temperature** | `Union[float, None]`: temperature degree, controlling how variant its response could be  \n",
        "  - **top_p** | `float`:\n",
        "  - **top_k** | `int`:\n",
        "\n",
        "  regarding temperature, top_p and top_k, check this [link](https://blog.csdn.net/REfusing/article/details/137866583)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Formatter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start_of_turn>user\n",
            "You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. You need to utilize your knowledge and reasoning skills to play a game of 20 questions. In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"a cross on the flag.\" Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" And after your thorough thinking, you need to denote the key binary attribute with double stars<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Now give me an example of thinking with the Chain of Thought<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given Information: [\"is a country\", \"is in Europe\", \"its flag is with red color\"] Considerations: 1. Geographical Location: European countries can be divided into regions such as Northern Europe, Southern Europe, Western Europe, and Eastern Europe. This division can effectively narrow down the options. 2. Population and Area: Considering population and area can also eliminate some countries. 3. Other Colors on the Flag: We know the flag has red; we can further narrow down by asking about other colors. 4. Language: European countries have various languages; asking about the official language can narrow it further. Reasoning: Southern Europe includes a few countries such as Spain, Italy, Portugal, Greece, etc. Determining if it is in Southern Europe can immediately eliminate about three-quarters of European countries. Compared to questions about flag colors or language, geographical location questions are more evenly distributed, and a **yes** or **no** answer will significantly narrow down the range. Possible Next Steps: If the answer is **yes**, we can further inquire about other notable characteristics of these countries, such as language or history. If the answer is **no**, we can turn our attention to Western, Eastern, or Northern Europe and continue narrowing down the options.**Question:** \"Is this country in Southern Europe?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Now let us play a game of 20 questions. You will be the questioner, and I will be the answerer.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It is the first round, so i will randomly choose a attribute from [\"country\", \"city\", \"landmark\"] as the first attribut. Here I choose country as the first attribute.**Question:** \"Is it a country?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\"] Considerations: 1. Type of Entity: Determining whether it is a city or a landmark can greatly narrow down our options. 2. Geographical Location: If we can determine the continent, we can significantly reduce the number of possible cities or landmarks. 3. Cultural/Historical Significance: Asking about cultural or historical significance can also be helpful, but it might be too broad at this stage. Reasoning: To effectively narrow it down, the most efficient next step is to determine if it is a city or a landmark. By determining if it is a city, we can immediately focus on known cities globally, which are fewer and more well-documented compared to all possible landmarks. If it is not a city, we can then focus on landmarks, which might require more specific questions but will also significantly narrow our scope. Possible Next Steps: If the answer is **yes**, we can then ask about the continent or specific features of the city. If the answer is **no**, we can ask about the type of landmark or its location. **Question:** \"Is it a city?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\"] Considerations: Alright, so it is not a city. Now we know it must be a landmark. Given this, our goal is to identify the type or location of the landmark. 1. Type of Landmark: There are various types such as natural landmarks (mountains, rivers), man-made landmarks (statues, buildings), or historical sites. 2. Geographical Location: Identifying the continent or region can greatly narrow down the possibilities. To get a high information gain, let us narrow down the type of landmark first. Reasoning: Landmarks can be broadly categorized into man-made or natural. This binary distinction will help us immediately focus on a more specific set of possibilities. Man-made landmarks include famous buildings, statues, monuments, etc., while natural landmarks include mountains, rivers, and natural formations. Possible Next Steps: If the answer is **yes**, we can then ask about the purpose or era of the man-made structure. If the answer is **no**, we can focus on identifying the type of natural landmark or its location. **Question:** \"Is it a man-made landmark?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is not a man-made landmark\"] Considerations: 1. Type of Natural Landmark: Common natural landmarks include mountains, rivers, lakes, forests, deserts, etc. 2. Geographical Location: Identifying the continent or region can still be very helpful. To further narrow down, let us focus on the type of natural landmark. Reasoning: River are one of the most well-known types of natural landmarks. Possible Next Steps: If the answer is **yes**, we can then focus on identifying which river by asking about its specific features or location. If the answer is **no**, we can eliminate a significant category and focus on other types of natural landmarks like mountains, lakes, deserts, or forests. **Question:** \"Is it a river?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\"] Considerations: Great, so it is a river. Now, we need to narrow down which river it could be. To do this effectively, identifying the continent or region will significantly help. 1. Geographical Location: Identifying the continent or region can narrow down the list of possible rivers considerably. 2. Length or Size: Asking about whether it is among the longest rivers might also help. 3. Famous Rivers: There are some rivers that are globally well-known due to their significance. To achieve a high information gain, let us focus on the geographical location. Reasoning: Asia has some of the world us most significant and longest rivers, such as the Yangtze, Ganges, and Mekong. Possible Next Steps: If the answer is **yes**, we can focus on Asian rivers specifically. If the answer is **no**, we can eliminate Asia and consider other continents like Africa, Europe, South America, or North America. **Question:** \"Is this reiver located in Asia?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\"] Excellent, the river is located in Asia. This narrows down our options significantly. Now, we need to focus on identifying which river it could be. Considerations: 1. Famous Asian Rivers: Asia has several famous rivers, such as the Yangtze, Ganges, Mekong, Indus, and Yellow River. 2. Geographical Features: We could ask about the river's length, the countries it flows through, or any significant historical or cultural associations. 3. To narrow it down further, let us focus on whether it is one of the most well-known rivers. Reasoning: These three rivers are among the longest and most significant in Asia. Possible Next Steps: If the answer is **yes**, we can focus on distinguishing between these three. If the answer is **no**, we can focus on other significant rivers like the Ganges, Indus, or Brahmaputra. **Question:** \"Is this river one of the three longest rivers in Asia (Yangtze, Yellow, or Mekong)?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\"] Great, we now know it is one of the three longest rivers in Asia: the Yangtze, Yellow, or Mekong. Let us narrow it down further by focusing on specific characteristics of these rivers. Considerations: 1. Geographical Location: We can ask about the countries the river flows through. 2. Cultural/Historical Significance: Each of these rivers has distinct cultural and historical significance. To narrow it down further, let us ask about the country. Reasoning: Both the Yangtze and Yellow rivers flow through China, while the Mekong flows through several countries including China, but predominantly in Southeast Asia. Possible Next Steps: If the answer is yes, it will most likely be the Yangtze or Yellow River. If the answer is no, it must be the Mekong River. **Question:** \"Does this river flow through China?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\", \"It flows through China\"] Great, so the river flows through China. This means it is either the Yangtze River or the Yellow River. To determine which one it is, we should focus on distinguishing features. Considerations: 1. Length: The Yangtze is the longest river in Asia and the third-longest in the world. 2. Color/Name: The Yellow River is named for its yellowish color due to loess sediments. Reasoning: The Yangtze River is the longest river in Asia. Possible Next Steps: If the answer is **yes**, it is the Yangtze River. If the answer is **no**, it must be the Yellow River. **Question:** \"Is this river the longest river in Asia?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It is the Yangtze River<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Correct!<end_of_turn>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Iterable\n",
        "import itertools\n",
        "\n",
        "class PromptFormatter:\n",
        "    \n",
        "    '''\n",
        "        formatter class to format the prompt text for the model. \n",
        "        A general idea is \n",
        "    '''\n",
        "    \n",
        "    _start_token = '<start_of_turn>'\n",
        "    _end_token = '<end_of_turn>'\n",
        "        \n",
        "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
        "        self._system_prompt = system_prompt\n",
        "        self._few_shot_examples = few_shot_examples\n",
        "        self._template_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
        "        self._template_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
        "        self._all_prompt = ''\n",
        "        self.reset()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self._all_prompt\n",
        "        \n",
        "    def reset(self):\n",
        "        self._all_prompt = ''\n",
        "        # if system prompt is provided, add it to the prompt\n",
        "        if self._system_prompt:\n",
        "            self._all_prompt += self._template_user.format(self._system_prompt)\n",
        "        # same for few shot examples\n",
        "        if self._few_shot_examples:\n",
        "            self.add_rounds(self._few_shot_examples, start_agent='user')\n",
        "        \n",
        "    def add_user_round(self, user_prompt: str):\n",
        "        # add user round to the prompt\n",
        "        self._all_prompt += self._template_user.format(user_prompt)\n",
        "\n",
        "    def add_agent_round(self, model_response: str):\n",
        "        self._all_prompt += self._template_model.format(model_response)\n",
        "    \n",
        "    def add_rounds(self, rounds: Iterable, start_agent: str):\n",
        "        '''\n",
        "            Apply a sequence of rounds to the formatter, starting with the specified agent.\n",
        "        '''\n",
        "        formatters = [self.add_agent_round, self.add_user_round] if start_agent == 'model' else [self.add_user_round, self.add_agent_round] # here, self.model and self.user are functions definded above\n",
        "        formatters = itertools.cycle(formatters)\n",
        "        for fmt, round in zip(formatters, rounds):\n",
        "            fmt(round)\n",
        "        return self\n",
        "    \n",
        "    # def add_end_token(self):\n",
        "    #     self._all_prompt += f\"{self._end_token}\\n\"\n",
        "    \n",
        "    def add_new_round(self, player: str, prompt:str = None, end_token: bool = False):\n",
        "        self._all_prompt += f\"{self._start_token}{player}\\n\"\n",
        "        if prompt:\n",
        "            self._all_prompt += f'{prompt}'\n",
        "        if end_token:\n",
        "            # self.add_end_token()\n",
        "            self._all_prompt += f\"{self._end_token}\\n\"\n",
        "    \n",
        "    def formate_MCQA(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "sys_prompt = 'You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. \\\n",
        "You need to utilize your knowledge and reasoning skills to play a game of 20 questions. \\\n",
        "In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"a cross on the flag.\" \\\n",
        "Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" \\\n",
        "And after your thorough thinking, you need to denote the key binary attribute with double stars.  Here are few examples of how to use Chain of thought:'\n",
        "\n",
        "few_shot_examples = [\n",
        "\n",
        "# 'Now give me an example of thinking with the Chain of Thought',\n",
        "                     \n",
        "# 'Given Information: [\"is a country\", \"is in Europe\", \"its flag is with red color\"] \\\n",
        "# Considerations: \\\n",
        "# 1. Geographical Location: European countries can be divided into regions such as Northern Europe, Southern Europe, Western Europe, and Eastern Europe. This division can effectively narrow down the options. \\\n",
        "# 2. Population and Area: Considering population and area can also eliminate some countries. \\\n",
        "# 3. Other Colors on the Flag: We know the flag has red; we can further narrow down by asking about other colors. \\\n",
        "# 4. Language: European countries have various languages; asking about the official language can narrow it further. \\\n",
        "# Reasoning: Southern Europe includes a few countries such as Spain, Italy, Portugal, Greece, etc. Determining if it is in Southern Europe can immediately eliminate about three-quarters of European countries. \\\n",
        "# Compared to questions about flag colors or language, geographical location questions are more evenly distributed, and a yes or no answer will significantly narrow down the range. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is yes, we can further inquire about other notable characteristics of these countries, such as language or history. \\\n",
        "# If the answer is no, we can turn our attention to Western, Eastern, or Northern Europe and continue narrowing down the options.\\\n",
        "# **Key Attribute** \"in Southern Europe\"\\\n",
        "# **Question** \"Is this country in Southern Europe?\"', \n",
        "\n",
        "'Now let us play a game of 20 questions. You will be the questioner, and I will be the answerer.',\n",
        "\n",
        "'It is the first round, so i will randomly choose a attribute from [\"country\", \"city\", \"landmark\"] as the first attribut. Here I choose country as the first attribute.\\\n",
        "**Key Attribute** \"a country\"\\\n",
        "**Question** \"Is it a country?\"',\n",
        "\n",
        "'**no**',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\"] \\\n",
        "Considerations: \\\n",
        "1. Type of Entity: Determining whether it is a city or a landmark can greatly narrow down our options. \\\n",
        "2. Geographical Location: If we can determine the continent, we can significantly reduce the number of possible cities or landmarks. \\\n",
        "3. Cultural/Historical Significance: Asking about cultural or historical significance can also be helpful, but it might be too broad at this stage. \\\n",
        "Reasoning: To effectively narrow it down, the most efficient next step is to determine if it is a city or a landmark. By determining if it is a city, we can immediately focus on known cities globally, which are fewer and more well-documented compared to all possible landmarks. \\\n",
        "If it is not a city, we can then focus on landmarks, which might require more specific questions but will also significantly narrow our scope. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is yes, we can then ask about the continent or specific features of the city. \\\n",
        "If the answer is no, we can ask about the type of landmark or its location. \\\n",
        "**Key Attribute** \"a city\\\n",
        "**Question** \"Is it a city?\"',\n",
        "\n",
        "'**no**',\n",
        "\n",
        "# 'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\"] \\\n",
        "# Considerations: \\\n",
        "# Alright, so it is not a city. Now we know it must be a landmark. Given this, our goal is to identify the type or location of the landmark. \\\n",
        "# 1. Type of Landmark: There are various types such as natural landmarks (mountains, rivers), man-made landmarks (statues, buildings), or historical sites. \\\n",
        "# 2. Geographical Location: Identifying the continent or region can greatly narrow down the possibilities. \\\n",
        "# To get a high information gain, let us narrow down the type of landmark first. \\\n",
        "# Reasoning: Landmarks can be broadly categorized into man-made or natural. This binary distinction will help us immediately focus on a more specific set of possibilities. \\\n",
        "# Man-made landmarks include famous buildings, statues, monuments, etc., while natural landmarks include mountains, rivers, and natural formations. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is yes, we can then ask about the purpose or era of the man-made structure. \\\n",
        "# If the answer is no, we can focus on identifying the type of natural landmark or its location. \\\n",
        "# **Key Attribute** \"a man-made landmark\"\\\n",
        "# **Question** \"Is it a man-made landmark?\"',\n",
        "\n",
        "# '**no**',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is not a man-made landmark\"] \\\n",
        "Considerations: \\\n",
        "1. Type of Natural Landmark: Common natural landmarks include mountains, rivers, lakes, forests, deserts, etc. \\\n",
        "2. Geographical Location: Identifying the continent or region can still be very helpful. \\\n",
        "To further narrow down, let us focus on the type of natural landmark. \\\n",
        "Reasoning: River are one of the most well-known types of natural landmarks. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is yes, we can then focus on identifying which river by asking about its specific features or location. \\\n",
        "If the answer is no, we can eliminate a significant category and focus on other types of natural landmarks like mountains, lakes, deserts, or forests. \\\n",
        "**Key Attribute** \"a river\"\\\n",
        "**Question** \"Is it a river?\"',\n",
        "\n",
        "'**yes**',\n",
        "\n",
        "# 'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\"] \\\n",
        "# Considerations: \\\n",
        "# Great, so it is a river. Now, we need to narrow down which river it could be. To do this effectively, identifying the continent or region will significantly help. \\\n",
        "# 1. Geographical Location: Identifying the continent or region can narrow down the list of possible rivers considerably. \\\n",
        "# 2. Length or Size: Asking about whether it is among the longest rivers might also help. \\\n",
        "# 3. Famous Rivers: There are some rivers that are globally well-known due to their significance. \\\n",
        "# To achieve a high information gain, let us focus on the geographical location. \\\n",
        "# Reasoning: Asia has some of the world us most significant and longest rivers, such as the Yangtze, Ganges, and Mekong. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is yes, we can focus on Asian rivers specifically. \\\n",
        "# If the answer is no, we can eliminate Asia and consider other continents like Africa, Europe, South America, or North America. \\\n",
        "# **Key Attribute** \"located in Asia\"\\\n",
        "# **Question** \"Is this reiver located in Asia?\"',\n",
        "\n",
        "# '**yes**', \n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\"] \\\n",
        "Excellent, the river is located in Asia. This narrows down our options significantly. Now, we need to focus on identifying which river it could be. \\\n",
        "Considerations: \\\n",
        "1. Famous Asian Rivers: Asia has several famous rivers, such as the Yangtze, Ganges, Mekong, Indus, and Yellow River. \\\n",
        "2. Geographical Features: We could ask about the river\\'s length, the countries it flows through, or any significant historical or cultural associations. \\\n",
        "3. To narrow it down further, let us focus on whether it is one of the most well-known rivers. \\\n",
        "Reasoning: These three rivers are among the longest and most significant in Asia. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is yes, we can focus on distinguishing between these three. \\\n",
        "If the answer is no, we can focus on other significant rivers like the Ganges, Indus, or Brahmaputra. \\\n",
        "**Key Attribute** \"one of the three longest rivers in Asia\"\\\n",
        "**Question** \"Is this river one of the three longest rivers in Asia (Yangtze, Yellow, or Mekong)?\"',\n",
        "\n",
        "'**yes**',\n",
        "\n",
        "# 'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\"] \\\n",
        "# Great, we now know it is one of the three longest rivers in Asia: the Yangtze, Yellow, or Mekong. Let us narrow it down further by focusing on specific characteristics of these rivers. \\\n",
        "# Considerations: \\\n",
        "# 1. Geographical Location: We can ask about the countries the river flows through. \\\n",
        "# 2. Cultural/Historical Significance: Each of these rivers has distinct cultural and historical significance. \\\n",
        "# To narrow it down further, let us ask about the country. \\\n",
        "# Reasoning: Both the Yangtze and Yellow rivers flow through China, while the Mekong flows through several countries including China, but predominantly in Southeast Asia. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is yes, it will most likely be the Yangtze or Yellow River. \\\n",
        "# If the answer is no, it must be the Mekong River. \\\n",
        "# **Key Attribute** \"flow through China\"\\\n",
        "# **Question** \"Does this river flow through China?\"',\n",
        "\n",
        "# '**yes**',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\", \"It flows through China\"] \\\n",
        "Great, so the river flows through China. This means it is either the Yangtze River or the Yellow River. To determine which one it is, we should focus on distinguishing features. \\\n",
        "Considerations: \\\n",
        "1. Length: The Yangtze is the longest river in Asia and the third-longest in the world. \\\n",
        "2. Color/Name: The Yellow River is named for its yellowish color due to loess sediments. \\\n",
        "Reasoning: The Yangtze River is the longest river in Asia. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is yes, it is the Yangtze River. \\\n",
        "If the answer is no, it must be the Yellow River. \\\n",
        "**Key Attribute** \"the longest river in Asia\"\\\n",
        "**Question** \"Is this river the longest river in Asia?\"',\n",
        "\n",
        "'**yes**, Now guess the keyword. Give your analysis with chain of thought and known information. And surround your guess with double asterisks in the end',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\", \"It flows through China\", \"It is the longest river in Asia\"] \\\n",
        "Excellent, we have successfully deduced that the river is the longest river in Asia. Given the information we have, the only river that fits this description is the Yangtze River. \\\n",
        "**Guess** \"Yangtze River\"',\n",
        "\n",
        "'Correct!']\n",
        "\n",
        "print(str(PromptFormatter(sys_prompt, few_shot_examples)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, [2, 3]]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "x = [1]\n",
        "x.append()\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class GemmaAgent_Guesser:\n",
        "    \n",
        "    def __init__(self, model_variant, device='cuda:0', env=\"kaggle\", system_prompt=None, few_shot_examples=None):\n",
        "        # model initialization\n",
        "        self.device = device\n",
        "        self.model_variant = model_variant\n",
        "        \n",
        "        WEIGHTS_PATH = self._set_up_env(env)\n",
        "                \n",
        "        # Ensure that the tokenizer is present\n",
        "        tokenizer_path = os.path.join(WEIGHTS_PATH, 'tokenizer.model')\n",
        "        assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
        "        \n",
        "        # Ensure that the checkpoint is present\n",
        "        ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{model_variant}.ckpt')\n",
        "        assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\n",
        "\n",
        "        # loading model configuration\n",
        "        model_config = get_config_for_2b() if \"2b\" in model_variant else get_config_for_7b()\n",
        "        model_config.quant = \"quant\" in model_variant\n",
        "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
        "        \n",
        "        with _set_default_tensor_type(model_config.get_dtype()):\n",
        "            self.model = GemmaForCausalLM(model_config)\n",
        "            self.model.load_weights(ckpt_path)\n",
        "            self.model = self.model.to(self.device).eval()\n",
        "            \n",
        "        # agent args\n",
        "        self.formatter = PromptFormatter(system_prompt=system_prompt, \n",
        "                                         few_shot_examples=few_shot_examples)\n",
        "        self.round_num = 0\n",
        "        self.known_info = []\n",
        "        self.last_key_attribute = None\n",
        "        self.last_guess = None\n",
        "            \n",
        "    def _set_up_env(self, env):\n",
        "        \n",
        "        if env == 'kaggle':\n",
        "            print(\"Loading model in Kaggle, model weights will be searched within local directories.\")\n",
        "            \n",
        "            # kaggle configuration\n",
        "            KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
        "            if os.path.exists(KAGGLE_AGENT_PATH):\n",
        "                WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, f\"gemma/pytorch/{self.model_variant}/2\")\n",
        "            else:\n",
        "                WEIGHTS_PATH = f\"/kaggle/input/gemma/pytorch/{self.model_variant}/2\"\n",
        "                            \n",
        "        elif env == 'colab':\n",
        "            print(\"Loading model in Colab, starting from downloading the model weights.\")\n",
        "            \n",
        "            WEIGHTS_PATH = kagglehub.model_download(f'google/gemma/pyTorch/{self.model_variant}')\n",
        "        else:\n",
        "            raise ValueError(\"Argument 'env' should be in ['kaggle', 'colab']\")\n",
        "\n",
        "        return WEIGHTS_PATH\n",
        "    \n",
        "    def _parse_response(self, response : str):\n",
        "        '''\n",
        "            parse the response into a dictionary.\n",
        "            may contain three keys: key_attribute, question, guess and their value respectively.\n",
        "            e.g.: {\"key_attribute\": 'a country', \"question\": 'Is it a country?'} for a parsed asker response.\n",
        "        '''\n",
        "        pattern = re.compile(r'\\*\\*([^*]+)\\*\\*[^*\"]+\"([^\"]+)\"')\n",
        "        matches = pattern.findall(response.lower())\n",
        "        return matches\n",
        "    \n",
        "    def _format_prompt(self, obs):\n",
        "        # reset formatter\n",
        "        self.formatter.reset()\n",
        "        self.formatter.add_new_round('user', 'Remember how to use CoT as above, now let us play a new game from the begining.', True)\n",
        "        # add played rounds\n",
        "        rounds = interleave_unequal(obs.questions, obs.answers)\n",
        "        self.formatter.add_rounds(rounds, start_agent='user')\n",
        "        if obs.turnType == 'ask':\n",
        "            self.formatter.add_new_round('user', 'Now it is your turn to ask a question. please using chain of thought to construct a new question based on what you have known and your knowledge storage.  Do not forget to indicate key attribute, question with double asterisk (*) when it is needed, such as **Key Attribute** \"your attribute\" and **Question** \"your question\"', True)\n",
        "        elif obs.turnType == 'guess':\n",
        "            self.formatter.add_new_round('user', 'Now guess the keyword. Give your analysis with chain of thought and known information. And surround your guess with double asterisks (*) in the end, such as **Guess** \"your guess\"', True)\n",
        "        # start of model response\n",
        "        self.formatter.add_new_round('model', f'Given information: {str(self.known_info)}', False)                \n",
        "\n",
        "    def __call__(self, obs):\n",
        "        '''\n",
        "            Main function to interact with the model. \n",
        "                1. update known information based on the observation and round number\n",
        "                2. format the prompt with the observation\n",
        "                3. generate response from the model\n",
        "                4. parse the response and update information if needed\n",
        "        '''\n",
        "        \n",
        "        # Update known information\n",
        "        if self.round_num == 0:\n",
        "            # for the first round, randomly choose an attribute as initialization\n",
        "            initial_categories = ['person', 'thing', 'place']\n",
        "            self.key_attributes = random.choice(initial_categories)\n",
        "            self.else_categories = list(set(initial_categories) - set([self.key_attributes]))\n",
        "            response = f'Is it a {self.key_attributes}?'\n",
        "            return response\n",
        "        elif self.round_num == 1:\n",
        "            # if it is the second round, grab the answer and update known information\n",
        "            last_answer = obs.answers[-1]\n",
        "            if last_answer == 'yes':\n",
        "                self.known_info.append(f'a {self.key_attributes}')\n",
        "            else:\n",
        "                self.known_info.append(f'not a {self.key_attributes}')\n",
        "                self.known_info.append(f'either a {self.else_categories[0]} or a {self.else_categories[1]}')\n",
        "        else:\n",
        "            # other rounds, update known information\n",
        "            last_answer = obs.answers[-1]\n",
        "            if last_answer == 'yes':\n",
        "                self.known_info.append(f'a {self.key_attributes}')\n",
        "            else:\n",
        "                self.known_info.append(f'not a {self.key_attributes}')\n",
        "        self.round_num += 1\n",
        "                \n",
        "        # Update guessed keywords\n",
        "        self.known_info.append(f'not {self.last_guess}')\n",
        "        \n",
        "        # Formatting prompt with observations\n",
        "        self._format_prompt(obs)\n",
        "        prompt = str(self.formatter)\n",
        "        \n",
        "        # Getting response from LLM\n",
        "        response = self.model.generate(prompt, device=self.device, output_len=200)   \n",
        "        \n",
        "        # parse response and update information if needed\n",
        "        parse_dict = self._parse_response(response)\n",
        "        \n",
        "        # if in an ask turn, try to grab the key attribute from the response and update last key attribute for the next information updating\n",
        "        if obs.turnType == 'ask':\n",
        "            # successfully grab the key attribute from the response\n",
        "            if parse_dict['key_attribute']:\n",
        "                self.last_key_attribute = parse_dict['key_attribute']\n",
        "            # no key attribute parsed from the response, directly use question as key attribute\n",
        "            elif parse_dict['question']:\n",
        "                self.last_key_attribute  = parse_dict['question']\n",
        "                ret = parse_dict['question']\n",
        "                return ret\n",
        "            # worst case, there is neither key attribute nor question parsed from the response, randomly choose an attribute\n",
        "            else:\n",
        "                random_alphabet = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "                ret = f'is it started with alphabet {random_alphabet}?'\n",
        "                self.key_attributes = f'started with alphabet {random_alphabet}'\n",
        "                return ret\n",
        "            \n",
        "            # parsed response has question, use question as response\n",
        "            if parse_dict['question']:\n",
        "                ret = parse_dict['question']\n",
        "            # parsed response has no question but have key attribute, use key attribute as question\n",
        "            else:\n",
        "                ret = f'is it {parse_dict['key_attribute']}?'   \n",
        "        # if in a guess turn, try to grab the guess from the response and update last guess for the next information updating\n",
        "        elif obs.turnType == 'guess':\n",
        "            if parse_dict['guess']:\n",
        "                self.last_guess = parse_dict['guess']\n",
        "                ret = parse_dict['guess']\n",
        "            # if there is no guess parsed from the response, return empty string\n",
        "            else:\n",
        "                ret = ''\n",
        "        else:\n",
        "            raise ValueError('Invalid turnType.')\n",
        "        \n",
        "        return ret\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbSO_x4kR_2Y"
      },
      "source": [
        "### format your prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpnczT4kR7pZ"
      },
      "outputs": [],
      "source": [
        "# Chat templates\n",
        "USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
        "MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'\n",
        "\n",
        "prompt = (\n",
        "    USER_CHAT_TEMPLATE.format(\n",
        "        prompt='What is a good place for travel in the US?'\n",
        "    )\n",
        ")\n",
        "print('Chat prompt:\\n', prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqh86PQqSLTY"
      },
      "source": [
        "### Chat & Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6VrHQ8__6W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDSPbiluSQwt",
        "outputId": "1c6efae1-8668-4519-873a-e0411c747bdb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# generate with formatted prompts\n",
        "start = time.time()\n",
        "response_1 = model.generate(\n",
        "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
        "    device=device,\n",
        "    output_len=100,\n",
        ")\n",
        "print(f\"[response in {(time.time()-start):.2f}s]\\n{response_1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9loBFRC0TQab",
        "outputId": "4f3e3ff0-26e5-4812-f41d-863c378b218e"
      },
      "outputs": [],
      "source": [
        "# or directly talk to Gemma\n",
        "start = time.time()\n",
        "response_2 = model.generate(\n",
        "    'Write a poem about an llm writing a poem.',\n",
        "    device=device,\n",
        "    output_len=2000,\n",
        ")\n",
        "print(f\"[response in {(time.time()-start):.2f}s]\\n{response_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QElPuSuVxomJ"
      },
      "source": [
        "### applying formatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummy_gemma = lambda x: \"A4\"\n",
        "\n",
        "rounds = ['Q1', 'A1', 'Q2', 'A2', 'Q3', 'A3']\n",
        "\n",
        "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
        "\n",
        "few_shot_examples = [\n",
        "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
        "    \"Is it a person?\", \"**no**\",\n",
        "    \"Is is a place?\", \"**yes**\",\n",
        "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
        "    \"**France**\", \"Correct!\",\n",
        "]\n",
        "\n",
        "dummy_formatter = PromptFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
        "\n",
        "print(str(dummy_formatter), end='\\n\\n')\n",
        "dummy_formatter.add_existing_rounds(rounds, start_agent='user')\n",
        "print(str(dummy_formatter), end='\\n\\n')\n",
        "dummy_formatter.add_new_round('user', prompt='Q4', end_token=False)\n",
        "print(str(dummy_formatter), end='\\n\\n')\n",
        "dummy_formatter.add_end_token()\n",
        "print(str(dummy_formatter), end='\\n\\n')\n",
        "dummy_formatter.add_new_round('model', end_token=False)\n",
        "print(str(dummy_formatter), end='\\n\\n')\n",
        "dummy_formatter.add_new_round('model', prompt=dummy_gemma(0), end_token=True)\n",
        "print(str(dummy_formatter), end='\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('key attribute', 'a city'), ('question', 'is it a city?'), ('fuji', 'fuji')]\n"
          ]
        }
      ],
      "source": [
        "text = 'Given information: [\"It is not a country\", \"It is either a landmark or a city\"] \\\n",
        "Considerations: \\\n",
        "1. Type of Entity: Determining whether it is a city or a landmark can greatly narrow down our options. \\\n",
        "2. Geographical Location: If we can determine the continent, we can significantly reduce the number of possible cities or landmarks. \\\n",
        "3. Cultural/Historical Significance: Asking about cultural or historical significance can also be helpful, but it might be too broad at this stage. \\\n",
        "Reasoning: To effectively narrow it down, the most efficient next step is to determine if it is a city or a landmark. By determining if it is a city, we can immediately focus on known cities globally, which are fewer and more well-documented compared to all possible landmarks. \\\n",
        "If it is not a city, we can then focus on landmarks, which might require more specific questions but will also significantly narrow our scope. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is yes, we can then ask about the continent or specific features of the city. \\\n",
        "If the answer is no, we can ask about the type of landmark or its location. \\\n",
        "**Key Attribute** \"a city\"\\\n",
        "**Question** \"Is it a city?\" \\\n",
        "**Fuji** \"fuji\"'\n",
        "\n",
        "import re\n",
        "\n",
        "pattern = re.compile(r'\\*\\*([^*]+)\\*\\*[^*\"]+\"([^\"]+)\"')\n",
        "matches = pattern.findall(text.lower())\n",
        "print(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['is a C', 'is a B', 'is a A']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = ['is a A', 'is a B', 'is a C', 'is a A']\n",
        "list(set(x))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNYMmzQ5FvJCWOwpRNhb8xe",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a412d2972f340b49ad07708fc95e8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6de1853e4d924c3d887c3fedbfa7d184",
            "placeholder": "",
            "style": "IPY_MODEL_df33256681e1416bb90d761086e6f02a",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "0f0521043eae4862938eb1ccfb160dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f7960a68e12442ebf4e560d1206f268": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_612928d980594c66aa52f38d6037d4e5",
            "style": "IPY_MODEL_85a7112f258748938453b9f66379a229",
            "tooltip": ""
          }
        },
        "1c49d383eb5a45a99f87d5814a3a2ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52f425bb48984e01acee327c0de2c6b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f83051bc9ca40d3b5f1f1a66f1e95c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_61e3c070558e4bde80ba13ebc6dd64ae",
            "placeholder": "",
            "style": "IPY_MODEL_0f0521043eae4862938eb1ccfb160dfa",
            "value": ""
          }
        },
        "612928d980594c66aa52f38d6037d4e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61e3c070558e4bde80ba13ebc6dd64ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6de1853e4d924c3d887c3fedbfa7d184": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78354105a73c48a6942a19416ed861e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85a7112f258748938453b9f66379a229": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ae11770fb0ce446bbcf16c4ff3a23ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3eae9c63d1545b786d49525d43deef9",
            "placeholder": "",
            "style": "IPY_MODEL_52f425bb48984e01acee327c0de2c6b3",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "baed24fddd77489ebe9d5d1f7ac9f8c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "bba625c15982460e98611f8edbdaa565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae11770fb0ce446bbcf16c4ff3a23ec6",
              "IPY_MODEL_eae207bb982e48b4b7d16973ab46866c",
              "IPY_MODEL_5f83051bc9ca40d3b5f1f1a66f1e95c1",
              "IPY_MODEL_0f7960a68e12442ebf4e560d1206f268",
              "IPY_MODEL_0a412d2972f340b49ad07708fc95e8f0"
            ],
            "layout": "IPY_MODEL_baed24fddd77489ebe9d5d1f7ac9f8c7"
          }
        },
        "df33256681e1416bb90d761086e6f02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eae207bb982e48b4b7d16973ab46866c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "TextModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1c49d383eb5a45a99f87d5814a3a2ae3",
            "placeholder": "",
            "style": "IPY_MODEL_78354105a73c48a6942a19416ed861e9",
            "value": ""
          }
        },
        "f3eae9c63d1545b786d49525d43deef9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
