{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"},{"sourceId":11359,"sourceType":"modelInstanceVersion","modelInstanceId":8749,"modelId":3301}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":169.923583,"end_time":"2024-04-17T13:50:23.369773","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-17T13:47:33.44619","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. ","metadata":{}},{"cell_type":"code","source":"%%bash\ncd /kaggle/working\npip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\ngit clone https://github.com/google/gemma_pytorch.git > /dev/null\nmkdir /kaggle/working/submission/lib/gemma/\nmv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/","metadata":{"papermill":{"duration":13.257308,"end_time":"2024-04-17T13:47:49.310664","exception":false,"start_time":"2024-04-17T13:47:36.053356","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-28T20:43:12.708360Z","iopub.execute_input":"2024-07-28T20:43:12.709038Z","iopub.status.idle":"2024-07-28T20:43:26.144708Z","shell.execute_reply.started":"2024-07-28T20:43:12.709007Z","shell.execute_reply":"2024-07-28T20:43:26.143894Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Cloning into 'gemma_pytorch'...\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile submission/main.py\n\n# configering setting\nsys_prompt_guesser = f'You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. \\\nYou need to utilize your knowledge and reasoning skills to play a game of 20 questions. \\\nIn each round of the game, choose the binary attribute most likely to aid your deduction, such as \"there is a cross on the flag.\" \\\nUsing this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" \\\nAnd after your thorough thinking, you need to wrap some key point with double stars and their corresponding content with double quotation marks (such as **key point**: \"corresponding content\"). \\\nBaiscally it should follows the format similar to within 100 words: **Key attribute**:\"your chosen key attribute\"\\n**Question**:\"your formatted question regarding to the key attribute\"\\n**Reasoning**:\"your reasoning abput why this key attribute can provide you the most information gain\" \\\n Here are few examples of how to use Chain of thought:'\n\nsys_prompt_answerer = f'You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing. Here is an example of answering a yes-no question:'\n\nfew_shot_examples_answerer = [\n    \n'''\nThe keyword is Gym Mat in the category thing. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\n''',\n     \n'''\nIs it a person?\n''',\n     \n'''\n**no**\n''',\n     \n'''\nIs it a thing?\n''',\n     \n'''\n**yes**\n'''\n                              \n]\n\nfew_shot_examples_ask = [\n    \n# Round 1 | Keyword: Yangtze River\n'''\nKnown information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\n**Key attribute**: \"man-made structure\" \n**Question**: \"Is it a man-made structure?\" \n****Reasoning****: \nMan-made vs. Natural: This question helps to differentiate between natural landmarks (e.g., Grand Canyon) and man-made structures (e.g., Eiffel Tower). \nBroad Categories: Man-made structures include a wide range of possibilities (e.g., buildings, monuments), whereas cities are all man-made but represent a different category. \nEven Split: The distinction between natural and man-made provides an even split, maximizing information gain by effectively narrowing down the possibilities based on the answer.\n''',\n\n'''\nKnown information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\n**Key attribute**: \"in Asia\" \n**Question**: \"Is it located in Asia?\" \n****Reasoning****: \nGeographic Focus: Identifying the continent will significantly narrow down the possible natural landmarks. \nLarge Landmarks: Asia has several major natural landmarks (e.g., Mount Everest, Great Wall). \nEven Split: Given the global distribution of landmarks, this question will effectively split the remaining possibilities.\n''',\n\n'''\nKnown information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\n**Key attribute**: \"longest river in China\"\n**Question**: \"Is it the longest river in China?\"\n****Reasoning****: \nLength-Specific Focus: Knowing whether it is the longest river will help confirm or eliminate the Yangtze River.\nMajor Rivers: The Yangtze River is the longest river in China, followed by the Yellow River.\nEffective Split: This question will help confirm or deny one of the most prominent landmarks in China, providing a clear yes/no split.',\n''',\n\n\n# Round 2 | Keyword: Congo\n'''\nKnown information: [\"is a country\"]\n**Key attribute**: \"in Europe\"\n**Question**: \"Is this country in Europe?\"\n**Reasoning**: \nContinental Focus: Knowing the continent will help significantly narrow down the list of possible countries.\nEffective Reduction: Europe has many countries, so confirming or eliminating Europe will reduce the search space.\nBalanced Distribution: Countries are evenly distributed across continents, providing an effective yes/no split.\n''',\n\n'''\nKnown information: [\"is a country\", \"not in Europe\", \"not in Asia\", \"in Africa\", \"not in Northern Africa\", \"not in Eastern Africa\", \"not in Western Africa\", \"in Southern Africa\"]\n**Key attribute**: \"landlocked country\"\n**Question**: \"Is this country landlocked?\"\n**Reasoning**:\nTo further narrow down the possibilities within Southern Africa, I will focus on another well-known country in that region.\nGeographical Focus: Knowing whether the country is landlocked will significantly narrow down the possibilities within Southern Africa.\nEffective Reduction: There are several landlocked countries in Southern Africa, so confirming or eliminating this will help focus the search.\nBalanced Distribution: This provides a clear yes/no split, effectively narrowing down the search.\n''',\n\n'''\nKnown information: [\"is a country\", \"not in Europe\", \"not in Asia\", \"in Africa\", \"not in Northern Africa\", \"not in Eastern Africa\", \"not in Western Africa\", \"in Southern Africa\", \"not landlocked\", \"Portuguese is not an official language\"]\n**Key attribute**: \"borders the Indian Ocean\"\n**Question**: \"Does this country border the Indian Ocean?\"\n**Reasoning**: \nGeographic Focus: Knowing whether the country borders the Indian Ocean can help narrow down the options.\nEffective Reduction: There are only a few countries in Southern Africa that border the Indian Ocean.\nBalanced Distribution: This question provides a clear yes/no split, effectively narrowing down the search.\n ''',\n \n# round 3 | Keyword: Ryan \n'''\nKnown information: [\"not is a place\", \"is a person\"]\n**Key attribute**: \"a historical figure\"\n**Question**: \"Is this person a historical figure?\"\n**Reasoning**:\nTime Frame: Identifying whether the person is historical can significantly narrow down the possibilities.\nEffective Reduction: This helps focus on either historical or contemporary figures.\nBalanced Distribution: This question provides a clear yes/no split, guiding the search effectively.\n''',\n\n'''\nKnown information: [\"not is a place\", \"is a person\", \"is a historical figure\", \"is involved in the entertainment industry.\", \"is not primarily known for their work in music.\", \"is primarily known for their work as an actor.\", \"has not won an Academy Award (Oscar).\"]\n**Key attribute**: \"primarily known for television work\"\n**Question**: \"Is this person primarily known for their work in television?\"\n**Reasoning**:\nMedium Focus: Identifying whether the person is known for television can help narrow down the possibilities.\nEffective Reduction: This helps distinguish between actors primarily known for television versus those known for film.\nBalanced Distribution: This question provides a clear yes/no split, effectively guiding the search.\n''',\n\n'''\nKnown information: [\"not is a place\", \"is a person\", \"is a historical figure\", \"is involved in the entertainment industry.\", \"is not primarily known for their work in music.\", \"is primarily known for their work as an actor.\", \"has not won an Academy Award (Oscar).\", \"is not primarily known for their work in television.\", \"is primarily known for their work in action movies.\", \"is associated with a major action movie franchise.\"]\n**Key attribute**: \"known for science fiction action movies\"\n**Question**: \"Is this person known for their work in science fiction action movies?\"\n**Reasoning**:\nGenre Specificity: Identifying whether the person is known for science fiction action movies can help narrow down the possibilities.\nEffective Reduction: This helps distinguish between different types of action movie genres, such as sci-fi, fantasy, or military.\nBalanced Distribution: This question provides a clear yes/no split, guiding the search effectively.\n''',\n\n# round 4 | Keyword: Noosa\n'''\nKnown information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\"]\n**Key attribute**: \"a capital city\"\n**Question**: \"Is it a capital city?\"\n**Reasoning**:\nGeographic Focus: This question helps determine if the city is a capital, significantly narrowing the possibilities.\nEffective Reduction: If the answer is yes, it focuses on capital cities, eliminating all non-capital cities. If no, it eliminates all capital cities from consideration.\nContext Relevance: Knowing whether the city is a capital helps tailor subsequent questions to specific types of cities.\nBalanced Distribution: Capital cities are a smaller subset of cities, providing an effective yes/no split.\n''',\n\n# '''\n# Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere.\"]\n# **Key attribute**: \"in the Southern Hemisphere\"\n# **Question**: \"Is this city in the Southern Hemisphere?\"\n# **Reasoning**:\n# Hemispheric Focus: This question splits the world into two large, nearly equal parts, effectively narrowing down possibilities.\n# Effective Reduction: If the answer is yes, it narrows the focus to cities in the Southern Hemisphere, eliminating those in the Northern Hemisphere. If no, it focuses on Northern Hemisphere cities.\n# Balanced Distribution: The Earth is divided evenly by the equator, providing a clear yes/no split that maximizes information gain.\n# ''',\n\n\n'''\nKnown information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city.\"]\n**Key attribute**: \"a well-known tourist destination\"\n**Question**: \"Is this city a well-known tourist destination?\"\n**Reasoning**:\nTourism Focus: This question helps determine if the city is popular with tourists, which can narrow down the list of coastal cities.\nEffective Reduction: If the answer is yes, it focuses on well-known tourist cities, eliminating less-known cities. If no, it narrows down to less-touristic coastal cities.\nContext Relevance: Many of Australia's coastal cities are also popular tourist destinations, making this an important distinction.\nBalanced Distribution: Tourist versus non-tourist cities provide a clear yes/no split.\n''',\n\n# '''\n# Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city\", \"is a well-known tourist destination\", \"is not Gold Coast\", \"is not in New South Wales\", \"is in Queensland\", \"is not associated with the Great Barrier Reef\"]\n# **Key attribute**: \"popular for its beaches\"\n# **Question**: \"Is this city popular for its beaches?\"\n# **Reasoning**:\n# Tourism Focus: Identifying if the city is known for its beaches can help narrow down the possibilities.\n# Effective Reduction: If the answer is yes, it focuses on coastal cities popular for their beaches, eliminating inland cities or those known for other attractions. If no, it narrows down to other types of tourist destinations.\n# Context Relevance: Many coastal cities in Queensland are known for their beaches, making this a significant distinction.\n# Balanced Distribution: Coastal versus inland tourist destinations provide a clear yes/no split.\n# ''',\n\n'''\nKnown information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city\", \"is a well-known tourist destination\", \"is not Gold Coast\", \"is not in New South Wales\", \"is in Queensland\", \"is not associated with the Great Barrier Reef\", \"popular for its beaches\"]\n**Key attribute**: \"located on the Sunshine Coast\"\n**Question**: \"Is this city located on the Sunshine Coast?\"\n**Reasoning**:\nRegional Focus: Identifying if the city is on the Sunshine Coast can significantly narrow down the possibilities.\nEffective Reduction: If the answer is yes, it focuses on cities on the Sunshine Coast, eliminating cities in other coastal regions of Queensland. If no, it narrows down to other coastal areas.\nContext Relevance: The Sunshine Coast is known for its beach destinations, making this a significant distinction.\nBalanced Distribution: This question provides a clear yes/no split, effectively guiding the search.\n'''\n]\n\n# few_shot_examples_ask = [\n\n# 'Now give me an example of thinking with the Chain of Thought',\n                     \n# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\\n\\\n# **Key attribute**: \"man-made structure\" \\\n# **Question**: \"Is it a man-made structure?\" \\\n# **Reasoning**: Man-made vs. Natural: This question helps to differentiate between natural landmarks (e.g., Grand Canyon) and man-made structures (e.g., Eiffel Tower). \\\n# Broad Categories: Man-made structures include a wide range of possibilities (e.g., buildings, monuments), whereas cities are all man-made but represent a different category. \\\n# Even Split: The distinction between natural and man-made provides an even split, maximizing information gain by effectively narrowing down the possibilities based on the answer.',\n\n# 'no',\n\n# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\"]\\n\\\n# **Key attribute**: \"in North America\" \\\n# **Question**: \"Is it located in North America?\" \\\n# **Reasoning**: Continental Location: Knowing the continent helps narrow down the options to specific regions.\\\n# Significant Reduction: This can significantly reduce the number of potential landmarks.\\\n# Balanced Distribution: The world major landmarks are fairly evenly distributed across continents, providing a balanced yes/no split.',\n \n# 'no',\n\n# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\\n\\\n# **Key attribute**: \"in Asia\" \\\n# **Question**: \"Is it located in Asia?\" \\\n# **Reasoning**: Geographic Focus: Identifying the continent will significantly narrow down the possible natural landmarks. \\\n# Large Landmarks: Asia has several major natural landmarks (e.g., Mount Everest, Great Wall). \\\n# Even Split: Given the global distribution of landmarks, this question will effectively split the remaining possibilities.',\n\n# 'yes',\n \n# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\"]\\n\\\n# **Key attribute**: \"in China\" \\\n# **Question**: \"Is it located in China?\" \\\n# **Reasoning**: Country-Specific Focus: Knowing the specific country will help narrow down the landmark significantly. \\\n# Major Landmarks: China has several well-known natural landmarks (e.g., Yangtze River, Mount Everest on the border). \\\n# Even Split: Given the number of countries in Asia with significant natural landmarks, this question provides a balanced yes/no split.',\n\n# 'yes',\n \n# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\"]\\n\\\n#  **Key attribute**: \"a river\" \\\n# **Question**: \"Is it a river?\" \\\n# **Reasoning**: Type-Specific Focus: Knowing whether the natural landmark is a river will help narrow down the possibilities significantly. \\\n# Major Landmarks: China has several famous rivers (e.g., Yangtze River, Yellow River). \\\n# Even Split: This question provides a balanced yes/no split, effectively narrowing down the possibilities.',\n\n# 'yes',\n \n# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\\n\\\n# **Key attribute**: \"longest river in China\" \\\n# **Question**: \"Is it the longest river in China?\" \\\n# **Reasoning**: Length-Specific Focus: Knowing whether it is the longest river will help confirm or eliminate the Yangtze River. \\\n# Major Rivers: The Yangtze River is the longest river in China, followed by the Yellow River. \\\n# Effective Split: This question will help confirm or deny one of the most prominent landmarks in China, providing a clear yes/no split.',\n\n# 'yes',\n \n# 'It is the Yangtze River'\n\n# 'Correct!']\n\nfew_shot_examples_guess = [\n'Now give me an example of guessing with the reasoning',\n                     \n'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\\n\\\nGiven these clues, it is likely a natural landmark rather than a city or a man-made structure. An example of a well-known natural landmark is the Grand Canyon.\\\nGuess: \"Grand Canyon\"',\n\n'no',\n\n'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\"]\\n\\\nGiven these clues, it is likely a natural landmark outside North America. An example of a famous natural landmark outside North America is Mount Everest.\\\nGuess: \"Mount Everest\"',\n                          \n'no',\n \n'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\\n\\\nGiven these clues, it is likely a natural landmark in Asia. A famous natural landmark in Asia is the Himalayas.\\\nGuess: \"the Himalayas\"',\n\n'no',\n \n'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\"]\\n\\\nGiven these clues, it is likely a famous natural landmark in China. An example of such a landmark is the Mekong River.\\\nGuess: \"the Mekong River\"',\n\n'no',\n \n'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\"]\\n\\\nGiven these clues, it is likely a natural landmark in China that is not a mountain. An example of such a landmark could be the Tianchi Lake.\\\nGuess: \"the Tianchi Lake\"',\n\n'no',\n \n'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\\n\\\nGiven these clues, a very famous river in China is the Yellow River.\\\nGuess: \"the Yellow River\"',\n                          \n'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\", \"is the longest river in China\"]\\n\\\nGiven these clues, it should be the longest river in China which is the Yangtze River.\\\nGuess: \"the Yangtze River\"',\n                          \n'Correct!']\n\nVARIANT = '7b-it-quant'\nMACHINE_TYPE = 'cuda'\nENV = 'kaggle'\nagent = None\n\nfrom typing import Iterable\nimport itertools\n\n# Formatter Definition\nclass PromptFormatter:\n    \n    '''\n        formatter class to format the prompt text for the model. \n        A general idea is \n    '''\n    \n    _start_token = '<start_of_turn>'\n    _end_token = '<end_of_turn>'\n        \n    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None, sample_num = None):\n        self._system_prompt = system_prompt\n        self._few_shot_examples = few_shot_examples\n        self._template_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n        self._template_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n        self._all_prompt = ''\n        if sample_num is not None:\n          self.sample_num = sample_num\n        elif self._few_shot_examples is not None:\n          self.sample_num = len(few_shot_examples)\n        else:\n          self.sample_num = None\n\n        self.reset()\n\n    def __repr__(self):\n        return self._all_prompt\n        \n    def reset(self):\n        self._all_prompt = ''\n        # if system prompt is provided, add it to the prompt\n        if self._system_prompt is not None:\n            self._all_prompt += self._template_user.format(self._system_prompt)\n        # same for few shot examples\n        if self._few_shot_examples is not None:\n            # self.add_rounds(self._few_shot_examples, start_agent='user')\n            self.add_new_round('user', 'Here are some examples of analyzing with causal reasoning:', True)\n            for example in random.sample(self._few_shot_examples, self.sample_num):\n              self.add_new_round('model', example.strip(), True)\n            \n        \n    def add_user_round(self, user_prompt: str):\n        # add user round to the prompt\n        self._all_prompt += self._template_user.format(user_prompt)\n\n    def add_agent_round(self, model_response: str):\n        self._all_prompt += self._template_model.format(model_response)\n    \n    def add_rounds(self, rounds: Iterable, start_agent: str):\n        '''\n            Apply a sequence of rounds to the formatter, starting with the specified agent.\n        '''\n        formatters = [self.add_agent_round, self.add_user_round] if start_agent == 'model' else [self.add_user_round, self.add_agent_round] # here, self.model and self.user are functions definded above\n        formatters = itertools.cycle(formatters)\n        for fmt, round in zip(formatters, rounds):\n            fmt(round)\n        return self\n    \n    # def add_end_token(self):\n    #     self._all_prompt += f\"{self._end_token}\\n\"\n    \n    def add_new_round(self, player: str, prompt:str = None, end_token: bool = False):\n        self._all_prompt += f\"{self._start_token}{player}\\n\"\n        if prompt is not None:\n            self._all_prompt += f'{prompt}'\n        if end_token:\n            # self.add_end_token()\n            self._all_prompt += f\"{self._end_token}\\n\"\n    \n    def formate_MCQA(self):\n        raise NotImplementedError\n        \n# Agent Definition\nfrom ast import parse\nimport random\nclass GemmaAgent:\n    def __init__(self, model_variant, device='cuda:0', env=\"kaggle\", output_len=200, system_prompt=None, few_shot_examples=None, example_sample_num=None):\n        # model initialization\n        self.device = device\n        self.model_variant = model_variant\n        self.example_sample_num = example_sample_num\n        \n        WEIGHTS_PATH = self._set_up_env(env)\n                \n        # Ensure that the tokenizer is present\n        tokenizer_path = os.path.join(WEIGHTS_PATH, 'tokenizer.model')\n        assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n        \n        # Ensure that the checkpoint is present\n        ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{model_variant}.ckpt')\n        assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\n\n        # loading model configuration\n        model_config = get_config_for_2b() if \"2b\" in model_variant else get_config_for_7b()\n        model_config.quant = \"quant\" in model_variant\n        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n        \n        with _set_default_tensor_type(model_config.get_dtype()):\n            self.model = GemmaForCausalLM(model_config)\n            self.model.load_weights(ckpt_path)\n            self.model = self.model.to(self.device).eval()\n            \n        self.round_num = 0\n        self.known_info = []\n        self.last_key_attribute = None\n        self.last_guess = None\n        self.output_len = output_len\n        \n        # formatter arguments   \n        self.system_prompt = system_prompt\n        self.few_shot_examples = few_shot_examples\n\n    def _set_up_env(self, env):\n        \n        if env == 'kaggle':\n            print(\"Loading model in Kaggle, model weights will be searched within local directories.\")\n            \n            # kaggle configuration\n            KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n            if os.path.exists(KAGGLE_AGENT_PATH):\n                WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, f\"gemma/pytorch/{self.model_variant}/2\")\n            else:\n                WEIGHTS_PATH = f\"/kaggle/input/gemma/pytorch/{self.model_variant}/2\"\n                            \n        elif env == 'colab':\n            print(\"Loading model in Colab, starting from downloading the model weights.\")\n            \n            WEIGHTS_PATH = kagglehub.model_download(f'google/gemma/pyTorch/{self.model_variant}')\n        else:\n            raise ValueError(\"Argument 'env' should be in ['kaggle', 'colab']\")\n\n        return WEIGHTS_PATH\n\n    def _parse_response(self, response : str):\n      raise NotImplementedError\n  \n    def _format_prompt(self, obs):\n      raise NotImplementedError\n\n\nclass GemmaAgent_Answer(GemmaAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # set up few_shot_examples\n        self.formatter = PromptFormatter(system_prompt=self.system_prompt,\n                                        few_shot_examples=self.few_shot_examples,\n                                        sample_num=self.example_sample_num)\n\n    def _parse_response(self, response : str):\n        pattern = re.compile(r'\\*\\*([^*]+)\\*\\*', re.DOTALL)\n        matches = pattern.findall(response.lower())\n        \n        flag_no = 'no' in matches\n        flag_yes = 'yes' in matches\n        \n        # if both yes and no are in the response, randomly choose one\n        if flag_no and not flag_yes:\n            ret = 'no'\n        elif flag_yes and not flag_no:\n            ret = 'yes'\n        else:\n            ret = random.choice(['yes', 'no'])\n        \n        return ret\n\n    def _format_prompt(self, obs):\n        self.formatter.reset()\n        self.formatter.add_new_round('user', f'Let us play 20 Questions. You are playing the role of the Answerer. The keyword is {obs[\"keyword\"]} in the category {obs[\"category\"]}.', True)\n        # add played rounds\n        rounds = interleave_unequal(obs[\"questions\"], obs[\"answers\"])\n        self.formatter.add_rounds(rounds, start_agent='user')\n        self.formatter.add_new_round('user', \n                                     f'The keyword is {obs[\"keyword\"]} in the category {obs[\"category\"]}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.', \n                                     True)\n        # start of model response\n        self.formatter.add_new_round('model', None, False)\n        \n    def __call__(self, obs, cfg=None):\n        self._format_prompt(obs)\n        prompt = str(self.formatter)\n        response = self.model.generate(prompt, device=self.device, output_len=self.output_len)\n        print(f'\\nTurn Type: {obs[\"turnType\"]}\\nResponse: {response}')\n        ret = self._parse_response(response)\n        print(f\"{ret}\")\n        return ret\n\n\n\nclass GemmaAgent_Guesser(GemmaAgent):\n    def __init__(self, *args, **kwargs):\n        # set up general configurations\n        super().__init__(*args, **kwargs)\n        # set up few_shot_examples\n        self.few_shot_exmaples_ask = self.few_shot_examples[0]\n        self.few_shot_exmaples_guess = self.few_shot_examples[1]\n        # agent args\n        self.formatter = PromptFormatter(system_prompt=self.system_prompt, \n                                        few_shot_examples=self.few_shot_exmaples_guess,\n                                        sample_num=self.example_sample_num)\n\n        \n    \n    def _parse_response(self, response : str):\n        '''\n            parse the response into a dictionary.\n            may contain three keys: key_attribute, question, guess and their value respectively.\n            e.g.: {\"key_attribute\": 'a country', \"question\": 'Is it a country?'} for a parsed asker response.\n        '''\n        pattern = re.compile(r'\\*\\*([^*]+)\\*\\*:?\\s*(.*?)(?=\\*\\*|$)', re.DOTALL)\n        matches = pattern.findall(response.lower())\n        parse_dict = {'key attribute':None, 'question':None, 'guess':None}\n        for k, v in matches:\n          if ('attr' in k) and (parse_dict['key attribute'] == None):\n            parse_dict['key attribute'] = v\n          elif ('ques' in k) and (parse_dict['question'] == None):\n            parse_dict['question'] = v\n          elif ('guess' in k) and (parse_dict['guess'] == None):\n            parse_dict['guess'] = v\n\n        return parse_dict\n    \n    def _format_prompt(self, obs):\n        if obs[\"turnType\"] == 'ask':\n          self.formatter._few_shot_examples = self.few_shot_exmaples_ask\n        if obs[\"turnType\"] == 'guess':\n          self.formatter._few_shot_examples = self.few_shot_exmaples_guess\n        # reset formatter\n        self.formatter.reset()\n        self.formatter.add_new_round('user', 'Remember how to use CoT as above and the format of answering, now let us play a new game from the begining.', True)\n        # add played rounds\n        rounds = interleave_unequal(obs[\"questions\"], obs[\"answers\"])\n        self.formatter.add_rounds(rounds, start_agent='user')\n        if obs[\"turnType\"] == 'ask':\n            self.formatter.add_new_round('user', \n                                         'Now it is your turn to ask a question. Please construct a new question based on what you have known and your common sense with your reasoning. Do not forget to indicate key attribute, question with double asterisk (**?**) and their content with double quotation markers (\"?\"), such as **Key Attribute** \"your attribute\" and **Question** \"your question\"', \n                                         True)\n        elif obs[\"turnType\"] == 'guess':\n            self.formatter.add_new_round('user', \n                                         'Now guess the keyword based on your analysis with reasoning and known information. And surround a pointer of guess with double asterisks (**?**) and your guessed keyword with doublr quotation markers (\"?\") in the end, such as **Guess** \"your guess\"', \n                                         True)\n        # start of model response\n        self.formatter.add_new_round('model', f'Given information: {str(self.known_info)}', False)                \n\n    def __call__(self, obs, cfg=None):\n        '''\n            Main function to interact with the model. \n                1. update known information based on the observation and round number\n                2. format the prompt with the observation\n                3. generate response from the model\n                4. parse the response and update information if needed\n        '''\n        print(f\"=========== round: {self.round_num} ===========\")\n        self.round_num += 1\n        # Update known information\n        if self.round_num - 1 == 0:\n            # for the first round, randomly choose an attribute as initialization\n            initial_categories = ['person', 'thing', 'place']\n            self.last_key_attribute = random.choice(initial_categories)\n            self.else_categories = list(set(initial_categories) - set([self.last_key_attribute]))\n            response = f'Is it a {self.last_key_attribute}?'\n            round\n            return response\n        elif self.round_num - 1 == 1:\n            # if it is the second round, grab the answer and update known information\n            last_answer = obs[\"answers\"][-1]\n            if last_answer == 'yes':\n                self.known_info.append(f'a {self.last_key_attribute}')\n            else:\n                self.known_info.append(f'not a {self.last_key_attribute}')\n                self.known_info.append(f'either a {self.else_categories[0]} or a {self.else_categories[1]}')\n        else:\n            # other rounds, update known information\n            last_answer = obs['answers'][-1]\n            if last_answer == 'yes':\n                self.known_info.append(f'is {self.last_key_attribute}')\n            else:\n                self.known_info.append(f'is not {self.last_key_attribute}')\n                \n        # # Update guessed keywords\n        # self.known_info.append(f'keyword is not {self.last_guess}')\n        # self.known_info = list(set(self.known_info))\n\n        # print(f\"\\nkey attribute: {self.last_key_attribute}, guess: {self.last_guess}\")\n        \n        # Formatting prompt with observations\n        self._format_prompt(obs)\n        prompt = str(self.formatter)\n        # print(f\"\\nPrompt: {prompt}\")\n        # Getting response from LLM\n        response = self.model.generate(prompt, device=self.device, output_len=self.output_len)   \n        print(f'\\nTurn Type: {obs[\"turnType\"]}\\nResponse: {response}')\n        \n        # parse response and update information if needed\n        parse_dict = self._parse_response(response)\n        print(f'\\nParse_dict: {parse_dict}')\n        \n        # if in an ask turn, try to grab the key attribute from the response and update last key attribute for the next information updating\n        if obs[\"turnType\"] == 'ask':\n            # successfully grab the key attribute from the response\n            if parse_dict['key attribute']:\n                self.last_key_attribute = parse_dict['key attribute']\n            # no key attribute parsed from the response, directly use question as key attribute\n            elif parse_dict['question']:\n                self.last_key_attribute  = parse_dict['question']\n                ret = parse_dict['question']\n                return ret\n            # worst case, there is neither key attribute nor question parsed from the response, randomly choose an attribute\n            else:\n                random_alphabet = random.choice('abcdefghijklmnopqrstuvwxyz')\n                ret = f'is it started with alphabet {random_alphabet}?'\n                self.last_key_attribute = f'started with alphabet {random_alphabet}'\n                return ret\n            \n            # parsed response has question, use question as response\n            if parse_dict['question']:\n                ret = parse_dict['question']\n            # parsed response has no question but have key attribute, use key attribute as question\n            else:\n                \n                ret = f'is it {self.last_key_attribute}?'   \n        # if in a guess turn, try to grab the guess from the response and update last guess for the next information updating\n        elif obs[\"turnType\"] == 'guess':\n            if parse_dict['guess']:\n                self.last_guess = parse_dict['guess']\n                ret = parse_dict['guess']\n            # if there is no guess parsed from the response, return empty string\n            else:\n                ret = ''\n        else:\n            raise ValueError('Invalid turnType.')\n        \n        return ret\n    \n# API Alignment\ndef get_agent(name):\n    global agent\n    \n    if agent is None and name == 'questioner':\n        agent = GemmaAgent_Guesser(model_variant=VARIANT, \n                                   device=MACHINE_TYPE, \n                                   env=ENV, \n                                   output_len=100, \n                                   system_prompt=sys_prompt_guesser, \n                                   few_shot_examples=[few_shot_examples_ask, few_shot_examples_guess])\n    elif agent is None and name == 'answerer':\n        agent = GemmaAgent_Answer(model_variant=VARIANT, \n                                  device=MACHINE_TYPE, \n                                  env=ENV, \n                                  output_len=10, \n                                  system_prompt=sys_prompt_answerer,\n                                  few_shot_examples=few_shot_examples_answerer)\n    assert agent is not None, 'Agent not found!'\n    return agent\n\ndef get_fn(obs, config):\n    if obs['turnType'] == \"ask\":\n        response = get_agent('questioner')(obs)\n    elif obs['turnType'] == \"guess\":\n        response = get_agent('questioner')(obs)\n    elif obs['turnType'] == \"answer\":\n        response = get_agent('answerer')(obs)\n    if response is None or len(response) <= 1:\n        return \"yes\"\n    else:\n        return response","metadata":{"papermill":{"duration":0.016612,"end_time":"2024-04-17T13:47:49.33012","exception":false,"start_time":"2024-04-17T13:47:49.313508","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-28T20:43:26.146891Z","iopub.execute_input":"2024-07-28T20:43:26.147278Z","iopub.status.idle":"2024-07-28T20:43:26.178472Z","shell.execute_reply.started":"2024-07-28T20:43:26.147252Z","shell.execute_reply":"2024-07-28T20:43:26.177502Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Writing submission/main.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!apt install pigz pv > /dev/null","metadata":{"papermill":{"duration":5.560311,"end_time":"2024-04-17T13:47:54.892856","exception":false,"start_time":"2024-04-17T13:47:49.332545","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-28T20:43:26.180108Z","iopub.execute_input":"2024-07-28T20:43:26.180527Z","iopub.status.idle":"2024-07-28T20:43:32.393367Z","shell.execute_reply.started":"2024-07-28T20:43:26.180486Z","shell.execute_reply":"2024-07-28T20:43:32.392451Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2","metadata":{"papermill":{"duration":148.240766,"end_time":"2024-04-17T13:50:23.136669","exception":false,"start_time":"2024-04-17T13:47:54.895903","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-28T20:43:32.396044Z","iopub.execute_input":"2024-07-28T20:43:32.396769Z","iopub.status.idle":"2024-07-28T20:45:59.770914Z","shell.execute_reply.started":"2024-07-28T20:43:32.396728Z","shell.execute_reply":"2024-07-28T20:45:59.769840Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"6.84GiB 0:02:26 [47.9MiB/s] [                                            <=>   ]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Runing test","metadata":{}},{"cell_type":"code","source":"def simple_agent1(obs, cfg):\n    # if agent is guesser and turnType is \"ask\"\n    if obs.turnType == \"ask\": response = \"Is it a duck?\"\n    elif obs.turnType == \"guess\": response = \"duck\"\n    elif obs.turnType == \"answer\": response = \"no\"\n    return response\n\ndef simple_agent2(obs, cfg):\n    # if agent is guesser and turnType is \"ask\"\n    if obs.turnType == \"ask\": response = \"Is it a bird?\"\n    elif obs.turnType == \"guess\": response = \"bird\"\n    elif obs.turnType == \"answer\": response = \"no\"\n    return response\n\ndef simple_agent3(obs, cfg):\n    # if agent is guesser and turnType is \"ask\"\n    if obs.turnType == \"ask\": response = \"Is it a pig?\"\n    elif obs.turnType == \"guess\": response = \"pig\"\n    elif obs.turnType == \"answer\": response = \"no\"\n    return response\n\ndef simple_agent4(obs, cfg):\n    # if agent is guesser and turnType is \"ask\"\n    if obs.turnType == \"ask\": response = \"Is it a cow?\"\n    elif obs.turnType == \"guess\": response = \"cow\"\n    elif obs.turnType == \"answer\": response = \"no\"\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:45:59.772422Z","iopub.execute_input":"2024-07-28T20:45:59.772784Z","iopub.status.idle":"2024-07-28T20:45:59.781538Z","shell.execute_reply.started":"2024-07-28T20:45:59.772751Z","shell.execute_reply":"2024-07-28T20:45:59.780579Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import kaggle_environments\nenv = kaggle_environments.make(environment=\"llm_20_questions\")\ngame_output = env.run(agents=[\"/kaggle/working/submission/main.py\", \"/kaggle/working/submission/main.py\", simple_agent3, simple_agent4])\nenv.render(mode=\"ipython\", width=600, height=500)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T20:45:59.782771Z","iopub.execute_input":"2024-07-28T20:45:59.783099Z","iopub.status.idle":"2024-07-28T20:46:08.208594Z","shell.execute_reply.started":"2024-07-28T20:45:59.783069Z","shell.execute_reply":"2024-07-28T20:46:08.207223Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"No pygame installed, ignoring import\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkaggle_environments\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m kaggle_environments\u001b[38;5;241m.\u001b[39mmake(environment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_20_questions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m game_output \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/submission/main.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/submission/main.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimple_agent3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimple_agent4\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipython\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kaggle_environments/core.py:268\u001b[0m, in \u001b[0;36mEnvironment.run\u001b[0;34m(self, agents)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;129;01mand\u001b[39;00m perf_counter() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mrunTimeout:\n\u001b[1;32m    267\u001b[0m     actions, logs \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39mact()\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;129;01mand\u001b[39;00m perf_counter() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mrunTimeout:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DeadlineExceeded(\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperf_counter()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceeded the runTimeout of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mrunTimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kaggle_environments/core.py:232\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, actions, logs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m             action_state[index][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run_interpreter\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Max Steps reached. Mark ACTIVE/INACTIVE agents as DONE.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mobservation\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mepisodeSteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kaggle_environments/core.py:605\u001b[0m, in \u001b[0;36mEnvironment.__run_interpreter\u001b[0;34m(self, state, logs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc(file\u001b[38;5;241m=\u001b[39merr_buffer)\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;66;03m# Reraise e to ensure that the program exits\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     out \u001b[38;5;241m=\u001b[39m out_buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kaggle_environments/core.py:583\u001b[0m, in \u001b[0;36mEnvironment.__run_interpreter\u001b[0;34m(self, state, logs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     args \u001b[38;5;241m=\u001b[39m [structify(state), \u001b[38;5;28mself\u001b[39m]\n\u001b[0;32m--> 583\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m structify(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpreter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_argcount\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    585\u001b[0m     new_state[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mobservation\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps)\n\u001b[1;32m    588\u001b[0m     )\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_state):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kaggle_environments/envs/llm_20_questions/llm_20_questions.py:123\u001b[0m, in \u001b[0;36minterpreter\u001b[0;34m(state, env)\u001b[0m\n\u001b[1;32m    121\u001b[0m active1\u001b[38;5;241m.\u001b[39mobservation\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m=\u001b[39m category\n\u001b[1;32m    122\u001b[0m response \u001b[38;5;241m=\u001b[39m active1\u001b[38;5;241m.\u001b[39maction\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    124\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'lower'","output_type":"error"}]}]}