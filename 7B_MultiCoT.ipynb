{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tttequila/Kaggle_20Q/blob/main/7B_MultiCoT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqjWHE8-MQVl"
      },
      "source": [
        "Configuring your kaggle token, see more details in the [**Configure your API key**](https://ai.google.dev/gemma/docs/setup) section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENaMz5-35Ayi"
      },
      "source": [
        "### set up env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndGYpeKSFV5U",
        "outputId": "3758a08c-e82f-4e15-d6ba-fe4b3bf8d374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nNrgspAf9T8W"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mkdir ~/.kaggle\n",
        "# change the first path to your path of kaggle.json\n",
        "cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-l45FATYz0T",
        "outputId": "ace9a2aa-9c29-41a4-bda9-24d3127f41c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 8.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 56.7 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -q -U torch immutabledict sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/gemma_pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EUA6-EY6DCH",
        "outputId": "83b84928-f38b-41be-fa54-f3480fa6bd85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gemma_pytorch'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 231 (delta 83), reused 52 (delta 52), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (231/231), 2.17 MiB | 5.49 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lbfq9Ee5Ayo"
      },
      "source": [
        "### set up Gemma lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "18862047dbf449d997024f448e6efb6a",
            "7a4ca48a3f7143afa22af6bf426aba25",
            "61c02926d0844fc888ed3234b1216886",
            "287e5179952645049d1bbdb559f091ec",
            "285b517e3d5b425d802af69cb9f8f6e0",
            "c91c9a6d4f024671b645ab97b59c1079",
            "7878ac667a004b9ca393bf733c6dc898",
            "9919c31f6b7642beb65d0b3fe241c9ee",
            "dd7b45a3b52942e7a28b8289921f2bfa",
            "05debab25add4b61884d7bd2be02bb8b",
            "eb8afd98c9564236877a15c2fcf299bf",
            "a7b76063834042bf910c9fe8f7ef4890",
            "7cd00191ea2d4eafa0f6be9f6c93f01f",
            "5d0a9d9d5e274a5ea0f07983e60f5ef2",
            "48fed3e385a547eda31407b0f63ea8d7",
            "c6f82114df8842d69f7edfb2ab701f13",
            "77adf0c230db44fdbb608529e7fbbeed"
          ]
        },
        "id": "HhC3bL88SqDk",
        "outputId": "817d9990-1e33-4bbc-f73d-efe6a7319dba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18862047dbf449d997024f448e6efb6a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# login the kaggle (need to store you kaggle.json to your google dirve)\n",
        "import kagglehub\n",
        "kagglehub.login()\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"gemma_pytorch/gemma\")\n",
        "sys.path.append(\"gemma_pytorch\")\n",
        "import contextlib, torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gemma.config import get_config_for_7b, get_config_for_2b\n",
        "from gemma.model import GemmaForCausalLM, GemmaModel"
      ],
      "metadata": {
        "id": "vmThtLaQ8nwA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y10m95XaBeQ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gemma\n",
        "import itertools\n",
        "from typing import Iterable\n",
        "from typing import Any, List, Optional, Sequence, Tuple, Union\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bURsXpMG5Ayu"
      },
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def _set_default_tensor_type(dtype: torch.dtype):\n",
        "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
        "  torch.set_default_dtype(dtype)\n",
        "  yield\n",
        "  torch.set_default_dtype(torch.float)\n",
        "\n",
        "\n",
        "def interleave_unequal(x, y):\n",
        "    '''\n",
        "        Interleave two lists of unequal length.\n",
        "    '''\n",
        "    return [\n",
        "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLOddhGA5Ayv"
      },
      "source": [
        "### Building Agents\n",
        "\n",
        "<details>\n",
        "  <summary> model </summary>\n",
        "  \n",
        "  - `self.forward()`: getting next token and corresponding logits\n",
        "  - `self.generate()`: see summary below. May need to be rewriten if we wanna get the cumulative logits for the whole sentence  \n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary> model.generate() </summary>\n",
        "  \n",
        "  - **prompts** | `Union[str, Sequence[str]]`: Your prompts\n",
        "  - **device** | `Any`: Devices\n",
        "  - **output_len** | `int`: max output length\n",
        "  - **temperature** | `Union[float, None]`: temperature degree, controlling how variant its response could be  \n",
        "  - **top_p** | `float`:\n",
        "  - **top_k** | `int`:\n",
        "\n",
        "  regarding temperature, top_p and top_k, check this [link](https://blog.csdn.net/REfusing/article/details/137866583)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL_iy3Ai5Ayw"
      },
      "source": [
        "#### Define Formatter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AtsqJUJs5Ayw"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "import itertools\n",
        "\n",
        "class PromptFormatter:\n",
        "\n",
        "    '''\n",
        "        formatter class to format the prompt text for the model.\n",
        "        A general idea is\n",
        "    '''\n",
        "\n",
        "    _start_token = '<start_of_turn>'\n",
        "    _end_token = '<end_of_turn>'\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
        "        self._system_prompt = system_prompt\n",
        "        self._few_shot_examples = few_shot_examples\n",
        "        self._template_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
        "        self._template_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
        "        self._all_prompt = ''\n",
        "        self.reset()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self._all_prompt\n",
        "\n",
        "    def reset(self):\n",
        "        self._all_prompt = ''\n",
        "        # if system prompt is provided, add it to the prompt\n",
        "        if self._system_prompt:\n",
        "            self._all_prompt += self._template_user.format(self._system_prompt)\n",
        "        # same for few shot examples\n",
        "        if self._few_shot_examples:\n",
        "            self.add_rounds(self._few_shot_examples, start_agent='user')\n",
        "\n",
        "    def add_user_round(self, user_prompt: str):\n",
        "        # add user round to the prompt\n",
        "        self._all_prompt += self._template_user.format(user_prompt)\n",
        "\n",
        "    def add_agent_round(self, model_response: str):\n",
        "        self._all_prompt += self._template_model.format(model_response)\n",
        "\n",
        "    def add_rounds(self, rounds: Iterable, start_agent: str):\n",
        "        '''\n",
        "            Apply a sequence of rounds to the formatter, starting with the specified agent.\n",
        "        '''\n",
        "        formatters = [self.add_agent_round, self.add_user_round] if start_agent == 'model' else [self.add_user_round, self.add_agent_round] # here, self.model and self.user are functions definded above\n",
        "        formatters = itertools.cycle(formatters)\n",
        "        for fmt, round in zip(formatters, rounds):\n",
        "            fmt(round)\n",
        "        return self\n",
        "\n",
        "    # def add_end_token(self):\n",
        "    #     self._all_prompt += f\"{self._end_token}\\n\"\n",
        "\n",
        "    def add_new_round(self, player: str, prompt:str = None, end_token: bool = False):\n",
        "        self._all_prompt += f\"{self._start_token}{player}\\n\"\n",
        "        if prompt:\n",
        "            self._all_prompt += f'{prompt}'\n",
        "        if end_token:\n",
        "            # self.add_end_token()\n",
        "            self._all_prompt += f\"{self._end_token}\\n\"\n",
        "\n",
        "    def formate_MCQA(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "sys_prompt = f'You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. \\\n",
        "You need to utilize your knowledge and reasoning skills to play a game of 20 questions. \\\n",
        "In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"there is a cross on the flag.\" \\\n",
        "Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" \\\n",
        "And after your thorough thinking, you need to wrap some key point with double stars and their corresponding content with double quotation marks (such as **key point**: \"corresponding content\"). \\\n",
        "Baiscally it should follows the format similar to within 100 words: **Key attribute**:\"your chosen key attribute\"\\n**Question**:\"your formatted question regarding to the key attribute\"\\n**Reasoning**:\"your reasoning abput why this key attribute can provide you the most information gain\" \\\n",
        " Here are few examples of how to use Chain of thought:'\n",
        "\n",
        "few_shot_examples_ask = [\n",
        "\n",
        "'Now give me an example of thinking with the Chain of Thought',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\\n\\\n",
        "**Key attribute**: \"man-made structure\" \\\n",
        "**Question**: \"Is it a man-made structure?\" \\\n",
        "**Reasoning**: Man-made vs. Natural: This question helps to differentiate between natural landmarks (e.g., Grand Canyon) and man-made structures (e.g., Eiffel Tower). \\\n",
        "Broad Categories: Man-made structures include a wide range of possibilities (e.g., buildings, monuments), whereas cities are all man-made but represent a different category. \\\n",
        "Even Split: The distinction between natural and man-made provides an even split, maximizing information gain by effectively narrowing down the possibilities based on the answer.',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\"]\\n\\\n",
        "**Key attribute**: \"in North America\" \\\n",
        "**Question**: \"Is it located in North America?\" \\\n",
        "**Reasoning**: Continental Location: Knowing the continent helps narrow down the options to specific regions.\\\n",
        "Significant Reduction: This can significantly reduce the number of potential landmarks.\\\n",
        "Balanced Distribution: The world major landmarks are fairly evenly distributed across continents, providing a balanced yes/no split.',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\\n\\\n",
        "**Key attribute**: \"in Asia\" \\\n",
        "**Question**: \"Is it located in Asia?\" \\\n",
        "**Reasoning**: Geographic Focus: Identifying the continent will significantly narrow down the possible natural landmarks. \\\n",
        "Large Landmarks: Asia has several major natural landmarks (e.g., Mount Everest, Great Wall). \\\n",
        "Even Split: Given the global distribution of landmarks, this question will effectively split the remaining possibilities.',\n",
        "\n",
        "'yes',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\"]\\n\\\n",
        "**Key attribute**: \"in China\" \\\n",
        "**Question**: \"Is it located in China?\" \\\n",
        "**Reasoning**: Country-Specific Focus: Knowing the specific country will help narrow down the landmark significantly. \\\n",
        "Major Landmarks: China has several well-known natural landmarks (e.g., Yangtze River, Mount Everest on the border). \\\n",
        "Even Split: Given the number of countries in Asia with significant natural landmarks, this question provides a balanced yes/no split.',\n",
        "\n",
        "'yes',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\"]\\n\\\n",
        " **Key attribute**: \"a river\" \\\n",
        "**Question**: \"Is it a river?\" \\\n",
        "**Reasoning**: Type-Specific Focus: Knowing whether the natural landmark is a river will help narrow down the possibilities significantly. \\\n",
        "Major Landmarks: China has several famous rivers (e.g., Yangtze River, Yellow River). \\\n",
        "Even Split: This question provides a balanced yes/no split, effectively narrowing down the possibilities.',\n",
        "\n",
        "'yes',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\\n\\\n",
        "**Key attribute**: \"longest river in China\" \\\n",
        "**Question**: \"Is it the longest river in China?\" \\\n",
        "**Reasoning**: Length-Specific Focus: Knowing whether it is the longest river will help confirm or eliminate the Yangtze River. \\\n",
        "Major Rivers: The Yangtze River is the longest river in China, followed by the Yellow River. \\\n",
        "Effective Split: This question will help confirm or deny one of the most prominent landmarks in China, providing a clear yes/no split.',\n",
        "\n",
        "'yes',\n",
        "\n",
        "'It is the Yangtze River'\n",
        "\n",
        "'Correct!']\n",
        "\n",
        "few_shot_examples_guess = [\n",
        "'Now give me an example of guessing with the reasoning',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark rather than a city or a man-made structure. An example of a well-known natural landmark is the Grand Canyon.\\\n",
        "Guess: \"Grand Canyon\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark outside North America. An example of a famous natural landmark outside North America is Mount Everest.\\\n",
        "Guess: \"Mount Everest\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark in Asia. A famous natural landmark in Asia is the Himalayas.\\\n",
        "Guess: \"the Himalayas\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\"]\\n\\\n",
        "Given these clues, it is likely a famous natural landmark in China. An example of such a landmark is the Mekong River.\\\n",
        "Guess: \"the Mekong River\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark in China that is not a mountain. An example of such a landmark could be the Tianchi Lake.\\\n",
        "Guess: \"the Tianchi Lake\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\\n\\\n",
        "Given these clues, a very famous river in China is the Yellow River.\\\n",
        "Guess: \"the Yellow River\"',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\", \"is the longest river in China\"]\\n\\\n",
        "Given these clues, it should be the longest river in China which is the Yangtze River.\\\n",
        "Guess: \"the Yangtze River\"',\n",
        "\n",
        "'Correct!']\n",
        "\n",
        "# print(str(PromptFormatter(sys_prompt, few_shot_examples_ask)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5NRQGRB5Ayz"
      },
      "source": [
        "#### Define Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hIPcPhjD5Ay0"
      },
      "outputs": [],
      "source": [
        "from ast import parse\n",
        "import random\n",
        "\n",
        "class GemmaAgent_Guesser:\n",
        "\n",
        "    def __init__(self, model_variant, device='cuda:0', env=\"kaggle\", output_len=200, system_prompt=None, few_shot_examples=None,):\n",
        "        # model initialization\n",
        "        self.device = device\n",
        "        self.model_variant = model_variant\n",
        "        self.few_shot_exmaples_ask = few_shot_examples[0]\n",
        "        self.few_shot_exmaples_guess = few_shot_examples[1]\n",
        "\n",
        "        WEIGHTS_PATH = self._set_up_env(env)\n",
        "\n",
        "        # Ensure that the tokenizer is present\n",
        "        tokenizer_path = os.path.join(WEIGHTS_PATH, 'tokenizer.model')\n",
        "        assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
        "\n",
        "        # Ensure that the checkpoint is present\n",
        "        ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{model_variant}.ckpt')\n",
        "        assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\n",
        "\n",
        "        # loading model configuration\n",
        "        model_config = get_config_for_2b() if \"2b\" in model_variant else get_config_for_7b()\n",
        "        model_config.quant = \"quant\" in model_variant\n",
        "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
        "\n",
        "        with _set_default_tensor_type(model_config.get_dtype()):\n",
        "            self.model = GemmaForCausalLM(model_config)\n",
        "            self.model.load_weights(ckpt_path)\n",
        "            self.model = self.model.to(self.device).eval()\n",
        "\n",
        "        # agent args\n",
        "        self.formatter = PromptFormatter(system_prompt=system_prompt,\n",
        "                                         few_shot_examples=few_shot_examples[0])\n",
        "        self.round_num = 0\n",
        "        self.known_info = []\n",
        "        self.last_key_attribute = None\n",
        "        self.last_guess = None\n",
        "        self.output_len = output_len\n",
        "\n",
        "    def _set_up_env(self, env):\n",
        "\n",
        "        if env == 'kaggle':\n",
        "            print(\"Loading model in Kaggle, model weights will be searched within local directories.\")\n",
        "\n",
        "            # kaggle configuration\n",
        "            KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
        "            if os.path.exists(KAGGLE_AGENT_PATH):\n",
        "                WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, f\"gemma/pytorch/{self.model_variant}/2\")\n",
        "            else:\n",
        "                WEIGHTS_PATH = f\"/kaggle/input/gemma/pytorch/{self.model_variant}/2\"\n",
        "\n",
        "        elif env == 'colab':\n",
        "            print(\"Loading model in Colab, starting from downloading the model weights.\")\n",
        "\n",
        "            WEIGHTS_PATH = kagglehub.model_download(f'google/gemma/pyTorch/{self.model_variant}')\n",
        "        else:\n",
        "            raise ValueError(\"Argument 'env' should be in ['kaggle', 'colab']\")\n",
        "\n",
        "        return WEIGHTS_PATH\n",
        "\n",
        "    def _parse_response(self, response : str):\n",
        "        '''\n",
        "            parse the response into a dictionary.\n",
        "            may contain three keys: key_attribute, question, guess and their value respectively.\n",
        "            e.g.: {\"key_attribute\": 'a country', \"question\": 'Is it a country?'} for a parsed asker response.\n",
        "        '''\n",
        "        pattern = re.compile(r'\\*\\*([^*]+)\\*\\*[^*\"]+\"([^\"]+)\"')\n",
        "        matches = pattern.findall(response.lower())\n",
        "        parse_dict = {'key attribute':None, 'question':None, 'guess':None}\n",
        "        for k, v in matches:\n",
        "          if 'attr' in k:\n",
        "            parse_dict['key attribute'] = v\n",
        "          elif 'ques' in k:\n",
        "            parse_dict['question'] = v\n",
        "          elif 'guess' in k:\n",
        "            parse_dict['guess'] = v\n",
        "\n",
        "        return parse_dict\n",
        "\n",
        "    def _format_prompt(self, obs):\n",
        "        if obs[\"turnType\"] == 'ask':\n",
        "          self.formatter._few_shot_examples = self.few_shot_exmaples_ask\n",
        "        if obs[\"turnType\"] == 'guess':\n",
        "          self.formatter._few_shot_examples = self.few_shot_exmaples_guess\n",
        "        # reset formatter\n",
        "        self.formatter.reset()\n",
        "        self.formatter.add_new_round('user', 'Remember how to use CoT as above and the format of answering, now let us play a new game from the begining.', True)\n",
        "        # add played rounds\n",
        "        rounds = interleave_unequal(obs[\"questions\"], obs[\"answers\"])\n",
        "        self.formatter.add_rounds(rounds, start_agent='user')\n",
        "        if obs[\"turnType\"] == 'ask':\n",
        "            self.formatter.add_new_round('user',\n",
        "                                         'Now it is your turn to ask a question. Please construct a new question based on what you have known and your common sense with your reasoning. Do not forget to indicate key attribute, question with double asterisk (**?**) and their content with double quotation markers (\"?\"), such as **Key Attribute** \"your attribute\" and **Question** \"your question\"',\n",
        "                                         True)\n",
        "        elif obs[\"turnType\"] == 'guess':\n",
        "            self.formatter.add_new_round('user',\n",
        "                                         'Now guess the keyword based on your analysis with reasoning and known information. And surround a pointer of guess with double asterisks (**?**) and your guessed keyword with doublr quotation markers (\"?\") in the end, such as **Guess** \"your guess\"',\n",
        "                                         True)\n",
        "        # start of model response\n",
        "        self.formatter.add_new_round('model', f'Given information: {str(self.known_info)}', False)\n",
        "\n",
        "    def __call__(self, obs, cfg=None):\n",
        "        '''\n",
        "            Main function to interact with the model.\n",
        "                1. update known information based on the observation and round number\n",
        "                2. format the prompt with the observation\n",
        "                3. generate response from the model\n",
        "                4. parse the response and update information if needed\n",
        "        '''\n",
        "        print(f\"=========== round: {self.round_num} ===========\")\n",
        "        self.round_num += 1\n",
        "        # Update known information\n",
        "        if self.round_num - 1 == 0:\n",
        "            # for the first round, randomly choose an attribute as initialization\n",
        "            initial_categories = ['person', 'thing', 'place']\n",
        "            self.last_key_attribute = random.choice(initial_categories)\n",
        "            self.else_categories = list(set(initial_categories) - set([self.last_key_attribute]))\n",
        "            response = f'Is it a {self.last_key_attribute}?'\n",
        "            round\n",
        "            return response\n",
        "        elif self.round_num - 1 == 1:\n",
        "            # if it is the second round, grab the answer and update known information\n",
        "            last_answer = obs[\"answers\"][-1]\n",
        "            if last_answer == 'yes':\n",
        "                self.known_info.append(f'a {self.last_key_attribute}')\n",
        "            else:\n",
        "                self.known_info.append(f'not a {self.last_key_attribute}')\n",
        "                self.known_info.append(f'either a {self.else_categories[0]} or a {self.else_categories[1]}')\n",
        "        else:\n",
        "            # other rounds, update known information\n",
        "            last_answer = obs['answers'][-1]\n",
        "            if last_answer == 'yes':\n",
        "                self.known_info.append(f'is {self.last_key_attribute}')\n",
        "            else:\n",
        "                self.known_info.append(f'is not {self.last_key_attribute}')\n",
        "\n",
        "        # # Update guessed keywords\n",
        "        # self.known_info.append(f'keyword is not {self.last_guess}')\n",
        "        # self.known_info = list(set(self.known_info))\n",
        "\n",
        "        print(f\"\\nkey attribute: {self.last_key_attribute}, guess: {self.last_guess}\")\n",
        "\n",
        "        # Formatting prompt with observations\n",
        "        self._format_prompt(obs)\n",
        "        prompt = str(self.formatter)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        # Getting response from LLM\n",
        "        response = self.model.generate(prompt, device=self.device, output_len=self.output_len)\n",
        "        print(f'\\nTurn Type: {obs[\"turnType\"]}\\nResponse: {response}')\n",
        "\n",
        "        # parse response and update information if needed\n",
        "        parse_dict = self._parse_response(response)\n",
        "        print(f'\\nParse_dict: {parse_dict}')\n",
        "\n",
        "        # if in an ask turn, try to grab the key attribute from the response and update last key attribute for the next information updating\n",
        "        if obs[\"turnType\"] == 'ask':\n",
        "            # successfully grab the key attribute from the response\n",
        "            if parse_dict['key attribute']:\n",
        "                self.last_key_attribute = parse_dict['key attribute']\n",
        "            # no key attribute parsed from the response, directly use question as key attribute\n",
        "            elif parse_dict['question']:\n",
        "                self.last_key_attribute  = parse_dict['question']\n",
        "                ret = parse_dict['question']\n",
        "                return ret\n",
        "            # worst case, there is neither key attribute nor question parsed from the response, randomly choose an attribute\n",
        "            else:\n",
        "                random_alphabet = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "                ret = f'is it started with alphabet {random_alphabet}?'\n",
        "                self.last_key_attribute = f'started with alphabet {random_alphabet}'\n",
        "                return ret\n",
        "\n",
        "            # parsed response has question, use question as response\n",
        "            if parse_dict['question']:\n",
        "                ret = parse_dict['question']\n",
        "            # parsed response has no question but have key attribute, use key attribute as question\n",
        "            else:\n",
        "\n",
        "                ret = f'is it {self.last_key_attribute}?'\n",
        "        # if in a guess turn, try to grab the guess from the response and update last guess for the next information updating\n",
        "        elif obs[\"turnType\"] == 'guess':\n",
        "            if parse_dict['guess']:\n",
        "                self.last_guess = parse_dict['guess']\n",
        "                ret = parse_dict['guess']\n",
        "            # if there is no guess parsed from the response, return empty string\n",
        "            else:\n",
        "                ret = ''\n",
        "        else:\n",
        "            raise ValueError('Invalid turnType.')\n",
        "\n",
        "        return ret\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug"
      ],
      "metadata": {
        "id": "KX1Zbm8X5IUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install kaggle_environments\n",
        "import kaggle_environments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0D3B9ks51xa",
        "outputId": "6db3b6ec-8c8e-4215-a14f-08488349971d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.1/134.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for vec-noise (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy agent\n",
        "def simple_agent1(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs['turnType'] == \"ask\": response = \"Is it a duck?\"\n",
        "    elif obs.turnType == \"guess\": response = \"duck\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response\n",
        "\n",
        "def simple_agent2(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs.turnType == \"ask\": response = \"Is it a bird?\"\n",
        "    elif obs.turnType == \"guess\": response = \"bird\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response\n",
        "\n",
        "def simple_agent3(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
        "    elif obs.turnType == \"guess\": response = \"pig\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response\n",
        "\n",
        "def simple_agent4(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs.turnType == \"ask\": response = \"Is it a cow?\"\n",
        "    elif obs.turnType == \"guess\": response = \"cow\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6ozTUpm5MUw",
        "outputId": "c9211559-c52d-4426-a775-ef332b1c664e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **IMPORTANT:** Define agent as a global so you only have to load\n",
        "# the agent you need. Loading both will likely lead to OOM.\n",
        "# del agent\n",
        "agent = None\n",
        "print(agent)\n",
        "\n",
        "# def get_agent(name: str):\n",
        "#     global agent\n",
        "\n",
        "#     if agent is None and name == 'questioner':\n",
        "#         agent = GemmaAgent_Guesser(VARIANT, MACHINE_TYPE, 'colab',\n",
        "#                                    sys_prompt, few_shot_examples)\n",
        "#     elif agent is None and name == 'answerer':\n",
        "#         raise NotImplementedError\n",
        "#     assert agent is not None, \"Agent not initialized.\"\n",
        "\n",
        "#     return agent\n",
        "\n",
        "\n",
        "# def agent_fn(obs, cfg):\n",
        "#     if obs.turnType == \"ask\":\n",
        "#         response = get_agent('questioner')(obs)\n",
        "#     elif obs.turnType == \"guess\":\n",
        "#         response = get_agent('questioner')(obs)\n",
        "#     elif obs.turnType == \"answer\":\n",
        "#         response = get_agent('answerer')(obs)\n",
        "#     if response is None or len(response) <= 1:\n",
        "#         return \"yes\"\n",
        "#     else:\n",
        "#         return response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqaOSuKrEo5h",
        "outputId": "8502a0f4-a40e-41ff-8df9-6d22222e4a98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug_config = {'episodeSteps': 10,     # initial step plus 3 steps per round (ask/answer/guess)\n",
        "                'actTimeout': 5,       # agent time per round in seconds; default is 60\n",
        "                'runTimeout': 60,      # max time for the episode in seconds; default is 1200\n",
        "                'agentTimeout': 3600}  # obsolete field; default is 3600\n",
        "env = kaggle_environments.make(\"llm_20_questions\", debug_config, debug=True)\n",
        "print(f'the key word is: {kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword}\\nwith alts as {kaggle_environments.envs.llm_20_questions.llm_20_questions.alts}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vJhIFse_rnX",
        "outputId": "b923943c-4bec-4da4-c502-b49a33eef408"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the key word is: Electrical outlet\n",
            "with alts as []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Choose variant and machine type\n",
        "VARIANT = '7b-it-quant'\n",
        "MACHINE_TYPE = 'cuda'\n",
        "agent = None\n"
      ],
      "metadata": {
        "id": "ti92oAf6CrEb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = GemmaAgent_Guesser(VARIANT, MACHINE_TYPE, 'colab', 150, sys_prompt, [few_shot_examples_ask, few_shot_examples_guess])\n",
        "# agent.output_len = 150\n",
        "agent.formatter._sys_prompt = sys_prompt\n",
        "agent.formatter._few_shot_examples = few_shot_examples_ask\n",
        "agent.round_num = 4\n",
        "agent.last_key_attribute = 'a person'\n",
        "agent.last_guess = 'pig'\n",
        "agent.known_info = ['not a place', 'either a person or a thing', 'not a thing']\n",
        "# print(str(agent.formatter))\n",
        "obs = {'remainingOverageTime': 300, 'questions': ['Is it a place?', 'Is it a thing?', 'Is it a person?'], 'guesses': ['pig', 'pig', 'pig'], 'answers': ['no', 'no', 'yes'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}\n",
        "start = time.time()\n",
        "agent(obs)\n",
        "print(f\"TIME: {(time.time()-start):.4f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygzpOabZM7ub",
        "outputId": "44de3089-b751-467d-9c5a-c329e0e6af50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model in Colab, starting from downloading the model weights.\n",
            "=========== round: 4 ===========\n",
            "\n",
            "key attribute: a person, guess: pig\n",
            "\n",
            "Prompt: <start_of_turn>user\n",
            "You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. You need to utilize your knowledge and reasoning skills to play a game of 20 questions. In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"there is a cross on the flag.\" Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" And after your thorough thinking, you need to wrap some key point with double stars and their corresponding content with double quotation marks (such as **key point**: \"corresponding content\"). Baiscally it should follows the format similar to within 100 words: **Key attribute**:\"your chosen key attribute\"\n",
            "**Question**:\"your formatted question regarding to the key attribute\"\n",
            "**Reasoning**:\"your reasoning abput why this key attribute can provide you the most information gain\"  Here are few examples of how to use Chain of thought:<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Now give me an example of thinking with the Chain of Thought<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\n",
            "**Key attribute**: \"man-made structure\" **Question**: \"Is it a man-made structure?\" **Reasoning**: Man-made vs. Natural: This question helps to differentiate between natural landmarks (e.g., Grand Canyon) and man-made structures (e.g., Eiffel Tower). Broad Categories: Man-made structures include a wide range of possibilities (e.g., buildings, monuments), whereas cities are all man-made but represent a different category. Even Split: The distinction between natural and man-made provides an even split, maximizing information gain by effectively narrowing down the possibilities based on the answer.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "no<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\"]\n",
            "**Key attribute**: \"in North America\" **Question**: \"Is it located in North America?\" **Reasoning**: Continental Location: Knowing the continent helps narrow down the options to specific regions.Significant Reduction: This can significantly reduce the number of potential landmarks.Balanced Distribution: The world major landmarks are fairly evenly distributed across continents, providing a balanced yes/no split.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "no<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\n",
            "**Key attribute**: \"in Asia\" **Question**: \"Is it located in Asia?\" **Reasoning**: Geographic Focus: Identifying the continent will significantly narrow down the possible natural landmarks. Large Landmarks: Asia has several major natural landmarks (e.g., Mount Everest, Great Wall). Even Split: Given the global distribution of landmarks, this question will effectively split the remaining possibilities.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "yes<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\"]\n",
            "**Key attribute**: \"in China\" **Question**: \"Is it located in China?\" **Reasoning**: Country-Specific Focus: Knowing the specific country will help narrow down the landmark significantly. Major Landmarks: China has several well-known natural landmarks (e.g., Yangtze River, Mount Everest on the border). Even Split: Given the number of countries in Asia with significant natural landmarks, this question provides a balanced yes/no split.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "yes<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\"]\n",
            " **Key attribute**: \"a river\" **Question**: \"Is it a river?\" **Reasoning**: Type-Specific Focus: Knowing whether the natural landmark is a river will help narrow down the possibilities significantly. Major Landmarks: China has several famous rivers (e.g., Yangtze River, Yellow River). Even Split: This question provides a balanced yes/no split, effectively narrowing down the possibilities.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "yes<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\n",
            "**Key attribute**: \"longest river in China\" **Question**: \"Is it the longest river in China?\" **Reasoning**: Length-Specific Focus: Knowing whether it is the longest river will help confirm or eliminate the Yangtze River. Major Rivers: The Yangtze River is the longest river in China, followed by the Yellow River. Effective Split: This question will help confirm or deny one of the most prominent landmarks in China, providing a clear yes/no split.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "yes<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It is the Yangtze RiverCorrect!<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Remember how to use CoT as above and the format of answering, now let us play a new game from the begining.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Is it a place?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "no<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Is it a thing?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "no<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Is it a person?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "yes<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Now it is your turn to ask a question. Please construct a new question based on what you have known and your common sense with your reasoning. Do not forget to indicate key attribute, question with double asterisk (**?**) and their content with double quotation markers (\"?\"), such as **Key Attribute** \"your attribute\" and **Question** \"your question\"<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: ['not a place', 'either a person or a thing', 'not a thing', 'is a person']\n",
            "\n",
            "Turn Type: ask\n",
            "Response: \n",
            "\n",
            "**Key attribute**: \"nationality\"\n",
            "\n",
            "**Question**: \"Is the person a Chinese citizen?\"\n",
            "\n",
            "**Reasoning**: Nationality provides the most information gain because the person's nationality significantly narrows down the possibilities. China has a large population of people, but narrowed down to a specific nationality, the number of possibilities further reduces.\n",
            "\n",
            "Parse_dict: {'key attribute': 'nationality', 'question': 'is the person a chinese citizen?', 'guess': None}\n",
            "TIME: 68.9725s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "game_output = env.run(agents=[simple_agent1,simple_agent2, simple_agent3, simple_agent4])\n",
        "print(game_output[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atPfPu0bD0rU",
        "outputId": "1aaa2011-e7a9-49e9-ad9f-ea97cf851e76"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'action': 'duck', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'step': 9, 'questions': ['Is it a duck?', 'Is it a duck?', 'Is it a duck?'], 'guesses': ['duck', 'duck', 'duck'], 'answers': ['no', 'no', 'no'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}, 'status': 'DONE'}, {'action': '', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'questions': ['Is it a duck?', 'Is it a duck?', 'Is it a duck?'], 'guesses': ['duck', 'duck', 'duck'], 'answers': ['no', 'no', 'no'], 'role': 'answerer', 'turnType': 'answer', 'keyword': 'Electrical outlet', 'category': 'things'}, 'status': 'DONE'}, {'action': 'pig', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'questions': ['Is it a pig?', 'Is it a pig?', 'Is it a pig?'], 'guesses': ['pig', 'pig', 'pig'], 'answers': ['no', 'no', 'no'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}, 'status': 'DONE'}, {'action': '', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'questions': ['Is it a pig?', 'Is it a pig?', 'Is it a pig?'], 'guesses': ['pig', 'pig', 'pig'], 'answers': ['no', 'no', 'no'], 'role': 'answerer', 'turnType': 'answer', 'keyword': 'Electrical outlet', 'category': 'things'}, 'status': 'DONE'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# env.render(mode=\"ipython\", width=600, height=500)"
      ],
      "metadata": {
        "id": "QwifCN55FVMd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Recordings\n",
        "\n",
        "- 1 question deduction,**150** output length\n",
        "  - 72s❌, no indication of answer❌\n",
        "- 3 question deduction,**150** output length\n",
        "  - 72s❌, no indication of answer❌\n",
        "- **6** question deduction,**150** output length\n",
        "  - 83s❌, completed answer and indicator✅\n",
        "- 3 question deduction,**100** output length\n",
        "  - 52s✅, incompleted response❌\n",
        "  (maybe we can limit the length of reasoning within 100 character)\n",
        "- **6** question deduction,**100** output length\n",
        "  - 59s✅, incompleted response❌\n",
        "\n",
        "---\n",
        "\n",
        "**👇 Try to downsize the length of CoT example. (better few shot cases and system prompt)**\n",
        "\n",
        "---\n",
        "\n",
        "- **6** question deduction, **100** output length\n",
        "  - 51s✅, completed and fair reasoning✅\n",
        "- **6** question deduction, **150** output length\n",
        "  - 68s❌, similar result to above✅\n",
        "\n",
        "---\n",
        "\n",
        "**👇 Apparent path dependence, maybe we need multiple deduction of different cases instead of all steps within one game**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "20AcwLvPvRsZ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18862047dbf449d997024f448e6efb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a4ca48a3f7143afa22af6bf426aba25",
              "IPY_MODEL_61c02926d0844fc888ed3234b1216886",
              "IPY_MODEL_287e5179952645049d1bbdb559f091ec",
              "IPY_MODEL_285b517e3d5b425d802af69cb9f8f6e0",
              "IPY_MODEL_c91c9a6d4f024671b645ab97b59c1079"
            ],
            "layout": "IPY_MODEL_7878ac667a004b9ca393bf733c6dc898"
          }
        },
        "7a4ca48a3f7143afa22af6bf426aba25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9919c31f6b7642beb65d0b3fe241c9ee",
            "placeholder": "​",
            "style": "IPY_MODEL_dd7b45a3b52942e7a28b8289921f2bfa",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "61c02926d0844fc888ed3234b1216886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_05debab25add4b61884d7bd2be02bb8b",
            "placeholder": "​",
            "style": "IPY_MODEL_eb8afd98c9564236877a15c2fcf299bf",
            "value": ""
          }
        },
        "287e5179952645049d1bbdb559f091ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a7b76063834042bf910c9fe8f7ef4890",
            "placeholder": "​",
            "style": "IPY_MODEL_7cd00191ea2d4eafa0f6be9f6c93f01f",
            "value": ""
          }
        },
        "285b517e3d5b425d802af69cb9f8f6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5d0a9d9d5e274a5ea0f07983e60f5ef2",
            "style": "IPY_MODEL_48fed3e385a547eda31407b0f63ea8d7",
            "tooltip": ""
          }
        },
        "c91c9a6d4f024671b645ab97b59c1079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6f82114df8842d69f7edfb2ab701f13",
            "placeholder": "​",
            "style": "IPY_MODEL_77adf0c230db44fdbb608529e7fbbeed",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "7878ac667a004b9ca393bf733c6dc898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9919c31f6b7642beb65d0b3fe241c9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd7b45a3b52942e7a28b8289921f2bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05debab25add4b61884d7bd2be02bb8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb8afd98c9564236877a15c2fcf299bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7b76063834042bf910c9fe8f7ef4890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd00191ea2d4eafa0f6be9f6c93f01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d0a9d9d5e274a5ea0f07983e60f5ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48fed3e385a547eda31407b0f63ea8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c6f82114df8842d69f7edfb2ab701f13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77adf0c230db44fdbb608529e7fbbeed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}