{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tttequila/Kaggle_20Q/blob/main/7B_mul_CoT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqjWHE8-MQVl"
      },
      "source": [
        "Configuring your kaggle token, see more details in the [**Configure your API key**](https://ai.google.dev/gemma/docs/setup) section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWh686-L0PG3"
      },
      "source": [
        "### set up env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndGYpeKSFV5U",
        "outputId": "8c3542a5-cbcc-4209-db0f-65d49ec6339d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nNrgspAf9T8W"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mkdir ~/.kaggle\n",
        "# change the first path to your path of kaggle.json\n",
        "cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-l45FATYz0T",
        "outputId": "b4d99847-9bf9-4e41-fe98-e860efc6eb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 9.1 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 55.2 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -q -U torch immutabledict sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/gemma_pytorch.git"
      ],
      "metadata": {
        "id": "HvMxjVQbJWH2",
        "outputId": "d4151b83-0559-4649-fbc3-caf4e9024625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gemma_pytorch'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 231 (delta 83), reused 52 (delta 52), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (231/231), 2.17 MiB | 21.59 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvOmFBjG0PG6"
      },
      "source": [
        "### set up Gemma lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "ed83ccd4ac704ef89c2ded236ad4f9b3",
            "d71076290beb41d18264d2560358c0c3",
            "a4b19462aed04bdcb8c0df2270b43445",
            "ea55071d78334477afce66b2aa485336",
            "e0b8bfced8664b5695f43f42c28df153",
            "3e7f6e06b53f428583293b1e4560f9f1",
            "75614d12875f49439ba6067c80cba528",
            "80cff55f7bdf46a79ac2cf66a044bc3e",
            "dc8abedb74f14495a74c2aac58b05fed",
            "56b2202ccd364a319d98a65732be74eb",
            "40ff517c8ae9412a949e3c2e1b392a02",
            "4695dcd15c2446a6bb1236a1fdc2cc0b",
            "623d8b4fa8b64e25899943cbd921a170",
            "53314143464b489e90294b156a7bf801",
            "926b1d8d122f40e789293bee0dc2e848",
            "75722ad814fd4e3a8e6f17a62250cacb",
            "30c75af5f5a849ec98c142bf16c9eb35"
          ]
        },
        "id": "HhC3bL88SqDk",
        "outputId": "1d9067fd-f1f9-4a5a-f146-46929325f64a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed83ccd4ac704ef89c2ded236ad4f9b3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# login the kaggle (need to store you kaggle.json to your google dirve)\n",
        "import kagglehub\n",
        "kagglehub.login()\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"gemma_pytorch/gemma\")\n",
        "sys.path.append(\"gemma_pytorch\")\n",
        "import contextlib, torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gemma.config import get_config_for_7b, get_config_for_2b\n",
        "from gemma.model import GemmaForCausalLM, GemmaModel"
      ],
      "metadata": {
        "id": "b2juvSJ46_tS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F70B5D8GKLSJ"
      },
      "outputs": [],
      "source": [
        "# Choose variant and machine type\n",
        "VARIANT = '7b-it-quant'\n",
        "MACHINE_TYPE = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y10m95XaBeQ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gemma\n",
        "import itertools\n",
        "from typing import Iterable\n",
        "from typing import Any, List, Optional, Sequence, Tuple, Union\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cl5f97qr0PG8"
      },
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def _set_default_tensor_type(dtype: torch.dtype):\n",
        "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
        "  torch.set_default_dtype(dtype)\n",
        "  yield\n",
        "  torch.set_default_dtype(torch.float)\n",
        "\n",
        "\n",
        "def interleave_unequal(x, y):\n",
        "    '''\n",
        "        Interleave two lists of unequal length.\n",
        "    '''\n",
        "    return [\n",
        "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR7ysxvB0PG8"
      },
      "source": [
        "### Building Agents\n",
        "\n",
        "<details>\n",
        "  <summary> model </summary>\n",
        "  \n",
        "  - `self.forward()`: getting next token and corresponding logits\n",
        "  - `self.generate()`: see summary below. May need to be rewriten if we wanna get the cumulative logits for the whole sentence  \n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary> model.generate() </summary>\n",
        "  \n",
        "  - **prompts** | `Union[str, Sequence[str]]`: Your prompts\n",
        "  - **device** | `Any`: Devices\n",
        "  - **output_len** | `int`: max output length\n",
        "  - **temperature** | `Union[float, None]`: temperature degree, controlling how variant its response could be  \n",
        "  - **top_p** | `float`:\n",
        "  - **top_k** | `int`:\n",
        "\n",
        "  regarding temperature, top_p and top_k, check this [link](https://blog.csdn.net/REfusing/article/details/137866583)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGSVplU80PG8"
      },
      "source": [
        "#### Define Formatter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RdbzZXCK0PG9",
        "outputId": "7e0f9080-30f4-418c-8535-bd38ebbe8757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. You need to utilize your knowledge and reasoning skills to play a game of 20 questions. In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"a cross on the flag.\" Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" And after your thorough thinking, you need to denote the key binary attribute with double stars. Here are few examples of how to use Chain of thought:<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Now let us play a game of 20 questions. You will be the questioner, and I will be the answerer.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It is the first round, so i will randomly choose a attribute from [\"country\", \"city\", \"landmark\"] as the first attribut. Here I choose country as the first attribute.**Question:** \"Is it a country?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\"] Considerations: 1. Type of Entity: Determining whether it is a city or a landmark can greatly narrow down our options. 2. Geographical Location: If we can determine the continent, we can significantly reduce the number of possible cities or landmarks. 3. Cultural/Historical Significance: Asking about cultural or historical significance can also be helpful, but it might be too broad at this stage. Reasoning: To effectively narrow it down, the most efficient next step is to determine if it is a city or a landmark. By determining if it is a city, we can immediately focus on known cities globally, which are fewer and more well-documented compared to all possible landmarks. If it is not a city, we can then focus on landmarks, which might require more specific questions but will also significantly narrow our scope. Possible Next Steps: If the answer is **yes**, we can then ask about the continent or specific features of the city. If the answer is **no**, we can ask about the type of landmark or its location. **Question:** \"Is it a city?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is not a man-made landmark\"] Considerations: 1. Type of Natural Landmark: Common natural landmarks include mountains, rivers, lakes, forests, deserts, etc. 2. Geographical Location: Identifying the continent or region can still be very helpful. To further narrow down, let us focus on the type of natural landmark. Reasoning: River are one of the most well-known types of natural landmarks. Possible Next Steps: If the answer is **yes**, we can then focus on identifying which river by asking about its specific features or location. If the answer is **no**, we can eliminate a significant category and focus on other types of natural landmarks like mountains, lakes, deserts, or forests. **Question:** \"Is it a river?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\"] Excellent, the river is located in Asia. This narrows down our options significantly. Now, we need to focus on identifying which river it could be. Considerations: 1. Famous Asian Rivers: Asia has several famous rivers, such as the Yangtze, Ganges, Mekong, Indus, and Yellow River. 2. Geographical Features: We could ask about the river's length, the countries it flows through, or any significant historical or cultural associations. 3. To narrow it down further, let us focus on whether it is one of the most well-known rivers. Reasoning: These three rivers are among the longest and most significant in Asia. Possible Next Steps: If the answer is **yes**, we can focus on distinguishing between these three. If the answer is **no**, we can focus on other significant rivers like the Ganges, Indus, or Brahmaputra. **Question:** \"Is this river one of the three longest rivers in Asia (Yangtze, Yellow, or Mekong)?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\", \"It flows through China\"] Great, so the river flows through China. This means it is either the Yangtze River or the Yellow River. To determine which one it is, we should focus on distinguishing features. Considerations: 1. Length: The Yangtze is the longest river in Asia and the third-longest in the world. 2. Color/Name: The Yellow River is named for its yellowish color due to loess sediments. Reasoning: The Yangtze River is the longest river in Asia. Possible Next Steps: If the answer is **yes**, it is the Yangtze River. If the answer is **no**, it must be the Yellow River. **Question:** \"Is this river the longest river in Asia?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It is the Yangtze River<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Correct!<end_of_turn>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Iterable\n",
        "import itertools\n",
        "\n",
        "class PromptFormatter:\n",
        "\n",
        "    '''\n",
        "        formatter class to format the prompt text for the model.\n",
        "        A general idea is\n",
        "    '''\n",
        "\n",
        "    _start_token = '<start_of_turn>'\n",
        "    _end_token = '<end_of_turn>'\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
        "        self._system_prompt = system_prompt\n",
        "        self._few_shot_examples = few_shot_examples\n",
        "        self._template_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
        "        self._template_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
        "        # self._all_prompt = ''\n",
        "        self.reset()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self._all_prompt\n",
        "\n",
        "    def reset(self):\n",
        "        self._all_prompt = ''\n",
        "        # if system prompt is provided, add it to the prompt\n",
        "        if self._system_prompt:\n",
        "            self._all_prompt += self._template_user.format(self._system_prompt)\n",
        "        # same for few shot examples\n",
        "        if self._few_shot_examples:\n",
        "            self.add_rounds(self._few_shot_examples, start_agent='user')\n",
        "\n",
        "    def add_user_round(self, user_prompt: str):\n",
        "        # add user round to the prompt\n",
        "        self._all_prompt += self._template_user.format(user_prompt)\n",
        "\n",
        "    def add_agent_round(self, model_response: str):\n",
        "        self._all_prompt += self._template_model.format(model_response)\n",
        "\n",
        "    def add_rounds(self, rounds: Iterable, start_agent: str):\n",
        "        '''\n",
        "            Apply a sequence of rounds to the formatter, starting with the specified agent.\n",
        "        '''\n",
        "        formatters = [self.add_agent_round, self.add_user_round] if start_agent == 'model' else [self.add_user_round, self.add_agent_round] # here, self.model and self.user are functions definded above\n",
        "        formatters = itertools.cycle(formatters)\n",
        "        for fmt, round in zip(formatters, rounds):\n",
        "            fmt(round)\n",
        "        return self\n",
        "\n",
        "    # def add_end_token(self):\n",
        "    #     self._all_prompt += f\"{self._end_token}\\n\"\n",
        "\n",
        "    def add_new_round(self, player: str, prompt:str = None, end_token: bool = False):\n",
        "        self._all_prompt += f\"{self._start_token}{player}\\n\"\n",
        "        if prompt:\n",
        "            self._all_prompt += f'{prompt}'\n",
        "        if end_token:\n",
        "            # self.add_end_token()\n",
        "            self._all_prompt += f\"{self._end_token}\\n\"\n",
        "\n",
        "    def formate_MCQA(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "sys_prompt = 'You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. \\\n",
        "You need to utilize your knowledge and reasoning skills to play a game of 20 questions. \\\n",
        "In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"a cross on the flag.\" \\\n",
        "Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" \\\n",
        "And after your thorough thinking, you need to denote the key binary attribute with double stars. Here are few examples of how to use Chain of thought:'\n",
        "\n",
        "few_shot_examples = [\n",
        "\n",
        "# 'Now give me an example of thinking with the Chain of Thought',\n",
        "\n",
        "# 'Given Information: [\"is a country\", \"is in Europe\", \"its flag is with red color\"] \\\n",
        "# Considerations: \\\n",
        "# 1. Geographical Location: European countries can be divided into regions such as Northern Europe, Southern Europe, Western Europe, and Eastern Europe. This division can effectively narrow down the options. \\\n",
        "# 2. Population and Area: Considering population and area can also eliminate some countries. \\\n",
        "# 3. Other Colors on the Flag: We know the flag has red; we can further narrow down by asking about other colors. \\\n",
        "# 4. Language: European countries have various languages; asking about the official language can narrow it further. \\\n",
        "# Reasoning: Southern Europe includes a few countries such as Spain, Italy, Portugal, Greece, etc. Determining if it is in Southern Europe can immediately eliminate about three-quarters of European countries. \\\n",
        "# Compared to questions about flag colors or language, geographical location questions are more evenly distributed, and a **yes** or **no** answer will significantly narrow down the range. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is **yes**, we can further inquire about other notable characteristics of these countries, such as language or history. \\\n",
        "# If the answer is **no**, we can turn our attention to Western, Eastern, or Northern Europe and continue narrowing down the options.\\\n",
        "# **Question:** \"Is this country in Southern Europe?\"',\n",
        "\n",
        "'Now let us play a game of 20 questions. You will be the questioner, and I will be the answerer.',\n",
        "\n",
        "'It is the first round, so i will randomly choose a attribute from [\"country\", \"city\", \"landmark\"] as the first attribut. Here I choose country as the first attribute.\\\n",
        "**Question:** \"Is it a country?\"',\n",
        "\n",
        "'**no**',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\"] \\\n",
        "Considerations: \\\n",
        "1. Type of Entity: Determining whether it is a city or a landmark can greatly narrow down our options. \\\n",
        "2. Geographical Location: If we can determine the continent, we can significantly reduce the number of possible cities or landmarks. \\\n",
        "3. Cultural/Historical Significance: Asking about cultural or historical significance can also be helpful, but it might be too broad at this stage. \\\n",
        "Reasoning: To effectively narrow it down, the most efficient next step is to determine if it is a city or a landmark. By determining if it is a city, we can immediately focus on known cities globally, which are fewer and more well-documented compared to all possible landmarks. \\\n",
        "If it is not a city, we can then focus on landmarks, which might require more specific questions but will also significantly narrow our scope. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is **yes**, we can then ask about the continent or specific features of the city. \\\n",
        "If the answer is **no**, we can ask about the type of landmark or its location. \\\n",
        "**Question:** \"Is it a city?\"',\n",
        "\n",
        "'**no**',\n",
        "\n",
        "# 'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\"] \\\n",
        "# Considerations: \\\n",
        "# Alright, so it is not a city. Now we know it must be a landmark. Given this, our goal is to identify the type or location of the landmark. \\\n",
        "# 1. Type of Landmark: There are various types such as natural landmarks (mountains, rivers), man-made landmarks (statues, buildings), or historical sites. \\\n",
        "# 2. Geographical Location: Identifying the continent or region can greatly narrow down the possibilities. \\\n",
        "# To get a high information gain, let us narrow down the type of landmark first. \\\n",
        "# Reasoning: Landmarks can be broadly categorized into man-made or natural. This binary distinction will help us immediately focus on a more specific set of possibilities. \\\n",
        "# Man-made landmarks include famous buildings, statues, monuments, etc., while natural landmarks include mountains, rivers, and natural formations. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is **yes**, we can then ask about the purpose or era of the man-made structure. \\\n",
        "# If the answer is **no**, we can focus on identifying the type of natural landmark or its location. \\\n",
        "# **Question:** \"Is it a man-made landmark?\"',\n",
        "\n",
        "# '**no**',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is not a man-made landmark\"] \\\n",
        "Considerations: \\\n",
        "1. Type of Natural Landmark: Common natural landmarks include mountains, rivers, lakes, forests, deserts, etc. \\\n",
        "2. Geographical Location: Identifying the continent or region can still be very helpful. \\\n",
        "To further narrow down, let us focus on the type of natural landmark. \\\n",
        "Reasoning: River are one of the most well-known types of natural landmarks. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is **yes**, we can then focus on identifying which river by asking about its specific features or location. \\\n",
        "If the answer is **no**, we can eliminate a significant category and focus on other types of natural landmarks like mountains, lakes, deserts, or forests. \\\n",
        "**Question:** \"Is it a river?\"',\n",
        "\n",
        "'**yes**',\n",
        "\n",
        "# 'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\"] \\\n",
        "# Considerations: \\\n",
        "# Great, so it is a river. Now, we need to narrow down which river it could be. To do this effectively, identifying the continent or region will significantly help. \\\n",
        "# 1. Geographical Location: Identifying the continent or region can narrow down the list of possible rivers considerably. \\\n",
        "# 2. Length or Size: Asking about whether it is among the longest rivers might also help. \\\n",
        "# 3. Famous Rivers: There are some rivers that are globally well-known due to their significance. \\\n",
        "# To achieve a high information gain, let us focus on the geographical location. \\\n",
        "# Reasoning: Asia has some of the world us most significant and longest rivers, such as the Yangtze, Ganges, and Mekong. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is **yes**, we can focus on Asian rivers specifically. \\\n",
        "# If the answer is **no**, we can eliminate Asia and consider other continents like Africa, Europe, South America, or North America. \\\n",
        "# **Question:** \"Is this reiver located in Asia?\"',\n",
        "\n",
        "# '**yes**',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\"] \\\n",
        "Excellent, the river is located in Asia. This narrows down our options significantly. Now, we need to focus on identifying which river it could be. \\\n",
        "Considerations: \\\n",
        "1. Famous Asian Rivers: Asia has several famous rivers, such as the Yangtze, Ganges, Mekong, Indus, and Yellow River. \\\n",
        "2. Geographical Features: We could ask about the river\\'s length, the countries it flows through, or any significant historical or cultural associations. \\\n",
        "3. To narrow it down further, let us focus on whether it is one of the most well-known rivers. \\\n",
        "Reasoning: These three rivers are among the longest and most significant in Asia. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is **yes**, we can focus on distinguishing between these three. \\\n",
        "If the answer is **no**, we can focus on other significant rivers like the Ganges, Indus, or Brahmaputra. \\\n",
        "**Question:** \"Is this river one of the three longest rivers in Asia (Yangtze, Yellow, or Mekong)?\"',\n",
        "\n",
        "'**yes**',\n",
        "\n",
        "# 'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\"] \\\n",
        "# Great, we now know it is one of the three longest rivers in Asia: the Yangtze, Yellow, or Mekong. Let us narrow it down further by focusing on specific characteristics of these rivers. \\\n",
        "# Considerations: \\\n",
        "# 1. Geographical Location: We can ask about the countries the river flows through. \\\n",
        "# 2. Cultural/Historical Significance: Each of these rivers has distinct cultural and historical significance. \\\n",
        "# To narrow it down further, let us ask about the country. \\\n",
        "# Reasoning: Both the Yangtze and Yellow rivers flow through China, while the Mekong flows through several countries including China, but predominantly in Southeast Asia. \\\n",
        "# Possible Next Steps: \\\n",
        "# If the answer is yes, it will most likely be the Yangtze or Yellow River. \\\n",
        "# If the answer is no, it must be the Mekong River. \\\n",
        "# **Question:** \"Does this river flow through China?\"',\n",
        "\n",
        "# '**yes**',\n",
        "\n",
        "'Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\", \"It flows through China\"] \\\n",
        "Great, so the river flows through China. This means it is either the Yangtze River or the Yellow River. To determine which one it is, we should focus on distinguishing features. \\\n",
        "Considerations: \\\n",
        "1. Length: The Yangtze is the longest river in Asia and the third-longest in the world. \\\n",
        "2. Color/Name: The Yellow River is named for its yellowish color due to loess sediments. \\\n",
        "Reasoning: The Yangtze River is the longest river in Asia. \\\n",
        "Possible Next Steps: \\\n",
        "If the answer is **yes**, it is the Yangtze River. \\\n",
        "If the answer is **no**, it must be the Yellow River. \\\n",
        "**Question:** \"Is this river the longest river in Asia?\"',\n",
        "\n",
        "'**yes**',\n",
        "\n",
        "'It is the Yangtze River',\n",
        "\n",
        "'Correct!']\n",
        "\n",
        "print(str(PromptFormatter(sys_prompt, few_shot_examples)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJF50Hpc0PG-"
      },
      "source": [
        "#### Define Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rOq4uGAF0PG-"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GemmaAgent:\n",
        "\n",
        "    def __init__(self, model_variant, device='cuda:0', env=\"kaggle\", system_prompt=None, few_shot_examples=None):\n",
        "        # model initialization\n",
        "        self.device = device\n",
        "        self.model_variant = model_variant\n",
        "\n",
        "        WEIGHTS_PATH = self._set_up_env(env)\n",
        "\n",
        "        # Ensure that the tokenizer is present\n",
        "        tokenizer_path = os.path.join(WEIGHTS_PATH, 'tokenizer.model')\n",
        "        assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
        "\n",
        "        # Ensure that the checkpoint is present\n",
        "        ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{model_variant}.ckpt')\n",
        "        assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\n",
        "\n",
        "        # loading model configuration\n",
        "        model_config = get_config_for_2b() if \"2b\" in model_variant else get_config_for_7b()\n",
        "        model_config.quant = \"quant\" in model_variant\n",
        "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
        "\n",
        "        with _set_default_tensor_type(model_config.get_dtype()):\n",
        "            self.model = GemmaForCausalLM(model_config)\n",
        "            self.model.load_weights(ckpt_path)\n",
        "            self.model = self.model.to(self.device).eval()\n",
        "\n",
        "        # agent args\n",
        "        self.formatter = PromptFormatter(system_prompt=system_prompt,\n",
        "                                         few_shot_examples=few_shot_examples)\n",
        "        self.round_num = 0\n",
        "\n",
        "    def _set_up_env(self, env):\n",
        "\n",
        "        if env == 'kaggle':\n",
        "            print(\"Loading model in Kaggle, model weights will be searched within local directories.\")\n",
        "\n",
        "            # kaggle configuration\n",
        "            KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
        "            if os.path.exists(KAGGLE_AGENT_PATH):\n",
        "                WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, f\"gemma/pytorch/{self.model_variant}/2\")\n",
        "            else:\n",
        "                WEIGHTS_PATH = f\"/kaggle/input/gemma/pytorch/{self.model_variant}/2\"\n",
        "\n",
        "        elif env == 'colab':\n",
        "            print(\"Loading model in Colab, starting from downloading the model weights.\")\n",
        "\n",
        "            WEIGHTS_PATH = kagglehub.model_download(f'google/gemma/pyTorch/{self.model_variant}')\n",
        "        else:\n",
        "            raise ValueError(\"Argument 'env' should be in ['kaggle', 'colab']\")\n",
        "\n",
        "        return WEIGHTS_PATH\n",
        "\n",
        "    def response(self, obs, *args, **kwargs):\n",
        "        '''\n",
        "            The game env will call this function.\n",
        "        '''\n",
        "\n",
        "        # formatting prompt\n",
        "        prompt = None\n",
        "\n",
        "        # getting response from LLM\n",
        "        response = self.model.generate(prompt, device=self.device, output_len=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbSO_x4kR_2Y"
      },
      "source": [
        "### format your prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BpnczT4kR7pZ"
      },
      "outputs": [],
      "source": [
        "# # Chat templates\n",
        "# USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
        "# MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'\n",
        "\n",
        "# prompt = (\n",
        "#     USER_CHAT_TEMPLATE.format(\n",
        "#         prompt='What is a good place for travel in the US?'\n",
        "#     )\n",
        "# )\n",
        "# print('Chat prompt:\\n', prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqh86PQqSLTY"
      },
      "source": [
        "### Chat & Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6VrHQ8__6W"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "# del agent"
      ],
      "metadata": {
        "id": "F55mke0NSCnI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose variant and machine type\n",
        "VARIANT = '7b-it-quant'\n",
        "MACHINE_TYPE = 'cuda'\n",
        "\n",
        "agent = GemmaAgent(model_variant=VARIANT, device=MACHINE_TYPE, env='colab',\n",
        "                   system_prompt=sys_prompt, few_shot_examples=few_shot_examples)"
      ],
      "metadata": {
        "id": "HBrhgdi40krX",
        "outputId": "6028ca2f-95bc-4560-c6e6-ae9de4cb48c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model in Colab, starting from downloading the model weights.\n",
            "Attaching model 'google/gemma/pyTorch/7b-it-quant' to your Colab notebook...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "model = agent.model\n",
        "agent.formatter.reset()\n",
        "agent.formatter.add_new_round('user', 'Remember how to use CoT as above, now let us play a new game from the begining , you are the guesser, remember to identify your question to with **Question:** at the end', True)\n",
        "agent.formatter.add_new_round('model','Is it a country?', True)\n",
        "agent.formatter.add_new_round('user','**no**', True)\n",
        "agent.formatter.add_new_round('model',None, False)\n",
        "print(str(agent.formatter))\n",
        "start = time.time()\n",
        "response = model.generate(str(agent.formatter), device=agent.device, output_len=200)\n",
        "print(f\"[response in {(time.time()-start):.2f}s]\\n{response}\")"
      ],
      "metadata": {
        "id": "e3pCBOlNKhNe",
        "outputId": "98f9221b-11cd-4259-e106-7b7a6ce73818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. You need to utilize your knowledge and reasoning skills to play a game of 20 questions. In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"a cross on the flag.\" Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" And after your thorough thinking, you need to denote the key binary attribute with double stars. Here are few examples of how to use Chain of thought:<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Now let us play a game of 20 questions. You will be the questioner, and I will be the answerer.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It is the first round, so i will randomly choose a attribute from [\"country\", \"city\", \"landmark\"] as the first attribut. Here I choose country as the first attribute.**Question:** \"Is it a country?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\"] Considerations: 1. Type of Entity: Determining whether it is a city or a landmark can greatly narrow down our options. 2. Geographical Location: If we can determine the continent, we can significantly reduce the number of possible cities or landmarks. 3. Cultural/Historical Significance: Asking about cultural or historical significance can also be helpful, but it might be too broad at this stage. Reasoning: To effectively narrow it down, the most efficient next step is to determine if it is a city or a landmark. By determining if it is a city, we can immediately focus on known cities globally, which are fewer and more well-documented compared to all possible landmarks. If it is not a city, we can then focus on landmarks, which might require more specific questions but will also significantly narrow our scope. Possible Next Steps: If the answer is **yes**, we can then ask about the continent or specific features of the city. If the answer is **no**, we can ask about the type of landmark or its location. **Question:** \"Is it a city?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is not a man-made landmark\"] Considerations: 1. Type of Natural Landmark: Common natural landmarks include mountains, rivers, lakes, forests, deserts, etc. 2. Geographical Location: Identifying the continent or region can still be very helpful. To further narrow down, let us focus on the type of natural landmark. Reasoning: River are one of the most well-known types of natural landmarks. Possible Next Steps: If the answer is **yes**, we can then focus on identifying which river by asking about its specific features or location. If the answer is **no**, we can eliminate a significant category and focus on other types of natural landmarks like mountains, lakes, deserts, or forests. **Question:** \"Is it a river?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\"] Excellent, the river is located in Asia. This narrows down our options significantly. Now, we need to focus on identifying which river it could be. Considerations: 1. Famous Asian Rivers: Asia has several famous rivers, such as the Yangtze, Ganges, Mekong, Indus, and Yellow River. 2. Geographical Features: We could ask about the river's length, the countries it flows through, or any significant historical or cultural associations. 3. To narrow it down further, let us focus on whether it is one of the most well-known rivers. Reasoning: These three rivers are among the longest and most significant in Asia. Possible Next Steps: If the answer is **yes**, we can focus on distinguishing between these three. If the answer is **no**, we can focus on other significant rivers like the Ganges, Indus, or Brahmaputra. **Question:** \"Is this river one of the three longest rivers in Asia (Yangtze, Yellow, or Mekong)?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: [\"It is not a country\", \"It is either a landmark or a city\", \"It is not a landmark\", \"It is a river\", \"It is located in Asia\", \"It is one of the three longest rivers in Asia\", \"It flows through China\"] Great, so the river flows through China. This means it is either the Yangtze River or the Yellow River. To determine which one it is, we should focus on distinguishing features. Considerations: 1. Length: The Yangtze is the longest river in Asia and the third-longest in the world. 2. Color/Name: The Yellow River is named for its yellowish color due to loess sediments. Reasoning: The Yangtze River is the longest river in Asia. Possible Next Steps: If the answer is **yes**, it is the Yangtze River. If the answer is **no**, it must be the Yellow River. **Question:** \"Is this river the longest river in Asia?\"<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**yes**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It is the Yangtze River<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Correct!<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Remember how to use CoT as above, now let us play a new game from the begining , you are the guesser, remember to identify your question to with **Question:** at the end<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Is it a country?<end_of_turn>\n",
            "<start_of_turn>user\n",
            "**no**<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n",
            "[response in 87.97s]\n",
            "## Round 2\n",
            "\n",
            "**Given Information:**\n",
            "\n",
            "It is not a country. It is either a city or a landmark. It is not a natural landmark and does not have any man-made structures of significance like a fort or a mountain range.\n",
            "\n",
            "**Reasoning:**\n",
            "\n",
            "Since it's not a natural landmark, the focus should be on cities. There are two broad categories of cities: those that were founded near water bodies and those that were founded on mountaintops.\n",
            "\n",
            "**Possible Next Steps:**\n",
            "\n",
            "1. If the answer is **yes**, I could ask about the type of city or its location.\n",
            "2. If the answer is **no**, I could ask about the the dominant terrain feature near the city or the historical significance of the city.\n",
            "\n",
            "**Question:** Is it a city?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kDSPbiluSQwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9loBFRC0TQab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QElPuSuVxomJ"
      },
      "source": [
        "### applying formatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dohKH_0w0PHA"
      },
      "outputs": [],
      "source": [
        "# dummy_gemma = lambda x: \"A4\"\n",
        "\n",
        "# rounds = ['Q1', 'A1', 'Q2', 'A2', 'Q3', 'A3']\n",
        "\n",
        "# system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
        "\n",
        "# few_shot_examples = [\n",
        "#     \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
        "#     \"Is it a person?\", \"**no**\",\n",
        "#     \"Is is a place?\", \"**yes**\",\n",
        "#     \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
        "#     \"**France**\", \"Correct!\",\n",
        "# ]\n",
        "\n",
        "# dummy_formatter = PromptFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
        "\n",
        "# print(str(dummy_formatter), end='\\n\\n')\n",
        "# dummy_formatter.add_existing_rounds(rounds, start_agent='user')\n",
        "# print(str(dummy_formatter), end='\\n\\n')\n",
        "# dummy_formatter.add_new_round('user', prompt='Q4', end_token=False)\n",
        "# print(str(dummy_formatter), end='\\n\\n')\n",
        "# dummy_formatter.add_end_token()\n",
        "# print(str(dummy_formatter), end='\\n\\n')\n",
        "# dummy_formatter.add_new_round('model', end_token=False)\n",
        "# print(str(dummy_formatter), end='\\n\\n')\n",
        "# dummy_formatter.add_new_round('model', prompt=dummy_gemma(0), end_token=True)\n",
        "# print(str(dummy_formatter), end='\\n\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed83ccd4ac704ef89c2ded236ad4f9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d71076290beb41d18264d2560358c0c3",
              "IPY_MODEL_a4b19462aed04bdcb8c0df2270b43445",
              "IPY_MODEL_ea55071d78334477afce66b2aa485336",
              "IPY_MODEL_e0b8bfced8664b5695f43f42c28df153",
              "IPY_MODEL_3e7f6e06b53f428583293b1e4560f9f1"
            ],
            "layout": "IPY_MODEL_75614d12875f49439ba6067c80cba528"
          }
        },
        "d71076290beb41d18264d2560358c0c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80cff55f7bdf46a79ac2cf66a044bc3e",
            "placeholder": "​",
            "style": "IPY_MODEL_dc8abedb74f14495a74c2aac58b05fed",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "a4b19462aed04bdcb8c0df2270b43445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_56b2202ccd364a319d98a65732be74eb",
            "placeholder": "​",
            "style": "IPY_MODEL_40ff517c8ae9412a949e3c2e1b392a02",
            "value": ""
          }
        },
        "ea55071d78334477afce66b2aa485336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4695dcd15c2446a6bb1236a1fdc2cc0b",
            "placeholder": "​",
            "style": "IPY_MODEL_623d8b4fa8b64e25899943cbd921a170",
            "value": ""
          }
        },
        "e0b8bfced8664b5695f43f42c28df153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_53314143464b489e90294b156a7bf801",
            "style": "IPY_MODEL_926b1d8d122f40e789293bee0dc2e848",
            "tooltip": ""
          }
        },
        "3e7f6e06b53f428583293b1e4560f9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75722ad814fd4e3a8e6f17a62250cacb",
            "placeholder": "​",
            "style": "IPY_MODEL_30c75af5f5a849ec98c142bf16c9eb35",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "75614d12875f49439ba6067c80cba528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "80cff55f7bdf46a79ac2cf66a044bc3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc8abedb74f14495a74c2aac58b05fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56b2202ccd364a319d98a65732be74eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ff517c8ae9412a949e3c2e1b392a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4695dcd15c2446a6bb1236a1fdc2cc0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623d8b4fa8b64e25899943cbd921a170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53314143464b489e90294b156a7bf801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "926b1d8d122f40e789293bee0dc2e848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "75722ad814fd4e3a8e6f17a62250cacb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c75af5f5a849ec98c142bf16c9eb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}