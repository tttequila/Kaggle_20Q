{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tttequila/Kaggle_20Q/blob/main/7B_MultiCoT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqjWHE8-MQVl"
      },
      "source": [
        "Configuring your kaggle token, see more details in the [**Configure your API key**](https://ai.google.dev/gemma/docs/setup) section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENaMz5-35Ayi"
      },
      "source": [
        "### set up env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndGYpeKSFV5U",
        "outputId": "43acb2d3-e10d-419e-d73e-d02520ea73d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nNrgspAf9T8W"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mkdir ~/.kaggle\n",
        "# change the first path to your path of kaggle.json\n",
        "cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-l45FATYz0T",
        "outputId": "159835d1-7407-41e0-b388-9dcaecd96902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 797.2/797.2 MB 1.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.0 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.4/209.4 MB 5.2 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 60.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 92.3 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.4.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -q -U torch immutabledict sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/gemma_pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EUA6-EY6DCH",
        "outputId": "a9b3452b-4825-42e9-9b89-aee6a4c074e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gemma_pytorch'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 231 (delta 83), reused 52 (delta 52), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (231/231), 2.17 MiB | 22.01 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lbfq9Ee5Ayo"
      },
      "source": [
        "### set up Gemma lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "fce82ce12d71409fa792c15cb0aa06ed",
            "a07a2c0b04404ad6a56b7887335a3bc3",
            "d2edf175776b4704b86fe1f2eb24db38",
            "b989da0f8ba642d9ab3017dd0bd69856",
            "acefd9c8b7304eb7863d506a6e2320c7",
            "313f0471e77f471ebef8018cd99f2902",
            "49d3864189fd4ac489f2e63848d24e82",
            "1d9aaff2e2524d1eb4f28fae59f109d3",
            "7b0741c3bc2940eb9e414291f99fc79e",
            "42c8724340bb4dca8e9104b50a5ceaff",
            "9acbac1acf4e41e4baf098090f27d042",
            "907c4f15adcd4747a916876f07a9df2e",
            "b7f5db031e4c4f289dbd22102f95282f",
            "443d92615685447aa6d94ddd36467bd6",
            "e1eac1c081344634b4496b47a03f7412",
            "d4cc78fb9ed9471b9af7d4ba113ac158",
            "2d93d615bd324b78a19c1f9761bdb2a2"
          ]
        },
        "id": "HhC3bL88SqDk",
        "outputId": "ea46804b-a9fc-4b3a-d884-40990d99e8e0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fce82ce12d71409fa792c15cb0aa06ed"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# login the kaggle (need to store you kaggle.json to your google dirve)\n",
        "import kagglehub\n",
        "kagglehub.login()\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"gemma_pytorch/gemma\")\n",
        "sys.path.append(\"gemma_pytorch\")\n",
        "import contextlib, torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gemma.config import get_config_for_7b, get_config_for_2b\n",
        "from gemma.model import GemmaForCausalLM, GemmaModel"
      ],
      "metadata": {
        "id": "vmThtLaQ8nwA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y10m95XaBeQ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gemma\n",
        "import itertools\n",
        "from typing import Iterable\n",
        "from typing import Any, List, Optional, Sequence, Tuple, Union\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bURsXpMG5Ayu"
      },
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def _set_default_tensor_type(dtype: torch.dtype):\n",
        "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
        "  torch.set_default_dtype(dtype)\n",
        "  yield\n",
        "  torch.set_default_dtype(torch.float)\n",
        "\n",
        "\n",
        "def interleave_unequal(x, y):\n",
        "    '''\n",
        "        Interleave two lists of unequal length.\n",
        "    '''\n",
        "    return [\n",
        "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLOddhGA5Ayv"
      },
      "source": [
        "### Building Agents\n",
        "\n",
        "<details>\n",
        "  <summary> model </summary>\n",
        "  \n",
        "  - `self.forward()`: getting next token and corresponding logits\n",
        "  - `self.generate()`: see summary below. May need to be rewriten if we wanna get the cumulative logits for the whole sentence  \n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary> model.generate() </summary>\n",
        "  \n",
        "  - **prompts** | `Union[str, Sequence[str]]`: Your prompts\n",
        "  - **device** | `Any`: Devices\n",
        "  - **output_len** | `int`: max output length\n",
        "  - **temperature** | `Union[float, None]`: temperature degree, controlling how variant its response could be  \n",
        "  - **top_p** | `float`:\n",
        "  - **top_k** | `int`:\n",
        "\n",
        "  regarding temperature, top_p and top_k, check this [link](https://blog.csdn.net/REfusing/article/details/137866583)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL_iy3Ai5Ayw"
      },
      "source": [
        "#### Define Formatter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AtsqJUJs5Ayw"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "import itertools\n",
        "\n",
        "class PromptFormatter:\n",
        "\n",
        "    '''\n",
        "        formatter class to format the prompt text for the model.\n",
        "        A general idea is\n",
        "    '''\n",
        "\n",
        "    _start_token = '<start_of_turn>'\n",
        "    _end_token = '<end_of_turn>'\n",
        "\n",
        "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None, sample_num = None):\n",
        "        self._system_prompt = system_prompt\n",
        "        self._few_shot_examples = few_shot_examples\n",
        "        self._template_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
        "        self._template_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
        "        self._all_prompt = ''\n",
        "        if sample_num:\n",
        "          self.sample_num = sample_num\n",
        "        else:\n",
        "          self.sample_num = len(few_shot_examples)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self._all_prompt\n",
        "\n",
        "    def reset(self):\n",
        "        self._all_prompt = ''\n",
        "        # if system prompt is provided, add it to the prompt\n",
        "        if self._system_prompt is not None:\n",
        "            self._all_prompt += self._template_user.format(self._system_prompt)\n",
        "        # same for few shot examples\n",
        "        if self._few_shot_examples in not None:\n",
        "            # self.add_rounds(self._few_shot_examples, start_agent='user')\n",
        "            self.add_new_round('user', 'Here are some examples of analyzing with causal reasoning:', True)\n",
        "            for example in random.sample(self._few_shot_examples, self.sample_num):\n",
        "              self.add_new_round('model', example.strip(), True)\n",
        "\n",
        "\n",
        "    def add_user_round(self, user_prompt: str):\n",
        "        # add user round to the prompt\n",
        "        self._all_prompt += self._template_user.format(user_prompt)\n",
        "\n",
        "    def add_agent_round(self, model_response: str):\n",
        "        self._all_prompt += self._template_model.format(model_response)\n",
        "\n",
        "    def add_rounds(self, rounds: Iterable, start_agent: str):\n",
        "        '''\n",
        "            Apply a sequence of rounds to the formatter, starting with the specified agent.\n",
        "        '''\n",
        "        formatters = [self.add_agent_round, self.add_user_round] if start_agent == 'model' else [self.add_user_round, self.add_agent_round] # here, self.model and self.user are functions definded above\n",
        "        formatters = itertools.cycle(formatters)\n",
        "        for fmt, round in zip(formatters, rounds):\n",
        "            fmt(round)\n",
        "        return self\n",
        "\n",
        "    # def add_end_token(self):\n",
        "    #     self._all_prompt += f\"{self._end_token}\\n\"\n",
        "\n",
        "    def add_new_round(self, player: str, prompt:str = None, end_token: bool = False):\n",
        "        self._all_prompt += f\"{self._start_token}{player}\\n\"\n",
        "        if prompt:\n",
        "            self._all_prompt += f'{prompt}'\n",
        "        if end_token:\n",
        "            # self.add_end_token()\n",
        "            self._all_prompt += f\"{self._end_token}\\n\"\n",
        "\n",
        "    def formate_MCQA(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# print(str(PromptFormatter(sys_prompt, few_shot_examples_ask)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5NRQGRB5Ayz"
      },
      "source": [
        "#### Define Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### backup agent"
      ],
      "metadata": {
        "id": "A73u5Ft6XHTA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hIPcPhjD5Ay0"
      },
      "outputs": [],
      "source": [
        "# from ast import parse\n",
        "# import random\n",
        "\n",
        "# class GemmaAgent:\n",
        "#     def __init__(self, model_variant, device='cuda:0', env=\"kaggle\", output_len=200, system_prompt=None, few_shot_examples=None, example_sample_num=None):\n",
        "#         # model initialization\n",
        "#         self.device = device\n",
        "#         self.model_variant = model_variant\n",
        "#         self.few_shot_exmaples_ask = few_shot_examples[0]\n",
        "#         self.few_shot_exmaples_guess = few_shot_examples[1]\n",
        "#         self.example_sample_num = example_sample_num\n",
        "\n",
        "#         WEIGHTS_PATH = self._set_up_env(env)\n",
        "\n",
        "#         # Ensure that the tokenizer is present\n",
        "#         tokenizer_path = os.path.join(WEIGHTS_PATH, 'tokenizer.model')\n",
        "#         assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
        "\n",
        "#         # Ensure that the checkpoint is present\n",
        "#         ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{model_variant}.ckpt')\n",
        "#         assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\n",
        "\n",
        "#         # loading model configuration\n",
        "#         model_config = get_config_for_2b() if \"2b\" in model_variant else get_config_for_7b()\n",
        "#         model_config.quant = \"quant\" in model_variant\n",
        "#         model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
        "\n",
        "#         with _set_default_tensor_type(model_config.get_dtype()):\n",
        "#             self.model = GemmaForCausalLM(model_config)\n",
        "#             self.model.load_weights(ckpt_path)\n",
        "#             self.model = self.model.to(self.device).eval()\n",
        "\n",
        "#         # agent args\n",
        "#         self.formatter = PromptFormatter(system_prompt=system_prompt,\n",
        "#                                          few_shot_examples=few_shot_examples[0],\n",
        "#                                          sample_num=self.example_sample_num)\n",
        "#         self.round_num = 0\n",
        "#         self.known_info = []\n",
        "#         self.last_key_attribute = None\n",
        "#         self.last_guess = None\n",
        "#         self.output_len = output_len\n",
        "\n",
        "#     def _set_up_env(self, env):\n",
        "\n",
        "#         if env == 'kaggle':\n",
        "#             print(\"Loading model in Kaggle, model weights will be searched within local directories.\")\n",
        "\n",
        "#             # kaggle configuration\n",
        "#             KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
        "#             if os.path.exists(KAGGLE_AGENT_PATH):\n",
        "#                 WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, f\"gemma/pytorch/{self.model_variant}/2\")\n",
        "#             else:\n",
        "#                 WEIGHTS_PATH = f\"/kaggle/input/gemma/pytorch/{self.model_variant}/2\"\n",
        "\n",
        "#         elif env == 'colab':\n",
        "#             print(\"Loading model in Colab, starting from downloading the model weights.\")\n",
        "\n",
        "#             WEIGHTS_PATH = kagglehub.model_download(f'google/gemma/pyTorch/{self.model_variant}')\n",
        "#         else:\n",
        "#             raise ValueError(\"Argument 'env' should be in ['kaggle', 'colab']\")\n",
        "\n",
        "#         return WEIGHTS_PATH\n",
        "\n",
        "#     def _parse_response(self, response : str):\n",
        "#       raise NotImplementedError\n",
        "\n",
        "#     def _format_prompt(self, obs):\n",
        "#       raise NotImplementedError\n",
        "\n",
        "\n",
        "# class GemmaAgent_Answer(GemmaAgent):\n",
        "\n",
        "#     def _parse_response(self, response : str):\n",
        "#       pass\n",
        "\n",
        "#     def _format_prompt(self, obs):\n",
        "#       pass\n",
        "\n",
        "\n",
        "\n",
        "# class GemmaAgent_Guesser(GemmaAgent):\n",
        "\n",
        "#     def _parse_response(self, response : str):\n",
        "#         '''\n",
        "#             parse the response into a dictionary.\n",
        "#             may contain three keys: key_attribute, question, guess and their value respectively.\n",
        "#             e.g.: {\"key_attribute\": 'a country', \"question\": 'Is it a country?'} for a parsed asker response.\n",
        "#         '''\n",
        "#         pattern = re.compile(r'\\*\\*([^*]+)\\*\\*:?\\s*(.*?)(?=\\*\\*|$)', re.DOTALL)\n",
        "#         matches = pattern.findall(response.lower())\n",
        "#         parse_dict = {'key attribute':None, 'question':None, 'guess':None}\n",
        "#         for k, v in matches:\n",
        "#           if ('attr' in k) and (parse_dict['key attribute'] == None):\n",
        "#             parse_dict['key attribute'] = v\n",
        "#           elif ('ques' in k) and (parse_dict['question'] == None):\n",
        "#             parse_dict['question'] = v\n",
        "#           elif ('guess' in k) and (parse_dict['guess'] == None):\n",
        "#             parse_dict['guess'] = v\n",
        "\n",
        "#         return parse_dict\n",
        "\n",
        "#     def _format_prompt(self, obs):\n",
        "#         if obs[\"turnType\"] == 'ask':\n",
        "#           self.formatter._few_shot_examples = self.few_shot_exmaples_ask\n",
        "#         if obs[\"turnType\"] == 'guess':\n",
        "#           self.formatter._few_shot_examples = self.few_shot_exmaples_guess\n",
        "#         # reset formatter\n",
        "#         self.formatter.reset()\n",
        "#         self.formatter.add_new_round('user', 'Remember how to use CoT as above and the format of answering, now let us play a new game from the begining.', True)\n",
        "#         # add played rounds\n",
        "#         rounds = interleave_unequal(obs[\"questions\"], obs[\"answers\"])\n",
        "#         self.formatter.add_rounds(rounds, start_agent='user')\n",
        "#         if obs[\"turnType\"] == 'ask':\n",
        "#             self.formatter.add_new_round('user',\n",
        "#                                          'Now it is your turn to ask a question. Please construct a new question based on what you have known and your common sense with your reasoning. Do not forget to indicate key attribute, question with double asterisk (**?**) and their content with double quotation markers (\"?\"), such as **Key Attribute** \"your attribute\" and **Question** \"your question\"',\n",
        "#                                          True)\n",
        "#         elif obs[\"turnType\"] == 'guess':\n",
        "#             self.formatter.add_new_round('user',\n",
        "#                                          'Now guess the keyword based on your analysis with reasoning and known information. And surround a pointer of guess with double asterisks (**?**) and your guessed keyword with doublr quotation markers (\"?\") in the end, such as **Guess** \"your guess\"',\n",
        "#                                          True)\n",
        "#         # start of model response\n",
        "#         self.formatter.add_new_round('model', f'Given information: {str(self.known_info)}', False)\n",
        "\n",
        "#     def __call__(self, obs, cfg=None):\n",
        "#         '''\n",
        "#             Main function to interact with the model.\n",
        "#                 1. update known information based on the observation and round number\n",
        "#                 2. format the prompt with the observation\n",
        "#                 3. generate response from the model\n",
        "#                 4. parse the response and update information if needed\n",
        "#         '''\n",
        "#         print(f\"=========== round: {self.round_num} ===========\")\n",
        "#         self.round_num += 1\n",
        "#         # Update known information\n",
        "#         if self.round_num - 1 == 0:\n",
        "#             # for the first round, randomly choose an attribute as initialization\n",
        "#             initial_categories = ['person', 'thing', 'place']\n",
        "#             self.last_key_attribute = random.choice(initial_categories)\n",
        "#             self.else_categories = list(set(initial_categories) - set([self.last_key_attribute]))\n",
        "#             response = f'Is it a {self.last_key_attribute}?'\n",
        "#             round\n",
        "#             return response\n",
        "#         elif self.round_num - 1 == 1:\n",
        "#             # if it is the second round, grab the answer and update known information\n",
        "#             last_answer = obs[\"answers\"][-1]\n",
        "#             if last_answer == 'yes':\n",
        "#                 self.known_info.append(f'a {self.last_key_attribute}')\n",
        "#             else:\n",
        "#                 self.known_info.append(f'not a {self.last_key_attribute}')\n",
        "#                 self.known_info.append(f'either a {self.else_categories[0]} or a {self.else_categories[1]}')\n",
        "#         else:\n",
        "#             # other rounds, update known information\n",
        "#             last_answer = obs['answers'][-1]\n",
        "#             if last_answer == 'yes':\n",
        "#                 self.known_info.append(f'is {self.last_key_attribute}')\n",
        "#             else:\n",
        "#                 self.known_info.append(f'is not {self.last_key_attribute}')\n",
        "\n",
        "#         # # Update guessed keywords\n",
        "#         # self.known_info.append(f'keyword is not {self.last_guess}')\n",
        "#         # self.known_info = list(set(self.known_info))\n",
        "\n",
        "#         print(f\"\\nkey attribute: {self.last_key_attribute}, guess: {self.last_guess}\")\n",
        "\n",
        "#         # Formatting prompt with observations\n",
        "#         self._format_prompt(obs)\n",
        "#         prompt = str(self.formatter)\n",
        "#         print(f\"\\nPrompt: {prompt}\")\n",
        "#         # Getting response from LLM\n",
        "#         response = self.model.generate(prompt, device=self.device, output_len=self.output_len)\n",
        "#         print(f'\\nTurn Type: {obs[\"turnType\"]}\\nResponse: {response}')\n",
        "\n",
        "#         # parse response and update information if needed\n",
        "#         parse_dict = self._parse_response(response)\n",
        "#         print(f'\\nParse_dict: {parse_dict}')\n",
        "\n",
        "#         # if in an ask turn, try to grab the key attribute from the response and update last key attribute for the next information updating\n",
        "#         if obs[\"turnType\"] == 'ask':\n",
        "#             # successfully grab the key attribute from the response\n",
        "#             if parse_dict['key attribute']:\n",
        "#                 self.last_key_attribute = parse_dict['key attribute']\n",
        "#             # no key attribute parsed from the response, directly use question as key attribute\n",
        "#             elif parse_dict['question']:\n",
        "#                 self.last_key_attribute  = parse_dict['question']\n",
        "#                 ret = parse_dict['question']\n",
        "#                 return ret\n",
        "#             # worst case, there is neither key attribute nor question parsed from the response, randomly choose an attribute\n",
        "#             else:\n",
        "#                 random_alphabet = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "#                 ret = f'is it started with alphabet {random_alphabet}?'\n",
        "#                 self.last_key_attribute = f'started with alphabet {random_alphabet}'\n",
        "#                 return ret\n",
        "\n",
        "#             # parsed response has question, use question as response\n",
        "#             if parse_dict['question']:\n",
        "#                 ret = parse_dict['question']\n",
        "#             # parsed response has no question but have key attribute, use key attribute as question\n",
        "#             else:\n",
        "\n",
        "#                 ret = f'is it {self.last_key_attribute}?'\n",
        "#         # if in a guess turn, try to grab the guess from the response and update last guess for the next information updating\n",
        "#         elif obs[\"turnType\"] == 'guess':\n",
        "#             if parse_dict['guess']:\n",
        "#                 self.last_guess = parse_dict['guess']\n",
        "#                 ret = parse_dict['guess']\n",
        "#             # if there is no guess parsed from the response, return empty string\n",
        "#             else:\n",
        "#                 ret = ''\n",
        "#         else:\n",
        "#             raise ValueError('Invalid turnType.')\n",
        "\n",
        "#         return ret\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### new version"
      ],
      "metadata": {
        "id": "jBfVMbYkXLJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import parse\n",
        "import random\n",
        "\n",
        "class GemmaAgent:\n",
        "    def __init__(self, model_variant, device='cuda:0', env=\"kaggle\", output_len=200, system_prompt=None, few_shot_examples=None, example_sample_num=None):\n",
        "        # model initialization\n",
        "        self.device = device\n",
        "        self.model_variant = model_variant\n",
        "        self.few_shot_exmaples_ask = few_shot_examples[0]\n",
        "        self.few_shot_exmaples_guess = few_shot_examples[1]\n",
        "        self.example_sample_num = example_sample_num\n",
        "\n",
        "        WEIGHTS_PATH = self._set_up_env(env)\n",
        "\n",
        "        # Ensure that the tokenizer is present\n",
        "        tokenizer_path = os.path.join(WEIGHTS_PATH, 'tokenizer.model')\n",
        "        assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
        "\n",
        "        # Ensure that the checkpoint is present\n",
        "        ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{model_variant}.ckpt')\n",
        "        assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\n",
        "\n",
        "        # loading model configuration\n",
        "        model_config = get_config_for_2b() if \"2b\" in model_variant else get_config_for_7b()\n",
        "        model_config.quant = \"quant\" in model_variant\n",
        "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
        "\n",
        "        with _set_default_tensor_type(model_config.get_dtype()):\n",
        "            self.model = GemmaForCausalLM(model_config)\n",
        "            self.model.load_weights(ckpt_path)\n",
        "            self.model = self.model.to(self.device).eval()\n",
        "\n",
        "        # agent args\n",
        "        self.formatter = PromptFormatter(system_prompt=system_prompt,\n",
        "                                         few_shot_examples=few_shot_examples[0],\n",
        "                                         sample_num=self.example_sample_num)\n",
        "        self.round_num = 0\n",
        "        self.known_info = []\n",
        "        self.last_key_attribute = None\n",
        "        self.last_guess = None\n",
        "        self.output_len = output_len\n",
        "\n",
        "    def _set_up_env(self, env):\n",
        "\n",
        "        if env == 'kaggle':\n",
        "            print(\"Loading model in Kaggle, model weights will be searched within local directories.\")\n",
        "\n",
        "            # kaggle configuration\n",
        "            KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
        "            if os.path.exists(KAGGLE_AGENT_PATH):\n",
        "                WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, f\"gemma/pytorch/{self.model_variant}/2\")\n",
        "            else:\n",
        "                WEIGHTS_PATH = f\"/kaggle/input/gemma/pytorch/{self.model_variant}/2\"\n",
        "\n",
        "        elif env == 'colab':\n",
        "            print(\"Loading model in Colab, starting from downloading the model weights.\")\n",
        "\n",
        "            WEIGHTS_PATH = kagglehub.model_download(f'google/gemma/pyTorch/{self.model_variant}')\n",
        "        else:\n",
        "            raise ValueError(\"Argument 'env' should be in ['kaggle', 'colab']\")\n",
        "\n",
        "        return WEIGHTS_PATH\n",
        "\n",
        "    def _parse_response(self, response : str):\n",
        "      raise NotImplementedError\n",
        "\n",
        "    def _format_prompt(self, obs):\n",
        "      raise NotImplementedError\n",
        "\n",
        "# TODO: to be done\n",
        "class GemmaAgent_Answer(GemmaAgent):\n",
        "\n",
        "    def _parse_response(self, response : str):\n",
        "        pass\n",
        "\n",
        "    def _format_prompt(self, obs):\n",
        "\n",
        "\n",
        "    def __call__(self, obs, cfg=None):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "class GemmaAgent_Guesser(GemmaAgent):\n",
        "\n",
        "    def _parse_response(self, response : str):\n",
        "        '''\n",
        "            parse the response into a dictionary.\n",
        "            may contain three keys: key_attribute, question, guess and their value respectively.\n",
        "            e.g.: {\"key_attribute\": 'a country', \"question\": 'Is it a country?'} for a parsed asker response.\n",
        "        '''\n",
        "        pattern = re.compile(r'\\*\\*([^*]+)\\*\\*:?\\s*(.*?)(?=\\*\\*|$)', re.DOTALL)\n",
        "        matches = pattern.findall(response.lower())\n",
        "        parse_dict = {'key attribute':None, 'question':None, 'guess':None}\n",
        "        for k, v in matches:\n",
        "          if ('attr' in k) and (parse_dict['key attribute'] == None):\n",
        "            parse_dict['key attribute'] = v\n",
        "          elif ('ques' in k) and (parse_dict['question'] == None):\n",
        "            parse_dict['question'] = v\n",
        "          elif ('guess' in k) and (parse_dict['guess'] == None):\n",
        "            parse_dict['guess'] = v\n",
        "\n",
        "        return parse_dict\n",
        "\n",
        "    def _format_prompt(self, obs):\n",
        "        if obs[\"turnType\"] == 'ask':\n",
        "          self.formatter._few_shot_examples = self.few_shot_exmaples_ask\n",
        "        if obs[\"turnType\"] == 'guess':\n",
        "          self.formatter._few_shot_examples = self.few_shot_exmaples_guess\n",
        "        # reset formatter\n",
        "        self.formatter.reset()\n",
        "        self.formatter.add_new_round('user', 'Remember how to use CoT as above and the format of answering, now let us play a new game from the begining.', True)\n",
        "        # add played rounds\n",
        "        rounds = interleave_unequal(obs[\"questions\"], obs[\"answers\"])\n",
        "        self.formatter.add_rounds(rounds, start_agent='user')\n",
        "        if obs[\"turnType\"] == 'ask':\n",
        "            self.formatter.add_new_round('user',\n",
        "                                         'Now it is your turn to ask a question. Please construct a new question based on what you have known and your common sense with your reasoning. Do not forget to indicate key attribute, question with double asterisk (**?**) and their content with double quotation markers (\"?\"), such as **Key Attribute** \"your attribute\" and **Question** \"your question\"',\n",
        "                                         True)\n",
        "        elif obs[\"turnType\"] == 'guess':\n",
        "            self.formatter.add_new_round('user',\n",
        "                                         'Now guess the keyword based on your analysis with reasoning and known information. And surround a pointer of guess with double asterisks (**?**) and your guessed keyword with doublr quotation markers (\"?\") in the end, such as **Guess** \"your guess\"',\n",
        "                                         True)\n",
        "        # start of model response\n",
        "        self.formatter.add_new_round('model', f'Given information: {str(self.known_info)}', False)\n",
        "\n",
        "    def __call__(self, obs, cfg=None):\n",
        "        '''\n",
        "            Main function to interact with the model.\n",
        "                1. update known information based on the observation and round number\n",
        "                2. format the prompt with the observation\n",
        "                3. generate response from the model\n",
        "                4. parse the response and update information if needed\n",
        "        '''\n",
        "        print(f\"=========== round: {self.round_num} ===========\")\n",
        "        self.round_num += 1\n",
        "        # Update known information\n",
        "        if self.round_num - 1 == 0:\n",
        "            # for the first round, randomly choose an attribute as initialization\n",
        "            initial_categories = ['person', 'thing', 'place']\n",
        "            self.last_key_attribute = random.choice(initial_categories)\n",
        "            self.else_categories = list(set(initial_categories) - set([self.last_key_attribute]))\n",
        "            response = f'Is it a {self.last_key_attribute}?'\n",
        "            round\n",
        "            return response\n",
        "        elif self.round_num - 1 == 1:\n",
        "            # if it is the second round, grab the answer and update known information\n",
        "            last_answer = obs[\"answers\"][-1]\n",
        "            if last_answer == 'yes':\n",
        "                self.known_info.append(f'a {self.last_key_attribute}')\n",
        "            else:\n",
        "                self.known_info.append(f'not a {self.last_key_attribute}')\n",
        "                self.known_info.append(f'either a {self.else_categories[0]} or a {self.else_categories[1]}')\n",
        "        else:\n",
        "            # other rounds, update known information\n",
        "            last_answer = obs['answers'][-1]\n",
        "            if last_answer == 'yes':\n",
        "                self.known_info.append(f'is {self.last_key_attribute}')\n",
        "            else:\n",
        "                self.known_info.append(f'is not {self.last_key_attribute}')\n",
        "\n",
        "        # # Update guessed keywords\n",
        "        # self.known_info.append(f'keyword is not {self.last_guess}')\n",
        "        # self.known_info = list(set(self.known_info))\n",
        "\n",
        "        print(f\"\\nkey attribute: {self.last_key_attribute}, guess: {self.last_guess}\")\n",
        "\n",
        "        # Formatting prompt with observations\n",
        "        self._format_prompt(obs)\n",
        "        prompt = str(self.formatter)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        # Getting response from LLM\n",
        "        response = self.model.generate(prompt, device=self.device, output_len=self.output_len)\n",
        "        print(f'\\nTurn Type: {obs[\"turnType\"]}\\nResponse: {response}')\n",
        "\n",
        "        # parse response and update information if needed\n",
        "        parse_dict = self._parse_response(response)\n",
        "        print(f'\\nParse_dict: {parse_dict}')\n",
        "\n",
        "        # if in an ask turn, try to grab the key attribute from the response and update last key attribute for the next information updating\n",
        "        if obs[\"turnType\"] == 'ask':\n",
        "            # successfully grab the key attribute from the response\n",
        "            if parse_dict['key attribute']:\n",
        "                self.last_key_attribute = parse_dict['key attribute']\n",
        "            # no key attribute parsed from the response, directly use question as key attribute\n",
        "            elif parse_dict['question']:\n",
        "                self.last_key_attribute  = parse_dict['question']\n",
        "                ret = parse_dict['question']\n",
        "                return ret\n",
        "            # worst case, there is neither key attribute nor question parsed from the response, randomly choose an attribute\n",
        "            else:\n",
        "                random_alphabet = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
        "                ret = f'is it started with alphabet {random_alphabet}?'\n",
        "                self.last_key_attribute = f'started with alphabet {random_alphabet}'\n",
        "                return ret\n",
        "\n",
        "            # parsed response has question, use question as response\n",
        "            if parse_dict['question']:\n",
        "                ret = parse_dict['question']\n",
        "            # parsed response has no question but have key attribute, use key attribute as question\n",
        "            else:\n",
        "\n",
        "                ret = f'is it {self.last_key_attribute}?'\n",
        "        # if in a guess turn, try to grab the guess from the response and update last guess for the next information updating\n",
        "        elif obs[\"turnType\"] == 'guess':\n",
        "            if parse_dict['guess']:\n",
        "                self.last_guess = parse_dict['guess']\n",
        "                ret = parse_dict['guess']\n",
        "            # if there is no guess parsed from the response, return empty string\n",
        "            else:\n",
        "                ret = ''\n",
        "        else:\n",
        "            raise ValueError('Invalid turnType.')\n",
        "\n",
        "        return ret\n",
        "\n",
        "\n",
        "def get_agent(name):\n",
        "    global agent\n",
        "\n",
        "    if agent is None and name == 'questioner':\n",
        "        agent = GemmaAgent_Guesser(model_variant=VARIANT,\n",
        "                                   device=MACHINE_TYPE,\n",
        "                                   env=ENV,\n",
        "                                   output_len=100,\n",
        "                                   system_prompt=sys_prompt,\n",
        "                                   few_shot_examples=[few_shot_examples_ask, few_shot_examples_guess])\n",
        "    elif agent is None and name == 'answerer':\n",
        "        agent = GemmaAgent_Answer(model_variant=VARIANT,\n",
        "                                  device=MACHINE_TYPE,\n",
        "                                  env=ENV,\n",
        "                                  output_len=10,\n",
        "                                  system_prompt=sys_prompt)\n",
        "    assert agent is not None, 'Agent not found!'\n",
        "    return agent\n",
        "\n",
        "def get_fn(obs, config):\n",
        "    if obs['turnType'] == \"ask\":\n",
        "        response = get_agent('questioner')(obs)\n",
        "    elif obs['turnType'] == \"guess\":\n",
        "        response = get_agent('questioner')(obs)\n",
        "    elif obs['turnType'] == \"answer\":\n",
        "        response = get_agent('answerer')(obs)\n",
        "    if response is None or len(response) <= 1:\n",
        "        return \"yes\"\n",
        "    else:\n",
        "        return response"
      ],
      "metadata": {
        "id": "T6XJEeRxXK6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug"
      ],
      "metadata": {
        "id": "KX1Zbm8X5IUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install kaggle_environments\n",
        "import kaggle_environments"
      ],
      "metadata": {
        "id": "z0D3B9ks51xa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy agent\n",
        "def simple_agent1(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs['turnType'] == \"ask\": response = \"Is it a duck?\"\n",
        "    elif obs.turnType == \"guess\": response = \"duck\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response\n",
        "\n",
        "def simple_agent2(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs.turnType == \"ask\": response = \"Is it a bird?\"\n",
        "    elif obs.turnType == \"guess\": response = \"bird\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response\n",
        "\n",
        "def simple_agent3(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
        "    elif obs.turnType == \"guess\": response = \"pig\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response\n",
        "\n",
        "def simple_agent4(obs, cfg):\n",
        "    # if agent is guesser and turnType is \"ask\"\n",
        "    if obs.turnType == \"ask\": response = \"Is it a cow?\"\n",
        "    elif obs.turnType == \"guess\": response = \"cow\"\n",
        "    elif obs.turnType == \"answer\": response = \"no\"\n",
        "    return response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6ozTUpm5MUw",
        "outputId": "6579c1bc-7e7a-4a40-e065-446d6a5e0512"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "debug_config = {'episodeSteps': 10,     # initial step plus 3 steps per round (ask/answer/guess)\n",
        "                'actTimeout': 5,       # agent time per round in seconds; default is 60\n",
        "                'runTimeout': 60,      # max time for the episode in seconds; default is 1200\n",
        "                'agentTimeout': 3600}  # obsolete field; default is 3600\n",
        "env = kaggle_environments.make(\"llm_20_questions\", debug_config, debug=True)\n",
        "print(f'the key word is: {kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword}\\nwith alts as {kaggle_environments.envs.llm_20_questions.llm_20_questions.alts}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vJhIFse_rnX",
        "outputId": "9d175371-5ad1-47ec-f7e8-2e5b6adfdb43"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the key word is: Utility Box\n",
            "with alts as []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Choose variant and machine type\n",
        "VARIANT = '7b-it-quant'\n",
        "MACHINE_TYPE = 'cuda'\n",
        "agent = None\n"
      ],
      "metadata": {
        "id": "ti92oAf6CrEb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Define"
      ],
      "metadata": {
        "id": "TMGl-sD9xqqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = f'You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. \\\n",
        "You need to utilize your knowledge and reasoning skills to play a game of 20 questions. \\\n",
        "In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"there is a cross on the flag.\" \\\n",
        "Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" \\\n",
        "And after your thorough thinking, you need to wrap some key point with double stars and their corresponding content with double quotation marks (such as **key point**: \"corresponding content\"). \\\n",
        "Baiscally it should follows the format similar to within 100 words: **Key attribute**:\"your chosen key attribute\"\\n**Question**:\"your formatted question regarding to the key attribute\"\\n**Reasoning**:\"your reasoning abput why this key attribute can provide you the most information gain\" \\\n",
        " Here are few examples of how to use Chain of thought:'\n",
        "\n",
        "few_shot_examples_ask = [\n",
        "\n",
        "# Round 1 | Keyword: Yangtze River\n",
        "'''\n",
        "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\n",
        "**Key attribute**: \"man-made structure\"\n",
        "**Question**: \"Is it a man-made structure?\"\n",
        "****Reasoning****:\n",
        "Man-made vs. Natural: This question helps to differentiate between natural landmarks (e.g., Grand Canyon) and man-made structures (e.g., Eiffel Tower).\n",
        "Broad Categories: Man-made structures include a wide range of possibilities (e.g., buildings, monuments), whereas cities are all man-made but represent a different category.\n",
        "Even Split: The distinction between natural and man-made provides an even split, maximizing information gain by effectively narrowing down the possibilities based on the answer.\n",
        "''',\n",
        "\n",
        "'''\n",
        "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\n",
        "**Key attribute**: \"in Asia\"\n",
        "**Question**: \"Is it located in Asia?\"\n",
        "****Reasoning****:\n",
        "Geographic Focus: Identifying the continent will significantly narrow down the possible natural landmarks.\n",
        "Large Landmarks: Asia has several major natural landmarks (e.g., Mount Everest, Great Wall).\n",
        "Even Split: Given the global distribution of landmarks, this question will effectively split the remaining possibilities.\n",
        "''',\n",
        "\n",
        "'''\n",
        "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\n",
        "**Key attribute**: \"longest river in China\"\n",
        "**Question**: \"Is it the longest river in China?\"\n",
        "****Reasoning****:\n",
        "Length-Specific Focus: Knowing whether it is the longest river will help confirm or eliminate the Yangtze River.\n",
        "Major Rivers: The Yangtze River is the longest river in China, followed by the Yellow River.\n",
        "Effective Split: This question will help confirm or deny one of the most prominent landmarks in China, providing a clear yes/no split.',\n",
        "''',\n",
        "\n",
        "\n",
        "# Round 2 | Keyword: Congo\n",
        "'''\n",
        "Known information: [\"is a country\"]\n",
        "**Key attribute**: \"in Europe\"\n",
        "**Question**: \"Is this country in Europe?\"\n",
        "**Reasoning**:\n",
        "Continental Focus: Knowing the continent will help significantly narrow down the list of possible countries.\n",
        "Effective Reduction: Europe has many countries, so confirming or eliminating Europe will reduce the search space.\n",
        "Balanced Distribution: Countries are evenly distributed across continents, providing an effective yes/no split.\n",
        "''',\n",
        "\n",
        "'''\n",
        "Known information: [\"is a country\", \"not in Europe\", \"not in Asia\", \"in Africa\", \"not in Northern Africa\", \"not in Eastern Africa\", \"not in Western Africa\", \"in Southern Africa\"]\n",
        "**Key attribute**: \"landlocked country\"\n",
        "**Question**: \"Is this country landlocked?\"\n",
        "**Reasoning**:\n",
        "To further narrow down the possibilities within Southern Africa, I will focus on another well-known country in that region.\n",
        "Geographical Focus: Knowing whether the country is landlocked will significantly narrow down the possibilities within Southern Africa.\n",
        "Effective Reduction: There are several landlocked countries in Southern Africa, so confirming or eliminating this will help focus the search.\n",
        "Balanced Distribution: This provides a clear yes/no split, effectively narrowing down the search.\n",
        "''',\n",
        "\n",
        "'''\n",
        "Known information: [\"is a country\", \"not in Europe\", \"not in Asia\", \"in Africa\", \"not in Northern Africa\", \"not in Eastern Africa\", \"not in Western Africa\", \"in Southern Africa\", \"not landlocked\", \"Portuguese is not an official language\"]\n",
        "**Key attribute**: \"borders the Indian Ocean\"\n",
        "**Question**: \"Does this country border the Indian Ocean?\"\n",
        "**Reasoning**:\n",
        "Geographic Focus: Knowing whether the country borders the Indian Ocean can help narrow down the options.\n",
        "Effective Reduction: There are only a few countries in Southern Africa that border the Indian Ocean.\n",
        "Balanced Distribution: This question provides a clear yes/no split, effectively narrowing down the search.\n",
        " ''',\n",
        "\n",
        "# round 3 | Keyword: Ryan\n",
        "'''\n",
        "Known information: [\"not is a place\", \"is a person\"]\n",
        "**Key attribute**: \"a historical figure\"\n",
        "**Question**: \"Is this person a historical figure?\"\n",
        "**Reasoning**:\n",
        "Time Frame: Identifying whether the person is historical can significantly narrow down the possibilities.\n",
        "Effective Reduction: This helps focus on either historical or contemporary figures.\n",
        "Balanced Distribution: This question provides a clear yes/no split, guiding the search effectively.\n",
        "''',\n",
        "\n",
        "'''\n",
        "Known information: [\"not is a place\", \"is a person\", \"is a historical figure\", \"is involved in the entertainment industry.\", \"is not primarily known for their work in music.\", \"is primarily known for their work as an actor.\", \"has not won an Academy Award (Oscar).\"]\n",
        "**Key attribute**: \"primarily known for television work\"\n",
        "**Question**: \"Is this person primarily known for their work in television?\"\n",
        "**Reasoning**:\n",
        "Medium Focus: Identifying whether the person is known for television can help narrow down the possibilities.\n",
        "Effective Reduction: This helps distinguish between actors primarily known for television versus those known for film.\n",
        "Balanced Distribution: This question provides a clear yes/no split, effectively guiding the search.\n",
        "''',\n",
        "\n",
        "'''\n",
        "Known information: [\"not is a place\", \"is a person\", \"is a historical figure\", \"is involved in the entertainment industry.\", \"is not primarily known for their work in music.\", \"is primarily known for their work as an actor.\", \"has not won an Academy Award (Oscar).\", \"is not primarily known for their work in television.\", \"is primarily known for their work in action movies.\", \"is associated with a major action movie franchise.\"]\n",
        "**Key attribute**: \"known for science fiction action movies\"\n",
        "**Question**: \"Is this person known for their work in science fiction action movies?\"\n",
        "**Reasoning**:\n",
        "Genre Specificity: Identifying whether the person is known for science fiction action movies can help narrow down the possibilities.\n",
        "Effective Reduction: This helps distinguish between different types of action movie genres, such as sci-fi, fantasy, or military.\n",
        "Balanced Distribution: This question provides a clear yes/no split, guiding the search effectively.\n",
        "''',\n",
        "\n",
        "# round 4 | Keyword: Noosa\n",
        "'''\n",
        "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\"]\n",
        "**Key attribute**: \"a capital city\"\n",
        "**Question**: \"Is it a capital city?\"\n",
        "**Reasoning**:\n",
        "Geographic Focus: This question helps determine if the city is a capital, significantly narrowing the possibilities.\n",
        "Effective Reduction: If the answer is yes, it focuses on capital cities, eliminating all non-capital cities. If no, it eliminates all capital cities from consideration.\n",
        "Context Relevance: Knowing whether the city is a capital helps tailor subsequent questions to specific types of cities.\n",
        "Balanced Distribution: Capital cities are a smaller subset of cities, providing an effective yes/no split.\n",
        "''',\n",
        "\n",
        "# '''\n",
        "# Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere.\"]\n",
        "# **Key attribute**: \"in the Southern Hemisphere\"\n",
        "# **Question**: \"Is this city in the Southern Hemisphere?\"\n",
        "# **Reasoning**:\n",
        "# Hemispheric Focus: This question splits the world into two large, nearly equal parts, effectively narrowing down possibilities.\n",
        "# Effective Reduction: If the answer is yes, it narrows the focus to cities in the Southern Hemisphere, eliminating those in the Northern Hemisphere. If no, it focuses on Northern Hemisphere cities.\n",
        "# Balanced Distribution: The Earth is divided evenly by the equator, providing a clear yes/no split that maximizes information gain.\n",
        "# ''',\n",
        "\n",
        "\n",
        "'''\n",
        "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city.\"]\n",
        "**Key attribute**: \"a well-known tourist destination\"\n",
        "**Question**: \"Is this city a well-known tourist destination?\"\n",
        "**Reasoning**:\n",
        "Tourism Focus: This question helps determine if the city is popular with tourists, which can narrow down the list of coastal cities.\n",
        "Effective Reduction: If the answer is yes, it focuses on well-known tourist cities, eliminating less-known cities. If no, it narrows down to less-touristic coastal cities.\n",
        "Context Relevance: Many of Australia's coastal cities are also popular tourist destinations, making this an important distinction.\n",
        "Balanced Distribution: Tourist versus non-tourist cities provide a clear yes/no split.\n",
        "''',\n",
        "\n",
        "# '''\n",
        "# Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city\", \"is a well-known tourist destination\", \"is not Gold Coast\", \"is not in New South Wales\", \"is in Queensland\", \"is not associated with the Great Barrier Reef\"]\n",
        "# **Key attribute**: \"popular for its beaches\"\n",
        "# **Question**: \"Is this city popular for its beaches?\"\n",
        "# **Reasoning**:\n",
        "# Tourism Focus: Identifying if the city is known for its beaches can help narrow down the possibilities.\n",
        "# Effective Reduction: If the answer is yes, it focuses on coastal cities popular for their beaches, eliminating inland cities or those known for other attractions. If no, it narrows down to other types of tourist destinations.\n",
        "# Context Relevance: Many coastal cities in Queensland are known for their beaches, making this a significant distinction.\n",
        "# Balanced Distribution: Coastal versus inland tourist destinations provide a clear yes/no split.\n",
        "# ''',\n",
        "\n",
        "'''\n",
        "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city\", \"is a well-known tourist destination\", \"is not Gold Coast\", \"is not in New South Wales\", \"is in Queensland\", \"is not associated with the Great Barrier Reef\", \"popular for its beaches\"]\n",
        "**Key attribute**: \"located on the Sunshine Coast\"\n",
        "**Question**: \"Is this city located on the Sunshine Coast?\"\n",
        "**Reasoning**:\n",
        "Regional Focus: Identifying if the city is on the Sunshine Coast can significantly narrow down the possibilities.\n",
        "Effective Reduction: If the answer is yes, it focuses on cities on the Sunshine Coast, eliminating cities in other coastal regions of Queensland. If no, it narrows down to other coastal areas.\n",
        "Context Relevance: The Sunshine Coast is known for its beach destinations, making this a significant distinction.\n",
        "Balanced Distribution: This question provides a clear yes/no split, effectively guiding the search.\n",
        "'''\n",
        "]\n",
        "\n",
        "# few_shot_examples_ask = [\n",
        "\n",
        "# 'Now give me an example of thinking with the Chain of Thought',\n",
        "\n",
        "# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\\n\\\n",
        "# **Key attribute**: \"man-made structure\" \\\n",
        "# **Question**: \"Is it a man-made structure?\" \\\n",
        "# **Reasoning**: Man-made vs. Natural: This question helps to differentiate between natural landmarks (e.g., Grand Canyon) and man-made structures (e.g., Eiffel Tower). \\\n",
        "# Broad Categories: Man-made structures include a wide range of possibilities (e.g., buildings, monuments), whereas cities are all man-made but represent a different category. \\\n",
        "# Even Split: The distinction between natural and man-made provides an even split, maximizing information gain by effectively narrowing down the possibilities based on the answer.',\n",
        "\n",
        "# 'no',\n",
        "\n",
        "# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\"]\\n\\\n",
        "# **Key attribute**: \"in North America\" \\\n",
        "# **Question**: \"Is it located in North America?\" \\\n",
        "# **Reasoning**: Continental Location: Knowing the continent helps narrow down the options to specific regions.\\\n",
        "# Significant Reduction: This can significantly reduce the number of potential landmarks.\\\n",
        "# Balanced Distribution: The world major landmarks are fairly evenly distributed across continents, providing a balanced yes/no split.',\n",
        "\n",
        "# 'no',\n",
        "\n",
        "# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\\n\\\n",
        "# **Key attribute**: \"in Asia\" \\\n",
        "# **Question**: \"Is it located in Asia?\" \\\n",
        "# **Reasoning**: Geographic Focus: Identifying the continent will significantly narrow down the possible natural landmarks. \\\n",
        "# Large Landmarks: Asia has several major natural landmarks (e.g., Mount Everest, Great Wall). \\\n",
        "# Even Split: Given the global distribution of landmarks, this question will effectively split the remaining possibilities.',\n",
        "\n",
        "# 'yes',\n",
        "\n",
        "# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\"]\\n\\\n",
        "# **Key attribute**: \"in China\" \\\n",
        "# **Question**: \"Is it located in China?\" \\\n",
        "# **Reasoning**: Country-Specific Focus: Knowing the specific country will help narrow down the landmark significantly. \\\n",
        "# Major Landmarks: China has several well-known natural landmarks (e.g., Yangtze River, Mount Everest on the border). \\\n",
        "# Even Split: Given the number of countries in Asia with significant natural landmarks, this question provides a balanced yes/no split.',\n",
        "\n",
        "# 'yes',\n",
        "\n",
        "# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\"]\\n\\\n",
        "#  **Key attribute**: \"a river\" \\\n",
        "# **Question**: \"Is it a river?\" \\\n",
        "# **Reasoning**: Type-Specific Focus: Knowing whether the natural landmark is a river will help narrow down the possibilities significantly. \\\n",
        "# Major Landmarks: China has several famous rivers (e.g., Yangtze River, Yellow River). \\\n",
        "# Even Split: This question provides a balanced yes/no split, effectively narrowing down the possibilities.',\n",
        "\n",
        "# 'yes',\n",
        "\n",
        "# 'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\\n\\\n",
        "# **Key attribute**: \"longest river in China\" \\\n",
        "# **Question**: \"Is it the longest river in China?\" \\\n",
        "# **Reasoning**: Length-Specific Focus: Knowing whether it is the longest river will help confirm or eliminate the Yangtze River. \\\n",
        "# Major Rivers: The Yangtze River is the longest river in China, followed by the Yellow River. \\\n",
        "# Effective Split: This question will help confirm or deny one of the most prominent landmarks in China, providing a clear yes/no split.',\n",
        "\n",
        "# 'yes',\n",
        "\n",
        "# 'It is the Yangtze River'\n",
        "\n",
        "# 'Correct!']\n",
        "\n",
        "few_shot_examples_guess = [\n",
        "'Now give me an example of guessing with the reasoning',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark rather than a city or a man-made structure. An example of a well-known natural landmark is the Grand Canyon.\\\n",
        "Guess: \"Grand Canyon\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark outside North America. An example of a famous natural landmark outside North America is Mount Everest.\\\n",
        "Guess: \"Mount Everest\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark in Asia. A famous natural landmark in Asia is the Himalayas.\\\n",
        "Guess: \"the Himalayas\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\"]\\n\\\n",
        "Given these clues, it is likely a famous natural landmark in China. An example of such a landmark is the Mekong River.\\\n",
        "Guess: \"the Mekong River\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\"]\\n\\\n",
        "Given these clues, it is likely a natural landmark in China that is not a mountain. An example of such a landmark could be the Tianchi Lake.\\\n",
        "Guess: \"the Tianchi Lake\"',\n",
        "\n",
        "'no',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\\n\\\n",
        "Given these clues, a very famous river in China is the Yellow River.\\\n",
        "Guess: \"the Yellow River\"',\n",
        "\n",
        "'Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\", \"is the longest river in China\"]\\n\\\n",
        "Given these clues, it should be the longest river in China which is the Yangtze River.\\\n",
        "Guess: \"the Yangtze River\"',\n",
        "\n",
        "'Correct!']"
      ],
      "metadata": {
        "id": "je07rQu9xttt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runing Test"
      ],
      "metadata": {
        "id": "gOi1VlrCxviH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# agent = GemmaAgent_Guesser(VARIANT, MACHINE_TYPE, 'colab', 100, sys_prompt, [few_shot_examples_ask, few_shot_examples_guess], None)\n",
        "agent.output_len = 100\n",
        "# agent.example_sample_num = None\n",
        "agent.formatter._sys_prompt = sys_prompt\n",
        "agent.formatter._few_shot_examples = few_shot_examples_ask\n",
        "agent.round_num = 4\n",
        "agent.last_key_attribute = 'a person'\n",
        "agent.last_guess = 'pig'\n",
        "agent.known_info = ['not a place', 'either a person or a thing', 'not a thing']\n",
        "# print(str(agent.formatter))\n",
        "obs = {'remainingOverageTime': 300, 'questions': ['Is it a place?', 'Is it a thing?', 'Is it a person?'], 'guesses': ['pig', 'pig', 'pig'], 'answers': ['no', 'no', 'yes'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}\n",
        "start = time.time()\n",
        "agent(obs)\n",
        "print(f\"TIME: {(time.time()-start):.4f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygzpOabZM7ub",
        "outputId": "99aba7e5-6332-498f-e1c6-3b1e3e720c68"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========== round: 4 ===========\n",
            "\n",
            "key attribute: a person, guess: pig\n",
            "\n",
            "Prompt: <start_of_turn>user\n",
            "You are a highly knowledgeable naturalist with extensive knowledge about objects, places, and people around the world, almost like a search engine. You also possess strong reasoning abilities, allowing you to deduce the answer to your query using existing knowledge and information. You need to utilize your knowledge and reasoning skills to play a game of 20 questions. In each round of the game, choose the binary attribute most likely to aid your deduction, such as \"there is a cross on the flag.\" Using this attribute, ask a question in English. Generally, the most helpful attribute provides the maximum information gain, meaning it significantly reduces the entropy of your guess, regardless of whether the answer is **yes** or **no**. For example, if you believe asking about the color is most helpful for your deduction, you should ask, \"Is it a thing with a red color?\" And after your thorough thinking, you need to wrap some key point with double stars and their corresponding content with double quotation marks (such as **key point**: \"corresponding content\"). Baiscally it should follows the format similar to within 100 words: **Key attribute**:\"your chosen key attribute\"\n",
            "**Question**:\"your formatted question regarding to the key attribute\"\n",
            "**Reasoning**:\"your reasoning abput why this key attribute can provide you the most information gain\"  Here are few examples of how to use Chain of thought:<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Here are some examples of analyzing with causal reasoning:<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\"]\n",
            "**Key attribute**: \"in Asia\" \n",
            "**Question**: \"Is it located in Asia?\" \n",
            "****Reasoning****: \n",
            "Geographic Focus: Identifying the continent will significantly narrow down the possible natural landmarks. \n",
            "Large Landmarks: Asia has several major natural landmarks (e.g., Mount Everest, Great Wall). \n",
            "Even Split: Given the global distribution of landmarks, this question will effectively split the remaining possibilities.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is a country\", \"not in Europe\", \"not in Asia\", \"in Africa\", \"not in Northern Africa\", \"not in Eastern Africa\", \"not in Western Africa\", \"in Southern Africa\", \"not landlocked\", \"Portuguese is not an official language\"]\n",
            "**Key attribute**: \"borders the Indian Ocean\"\n",
            "**Question**: \"Does this country border the Indian Ocean?\"\n",
            "**Reasoning**: \n",
            "Geographic Focus: Knowing whether the country borders the Indian Ocean can help narrow down the options.\n",
            "Effective Reduction: There are only a few countries in Southern Africa that border the Indian Ocean.\n",
            "Balanced Distribution: This question provides a clear yes/no split, effectively narrowing down the search.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is a country\", \"not in Europe\", \"not in Asia\", \"in Africa\", \"not in Northern Africa\", \"not in Eastern Africa\", \"not in Western Africa\", \"in Southern Africa\"]\n",
            "**Key attribute**: \"landlocked country\"\n",
            "**Question**: \"Is this country landlocked?\"\n",
            "**Reasoning**:\n",
            "To further narrow down the possibilities within Southern Africa, I will focus on another well-known country in that region.\n",
            "Geographical Focus: Knowing whether the country is landlocked will significantly narrow down the possibilities within Southern Africa.\n",
            "Effective Reduction: There are several landlocked countries in Southern Africa, so confirming or eliminating this will help focus the search.\n",
            "Balanced Distribution: This provides a clear yes/no split, effectively narrowing down the search.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere.\"]\n",
            "**Key attribute**: \"in the Southern Hemisphere\"\n",
            "**Question**: \"Is this city in the Southern Hemisphere?\"\n",
            "**Reasoning**:\n",
            "Hemispheric Focus: This question splits the world into two large, nearly equal parts, effectively narrowing down possibilities.\n",
            "Effective Reduction: If the answer is yes, it narrows the focus to cities in the Southern Hemisphere, eliminating those in the Northern Hemisphere. If no, it focuses on Northern Hemisphere cities.\n",
            "Balanced Distribution: The Earth is divided evenly by the equator, providing a clear yes/no split that maximizes information gain.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\"]\n",
            "**Key attribute**: \"man-made structure\" \n",
            "**Question**: \"Is it a man-made structure?\" \n",
            "****Reasoning****: \n",
            "Man-made vs. Natural: This question helps to differentiate between natural landmarks (e.g., Grand Canyon) and man-made structures (e.g., Eiffel Tower). \n",
            "Broad Categories: Man-made structures include a wide range of possibilities (e.g., buildings, monuments), whereas cities are all man-made but represent a different category. \n",
            "Even Split: The distinction between natural and man-made provides an even split, maximizing information gain by effectively narrowing down the possibilities based on the answer.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\"]\n",
            "**Key attribute**: \"a capital city\"\n",
            "**Question**: \"Is it a capital city?\"\n",
            "**Reasoning**:\n",
            "Geographic Focus: This question helps determine if the city is a capital, significantly narrowing the possibilities.\n",
            "Effective Reduction: If the answer is yes, it focuses on capital cities, eliminating all non-capital cities. If no, it eliminates all capital cities from consideration.\n",
            "Context Relevance: Knowing whether the city is a capital helps tailor subsequent questions to specific types of cities.\n",
            "Balanced Distribution: Capital cities are a smaller subset of cities, providing an effective yes/no split.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is a country\"]\n",
            "**Key attribute**: \"in Europe\"\n",
            "**Question**: \"Is this country in Europe?\"\n",
            "**Reasoning**: \n",
            "Continental Focus: Knowing the continent will help significantly narrow down the list of possible countries.\n",
            "Effective Reduction: Europe has many countries, so confirming or eliminating Europe will reduce the search space.\n",
            "Balanced Distribution: Countries are evenly distributed across continents, providing an effective yes/no split.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city\", \"is a well-known tourist destination\", \"is not Gold Coast\", \"is not in New South Wales\", \"is in Queensland\", \"is not associated with the Great Barrier Reef\", \"popular for its beaches\"]\n",
            "**Key attribute**: \"located on the Sunshine Coast\"\n",
            "**Question**: \"Is this city located on the Sunshine Coast?\"\n",
            "**Reasoning**:\n",
            "Regional Focus: Identifying if the city is on the Sunshine Coast can significantly narrow down the possibilities.\n",
            "Effective Reduction: If the answer is yes, it focuses on cities on the Sunshine Coast, eliminating cities in other coastal regions of Queensland. If no, it narrows down to other coastal areas.\n",
            "Context Relevance: The Sunshine Coast is known for its beach destinations, making this a significant distinction.\n",
            "Balanced Distribution: This question provides a clear yes/no split, effectively guiding the search.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city.\"]\n",
            "**Key attribute**: \"a well-known tourist destination\"\n",
            "**Question**: \"Is this city a well-known tourist destination?\"\n",
            "**Reasoning**:\n",
            "Tourism Focus: This question helps determine if the city is popular with tourists, which can narrow down the list of coastal cities.\n",
            "Effective Reduction: If the answer is yes, it focuses on well-known tourist cities, eliminating less-known cities. If no, it narrows down to less-touristic coastal cities.\n",
            "Context Relevance: Many of Australia's coastal cities are also popular tourist destinations, making this an important distinction.\n",
            "Balanced Distribution: Tourist versus non-tourist cities provide a clear yes/no split.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a country\", \"is either a city or a landmark\", \"is not a city\", \"not a man-made structure\", \"is not in North America\", \"is in Aisa\", \"is in China\", \"is a river\"]\n",
            "**Key attribute**: \"longest river in China\"\n",
            "**Question**: \"Is it the longest river in China?\"\n",
            "****Reasoning****: \n",
            "Length-Specific Focus: Knowing whether it is the longest river will help confirm or eliminate the Yangtze River.\n",
            "Major Rivers: The Yangtze River is the longest river in China, followed by the Yellow River.\n",
            "Effective Split: This question will help confirm or deny one of the most prominent landmarks in China, providing a clear yes/no split.',<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"not is a place\", \"is a person\"]\n",
            "**Key attribute**: \"a historical figure\"\n",
            "**Question**: \"Is this person a historical figure?\"\n",
            "**Reasoning**:\n",
            "Time Frame: Identifying whether the person is historical can significantly narrow down the possibilities.\n",
            "Effective Reduction: This helps focus on either historical or contemporary figures.\n",
            "Balanced Distribution: This question provides a clear yes/no split, guiding the search effectively.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"not is a place\", \"is a person\", \"is a historical figure\", \"is involved in the entertainment industry.\", \"is not primarily known for their work in music.\", \"is primarily known for their work as an actor.\", \"has not won an Academy Award (Oscar).\"]\n",
            "**Key attribute**: \"primarily known for television work\"\n",
            "**Question**: \"Is this person primarily known for their work in television?\"\n",
            "**Reasoning**:\n",
            "Medium Focus: Identifying whether the person is known for television can help narrow down the possibilities.\n",
            "Effective Reduction: This helps distinguish between actors primarily known for television versus those known for film.\n",
            "Balanced Distribution: This question provides a clear yes/no split, effectively guiding the search.\n",
            "\n",
            "Known information: [\"not is a place\", \"is a person\", \"is a historical figure\", \"is involved in the entertainment industry.\", \"is not primarily known for their work in music.\", \"is primarily known for their work as an actor.\", \"has not won an Academy Award (Oscar).\", \"is not primarily known for their work in television.\", \"is primarily known for their work in action movies.\", \"is associated with a major action movie franchise.\"]\n",
            "**Key attribute**: \"known for science fiction action movies\"\n",
            "**Question**: \"Is this person known for their work in science fiction action movies?\"\n",
            "**Reasoning**:\n",
            "Genre Specificity: Identifying whether the person is known for science fiction action movies can help narrow down the possibilities.\n",
            "Effective Reduction: This helps distinguish between different types of action movie genres, such as sci-fi, fantasy, or military.\n",
            "Balanced Distribution: This question provides a clear yes/no split, guiding the search effectively.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Known information: [\"is not a person\", \"is a place\", \"is not a country\", \"is a city\", \"is not a capital city\", \"is in the Southern Hemisphere\", \"not in Africa.\", \"not in South America.\", \"is in Australia or Oceania\", \"in Australia\", \"is a coastal city\", \"is a well-known tourist destination\", \"is not Gold Coast\", \"is not in New South Wales\", \"is in Queensland\", \"is not associated with the Great Barrier Reef\"]\n",
            "**Key attribute**: \"popular for its beaches\"\n",
            "**Question**: \"Is this city popular for its beaches?\"\n",
            "**Reasoning**:\n",
            "Tourism Focus: Identifying if the city is known for its beaches can help narrow down the possibilities.\n",
            "Effective Reduction: If the answer is yes, it focuses on coastal cities popular for their beaches, eliminating inland cities or those known for other attractions. If no, it narrows down to other types of tourist destinations.\n",
            "Context Relevance: Many coastal cities in Queensland are known for their beaches, making this a significant distinction.\n",
            "Balanced Distribution: Coastal versus inland tourist destinations provide a clear yes/no split.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Remember how to use CoT as above and the format of answering, now let us play a new game from the begining.<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Is it a place?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "no<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Is it a thing?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "no<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Is it a person?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "yes<end_of_turn>\n",
            "<start_of_turn>user\n",
            "Now it is your turn to ask a question. Please construct a new question based on what you have known and your common sense with your reasoning. Do not forget to indicate key attribute, question with double asterisk (**?**) and their content with double quotation markers (\"?\"), such as **Key Attribute** \"your attribute\" and **Question** \"your question\"<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Given information: ['not a place', 'either a person or a thing', 'not a thing', 'is a person']\n",
            "\n",
            "Turn Type: ask\n",
            "Response: \n",
            "\n",
            "**Key attribute:** \"a person\"\n",
            "**Question:** Is the thing a person?\n",
            "\n",
            "**Reasoning:**\n",
            "The given information includes the statement \"is a person,\" which suggests that the answer to the question should be yes or no based on whether the unknown object is a person or not.\n",
            "\n",
            "Parse_dict: {'key attribute': '\"a person\"\\n', 'question': 'is the thing a person?\\n\\n', 'guess': None}\n",
            "TIME: 58.6354s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "game_output = env.run(agents=[simple_agent1,simple_agent2, simple_agent3, simple_agent4])\n",
        "# print(game_output[9])\n",
        "env.render(mode=\"ipython\", width=600, height=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atPfPu0bD0rU",
        "outputId": "c3f42907-2150-4c2d-c578-b8d5ba73d8c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'action': 'duck', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'step': 9, 'questions': ['Is it a duck?', 'Is it a duck?', 'Is it a duck?'], 'guesses': ['duck', 'duck', 'duck'], 'answers': ['no', 'no', 'no'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}, 'status': 'DONE'}, {'action': '', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'questions': ['Is it a duck?', 'Is it a duck?', 'Is it a duck?'], 'guesses': ['duck', 'duck', 'duck'], 'answers': ['no', 'no', 'no'], 'role': 'answerer', 'turnType': 'answer', 'keyword': 'Utility Box', 'category': 'things'}, 'status': 'DONE'}, {'action': 'pig', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'questions': ['Is it a pig?', 'Is it a pig?', 'Is it a pig?'], 'guesses': ['pig', 'pig', 'pig'], 'answers': ['no', 'no', 'no'], 'role': 'guesser', 'turnType': 'ask', 'keyword': '', 'category': ''}, 'status': 'DONE'}, {'action': '', 'reward': 0, 'info': {}, 'observation': {'remainingOverageTime': 300, 'questions': ['Is it a pig?', 'Is it a pig?', 'Is it a pig?'], 'guesses': ['pig', 'pig', 'pig'], 'answers': ['no', 'no', 'no'], 'role': 'answerer', 'turnType': 'answer', 'keyword': 'Utility Box', 'category': 'things'}, 'status': 'DONE'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwifCN55FVMd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Recordings\n",
        "\n",
        "- 1 question deduction,**150** output length\n",
        "  - 72s❌, no indication of answer❌\n",
        "- 3 question deduction,**150** output length\n",
        "  - 72s❌, no indication of answer❌\n",
        "- **6** question deduction,**150** output length\n",
        "  - 83s❌, completed answer and indicator✅\n",
        "- 3 question deduction,**100** output length\n",
        "  - 52s✅, incompleted response❌\n",
        "  (maybe we can limit the length of reasoning within 100 character)\n",
        "- **6** question deduction,**100** output length\n",
        "  - 59s✅, incompleted response❌\n",
        "\n",
        "---\n",
        "\n",
        "**👇 Try to downsize the length of CoT example. (better few shot cases and system prompt)**\n",
        "\n",
        "---\n",
        "\n",
        "- **6** question deduction, **100** output length\n",
        "  - 51s✅, completed and fair reasoning✅\n",
        "- **6** question deduction, **150** output length\n",
        "  - 68s❌, similar result to above✅\n",
        "\n",
        "---\n",
        "\n",
        "**👇 Apparent path dependence, maybe we need multiple deduction of different cases instead of all steps within one game**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "20AcwLvPvRsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answerer Debug"
      ],
      "metadata": {
        "id": "Lqg5WQ_ojefY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Choose variant and machine type\n",
        "VARIANT = '7b-it-quant'\n",
        "MACHINE_TYPE = 'cuda'\n",
        "agent = None\n",
        "agent = GemmaAgent_Guesser(VARIANT, MACHINE_TYPE, 'colab', 50, sys_prompt,[ few_shot_examples_guess, few_shot_examples_ask], None)"
      ],
      "metadata": {
        "id": "GiZiAvjIjd8p",
        "outputId": "2c061c40-2869-49bb-80d2-26a8fb89746c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model in Colab, starting from downloading the model weights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = agent.model\n",
        "model.generate('please answer my question with only **yes** or **no**. No other answers allowed. Question: is Ryan Goslin a male?', device=MACHINE_TYPE, output_len=50)"
      ],
      "metadata": {
        "id": "ndZBJSmJlpCW",
        "outputId": "be0ed656-53b2-4a6a-bd84-9a4713340d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n**Yes/No:**\\n\\n[Your Answer Here]\\n```\\n\\n\\n**The answer should be:**\\n\\nYes\\n\\nRyan Goslin is a male.\\n```\\n\\n\\n**Please provide your answer below:**\\n\\n```\\n[Your Answer Here]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fce82ce12d71409fa792c15cb0aa06ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a07a2c0b04404ad6a56b7887335a3bc3",
              "IPY_MODEL_d2edf175776b4704b86fe1f2eb24db38",
              "IPY_MODEL_b989da0f8ba642d9ab3017dd0bd69856",
              "IPY_MODEL_acefd9c8b7304eb7863d506a6e2320c7",
              "IPY_MODEL_313f0471e77f471ebef8018cd99f2902"
            ],
            "layout": "IPY_MODEL_49d3864189fd4ac489f2e63848d24e82"
          }
        },
        "a07a2c0b04404ad6a56b7887335a3bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d9aaff2e2524d1eb4f28fae59f109d3",
            "placeholder": "​",
            "style": "IPY_MODEL_7b0741c3bc2940eb9e414291f99fc79e",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "d2edf175776b4704b86fe1f2eb24db38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_42c8724340bb4dca8e9104b50a5ceaff",
            "placeholder": "​",
            "style": "IPY_MODEL_9acbac1acf4e41e4baf098090f27d042",
            "value": ""
          }
        },
        "b989da0f8ba642d9ab3017dd0bd69856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_907c4f15adcd4747a916876f07a9df2e",
            "placeholder": "​",
            "style": "IPY_MODEL_b7f5db031e4c4f289dbd22102f95282f",
            "value": ""
          }
        },
        "acefd9c8b7304eb7863d506a6e2320c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_443d92615685447aa6d94ddd36467bd6",
            "style": "IPY_MODEL_e1eac1c081344634b4496b47a03f7412",
            "tooltip": ""
          }
        },
        "313f0471e77f471ebef8018cd99f2902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4cc78fb9ed9471b9af7d4ba113ac158",
            "placeholder": "​",
            "style": "IPY_MODEL_2d93d615bd324b78a19c1f9761bdb2a2",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "49d3864189fd4ac489f2e63848d24e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1d9aaff2e2524d1eb4f28fae59f109d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b0741c3bc2940eb9e414291f99fc79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42c8724340bb4dca8e9104b50a5ceaff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9acbac1acf4e41e4baf098090f27d042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "907c4f15adcd4747a916876f07a9df2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f5db031e4c4f289dbd22102f95282f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "443d92615685447aa6d94ddd36467bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1eac1c081344634b4496b47a03f7412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d4cc78fb9ed9471b9af7d4ba113ac158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d93d615bd324b78a19c1f9761bdb2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}